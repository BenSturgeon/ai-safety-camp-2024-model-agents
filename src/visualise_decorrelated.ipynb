{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.helpers import load_interpretable_model\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import visualization utilities\n",
    "from feature_vis_impala import (\n",
    "    total_variation, \n",
    "    jitter, \n",
    "    random_scale, \n",
    "    random_rotate\n",
    ")\n",
    "\n",
    "# Import SAE-related modules\n",
    "from sae_cnn import ConvSAE\n",
    "from extract_sae_features import replace_layer_with_sae\n",
    "from feature_vis_sae import load_sae_from_checkpoint\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = load_interpretable_model()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load SAE model\n",
    "sae_checkpoint_path = \"../checkpoints/sae_checkpoint_step_4500000.pt\"  # Update with your checkpoint path\n",
    "sae = load_sae_from_checkpoint(sae_checkpoint_path, device)\n",
    "layer_name = \"conv4a\"  # Update with your target layer\n",
    "layer_number = 8\n",
    "feature_idx = 1  # Change to the feature you want to visualize\n",
    "\n",
    "# Function to visualize a feature with optional color decorrelation\n",
    "def visualize_feature(model, sae, layer_name, feature_idx, num_steps=1000, \n",
    "                      use_decorrelation=False, color_matrices_path=None):\n",
    "    \"\"\"\n",
    "    Visualize what maximally activates a specific SAE feature\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        sae: The SAE model\n",
    "        layer_name: Name of the layer containing the feature\n",
    "        feature_idx: Index of the feature to visualize\n",
    "        num_steps: Number of optimization steps\n",
    "        use_decorrelation: Whether to use color decorrelation\n",
    "        color_matrices_path: Path to color matrices file\n",
    "        \n",
    "    Returns:\n",
    "        visualization_image: numpy array of the visualization (H, W, C)\n",
    "    \"\"\"\n",
    "    # Set up color matrices if using decorrelation\n",
    "    if use_decorrelation and color_matrices_path and os.path.exists(color_matrices_path):\n",
    "        print(f\"Loading color matrices from {color_matrices_path}\")\n",
    "        data = torch.load(color_matrices_path, map_location=device)\n",
    "        whitening_matrix = data['whitening_matrix'].to(device)\n",
    "        unwhitening_matrix = data['unwhitening_matrix'].to(device)\n",
    "        mean_color = data['mean_color'].to(device)\n",
    "    else:\n",
    "        use_decorrelation = False\n",
    "        print(\"Not using color decorrelation\")\n",
    "    \n",
    "    sae = sae[0]\n",
    "    # Attach SAE to the model\n",
    "    sae_hook = replace_layer_with_sae(model, sae, layer_number)\n",
    "    \n",
    "    # Set up hooks to capture activations\n",
    "    activations = {}\n",
    "    \n",
    "    def sae_activation_hook(module, input, output):\n",
    "        # For ConvSAE, the 3rd return value contains the activations\n",
    "        if isinstance(output, tuple) and len(output) >= 3:\n",
    "            activations['sae_features'] = output[2]\n",
    "        return output\n",
    "    \n",
    "    # Register hook on the SAE\n",
    "    hook = sae.register_forward_hook(sae_activation_hook)\n",
    "    \n",
    "    # Initialize image\n",
    "    padded_size = 64 + 8  # 4 pixels on each side\n",
    "    input_img = torch.randint(0, 256, (1, 3, padded_size, padded_size), \n",
    "                            device=device, \n",
    "                            dtype=torch.float32).requires_grad_(True)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.Adam([input_img], lr=0.08)\n",
    "    \n",
    "    # Parameters for transformations\n",
    "    scales = [1.0, 0.975, 1.025, 0.95, 1.05]\n",
    "    angles = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "    \n",
    "    best_activation = float('-inf')\n",
    "    best_img = None\n",
    "    \n",
    "    # Optimization loop\n",
    "    pbar = tqdm(range(num_steps))\n",
    "    for step in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Create a copy for transformations\n",
    "        processed_img = input_img.clone()\n",
    "        \n",
    "        # Apply transformations\n",
    "        ox, oy = np.random.randint(-8, 9, 2)\n",
    "        processed_img = jitter(processed_img, ox, oy)\n",
    "        processed_img = random_scale(processed_img, scales)\n",
    "        processed_img = random_rotate(processed_img, angles)\n",
    "        ox, oy = np.random.randint(-4, 5, 2)\n",
    "        processed_img = jitter(processed_img, ox, oy)\n",
    "        \n",
    "        # Crop padding\n",
    "        processed_img = processed_img[:, :, 4:-4, 4:-4]\n",
    "        \n",
    "        # Ensure values are in [0, 255]\n",
    "        processed_img.data.clamp_(0, 255)\n",
    "        \n",
    "        # Normalize to [0,1] for model input\n",
    "        normalized_img = processed_img / 255.0\n",
    "        \n",
    "        # Forward pass\n",
    "        _ = model(normalized_img)\n",
    "        \n",
    "        # Get the activation for the target feature\n",
    "        if 'sae_features' in activations:\n",
    "            feature_activation = activations['sae_features'][0, feature_idx]\n",
    "            activation_loss = -feature_activation.mean()  # negative because we want to maximize\n",
    "        else:\n",
    "            activation_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Calculate regularization losses\n",
    "        tv_loss = 1e-3 * total_variation(input_img / 255.0)\n",
    "        l2_loss = 1e-3 * torch.norm(input_img / 255.0)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = activation_loss + tv_loss + l2_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Post-processing\n",
    "        with torch.no_grad():\n",
    "            input_img.data.clamp_(0, 255)\n",
    "            \n",
    "            # Track best activation\n",
    "            current_activation = -activation_loss.item()\n",
    "            if current_activation > best_activation:\n",
    "                best_activation = current_activation\n",
    "                best_img = processed_img.clone()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': loss.item(), 'act': current_activation})\n",
    "    \n",
    "    # Clean up hooks\n",
    "    hook.remove()\n",
    "    sae_hook.remove()\n",
    "    \n",
    "    # If we used decorrelation, apply it to the final image\n",
    "    if use_decorrelation and best_img is not None:\n",
    "        # This would be where we apply decorrelation, but we'll skip it for now\n",
    "        # to avoid the range issues\n",
    "        pass\n",
    "    \n",
    "    # Convert to numpy for display\n",
    "    result = (best_img.detach().cpu().squeeze().permute(1, 2, 0) / 255.0).numpy()\n",
    "    \n",
    "    return result, best_activation\n",
    "\n",
    "# Run the visualization\n",
    "vis, activation = visualize_feature(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    layer_name=layer_name,\n",
    "    feature_idx=feature_idx,\n",
    "    num_steps=5000,  # Reduce for faster results\n",
    "    use_decorrelation=False,  # Start with this off to ensure it works\n",
    "    color_matrices_path=\"color_matrices.pt\"\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(vis)\n",
    "plt.title(f'SAE Feature {feature_idx} (Act: {activation:.2f})')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.helpers import load_interpretable_model\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import visualization utilities\n",
    "from feature_vis_impala import (\n",
    "    total_variation, \n",
    "    jitter, \n",
    "    random_scale, \n",
    "    random_rotate\n",
    ")\n",
    "\n",
    "# Import SAE-related modules\n",
    "from sae_cnn import ConvSAE\n",
    "from extract_sae_features import replace_layer_with_sae\n",
    "from feature_vis_sae import load_sae_from_checkpoint\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = load_interpretable_model()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load SAE model with adjusted path and access\n",
    "sae_checkpoint_path = \"../checkpoints/sae_checkpoint_step_4500000.pt\"  # Adjusted with ..\n",
    "sae_result = load_sae_from_checkpoint(sae_checkpoint_path, device)\n",
    "layer_name = \"conv4a\"  # Update with your target layer\n",
    "layer_number = 8\n",
    "\n",
    "# List of feature indices to visualize\n",
    "feature_indices = [0, 1, 2, 3]  # Change to the features you want to visualize\n",
    "\n",
    "# Function to apply whitening to an image tensor\n",
    "def apply_whitening(x, whitening_matrix, mean_color=None):\n",
    "    \"\"\"Apply whitening transformation to an image tensor\"\"\"\n",
    "    # Store original shape\n",
    "    original_shape = x.shape\n",
    "    \n",
    "    # Reshape to (B*H*W, C)\n",
    "    x_flat = x.permute(0, 2, 3, 1).reshape(-1, 3)\n",
    "    \n",
    "    # Center if mean_color is provided\n",
    "    if mean_color is not None:\n",
    "        x_flat = x_flat - mean_color\n",
    "    \n",
    "    # Apply whitening\n",
    "    x_whitened = torch.mm(x_flat, whitening_matrix.t())\n",
    "    \n",
    "    # Reshape back to original shape\n",
    "    x_whitened = x_whitened.reshape(original_shape[0], original_shape[2], original_shape[3], 3).permute(0, 3, 1, 2)\n",
    "    \n",
    "    return x_whitened\n",
    "\n",
    "# Function to apply unwhitening to an image tensor\n",
    "def apply_unwhitening(x, unwhitening_matrix, mean_color=None):\n",
    "    \"\"\"Apply unwhitening transformation to an image tensor\"\"\"\n",
    "    # Store original shape\n",
    "    original_shape = x.shape\n",
    "    \n",
    "    # Reshape to (B*H*W, C)\n",
    "    x_flat = x.permute(0, 2, 3, 1).reshape(-1, 3)\n",
    "    \n",
    "    # Apply unwhitening\n",
    "    x_unwhitened = torch.mm(x_flat, unwhitening_matrix.t())\n",
    "    \n",
    "    # Add back mean if provided\n",
    "    if mean_color is not None:\n",
    "        x_unwhitened = x_unwhitened + mean_color\n",
    "    \n",
    "    # Reshape back to original shape\n",
    "    x_unwhitened = x_unwhitened.reshape(original_shape[0], original_shape[2], original_shape[3], 3).permute(0, 3, 1, 2)\n",
    "    \n",
    "    return x_unwhitened\n",
    "\n",
    "# Function to visualize a feature with optional color decorrelation\n",
    "def visualize_feature(model, sae, layer_name, feature_idx, num_steps=1000, \n",
    "                      use_decorrelation=False, color_matrices_path=None):\n",
    "    \"\"\"\n",
    "    Visualize what maximally activates a specific SAE feature\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        sae: The SAE model\n",
    "        layer_name: Name of the layer containing the feature\n",
    "        feature_idx: Index of the feature to visualize\n",
    "        num_steps: Number of optimization steps\n",
    "        use_decorrelation: Whether to use color decorrelation\n",
    "        color_matrices_path: Path to color matrices file\n",
    "        \n",
    "    Returns:\n",
    "        visualization_image: numpy array of the visualization (H, W, C)\n",
    "    \"\"\"\n",
    "    # Set up color matrices if using decorrelation\n",
    "    if use_decorrelation and color_matrices_path and os.path.exists(color_matrices_path):\n",
    "        print(f\"Loading color matrices from {color_matrices_path}\")\n",
    "        data = torch.load(color_matrices_path, map_location=device)\n",
    "        whitening_matrix = data['whitening_matrix'].to(device)\n",
    "        unwhitening_matrix = data['unwhitening_matrix'].to(device)\n",
    "        mean_color = data['mean_color'].to(device)\n",
    "    else:\n",
    "        use_decorrelation = False\n",
    "        whitening_matrix = torch.eye(3, device=device)\n",
    "        unwhitening_matrix = torch.eye(3, device=device)\n",
    "        mean_color = torch.zeros(3, device=device)\n",
    "        print(\"Not using color decorrelation\")\n",
    "    \n",
    "    # Attach SAE to the model\n",
    "    sae = sae[0]\n",
    "    sae_hook = replace_layer_with_sae(model, sae, layer_number)\n",
    "    \n",
    "    # Set up hooks to capture activations\n",
    "    activations = {}\n",
    "    \n",
    "    def sae_activation_hook(module, input, output):\n",
    "        # For ConvSAE, the 3rd return value contains the activations\n",
    "        if isinstance(output, tuple) and len(output) >= 3:\n",
    "            activations['sae_features'] = output[2]\n",
    "        return output\n",
    "    \n",
    "    # Register hook on the SAE\n",
    "    hook = sae.register_forward_hook(sae_activation_hook)\n",
    "    \n",
    "    # Initialize image\n",
    "    padded_size = 64 + 8  # 4 pixels on each side\n",
    "    input_img = torch.randint(0, 256, (1, 3, padded_size, padded_size), \n",
    "                            device=device, \n",
    "                            dtype=torch.float32).requires_grad_(True)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.Adam([input_img], lr=0.08)\n",
    "    \n",
    "    # Parameters for transformations\n",
    "    scales = [1.0, 0.975, 1.025, 0.95, 1.05]\n",
    "    angles = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "    \n",
    "    best_activation = float('-inf')\n",
    "    best_img = None\n",
    "    \n",
    "    # Optimization loop\n",
    "    pbar = tqdm(range(num_steps))\n",
    "    for step in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Create a copy for transformations\n",
    "        processed_img = input_img.clone()\n",
    "        \n",
    "        # Apply transformations\n",
    "        ox, oy = np.random.randint(-8, 9, 2)\n",
    "        processed_img = jitter(processed_img, ox, oy)\n",
    "        processed_img = random_scale(processed_img, scales)\n",
    "        processed_img = random_rotate(processed_img, angles)\n",
    "        ox, oy = np.random.randint(-4, 5, 2)\n",
    "        processed_img = jitter(processed_img, ox, oy)\n",
    "        \n",
    "        # Crop padding\n",
    "        processed_img = processed_img[:, :, 4:-4, 4:-4]\n",
    "        \n",
    "        # Ensure values are in [0, 255]\n",
    "        processed_img.data.clamp_(0, 255)\n",
    "        \n",
    "        # Normalize to [0,1] for model input\n",
    "        normalized_img = processed_img / 255.0\n",
    "        \n",
    "        # Apply whitening if using decorrelation\n",
    "        # Note: We optimize in the original space but do the forward pass in whitened space\n",
    "        if use_decorrelation:\n",
    "            # Apply whitening\n",
    "            whitened_img = apply_whitening(normalized_img, whitening_matrix, mean_color)\n",
    "            \n",
    "            # Rescale to [0,1] range for the model\n",
    "            # This is a key step to avoid the out-of-range error\n",
    "            whitened_min = whitened_img.min()\n",
    "            whitened_max = whitened_img.max()\n",
    "            rescaled_img = (whitened_img - whitened_min) / (whitened_max - whitened_min)\n",
    "            \n",
    "            # Use the rescaled whitened image for the forward pass\n",
    "            model_input = rescaled_img\n",
    "        else:\n",
    "            # Use the normalized image directly\n",
    "            model_input = normalized_img\n",
    "        \n",
    "        # Forward pass\n",
    "        _ = model(model_input)\n",
    "        \n",
    "        # Get the activation for the target feature\n",
    "        if 'sae_features' in activations:\n",
    "            feature_activation = activations['sae_features'][0, feature_idx]\n",
    "            activation_loss = -feature_activation.mean()  # negative because we want to maximize\n",
    "        else:\n",
    "            activation_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Calculate regularization losses\n",
    "        tv_loss = 1e-3 * total_variation(input_img / 255.0)\n",
    "        l2_loss = 1e-3 * torch.norm(input_img / 255.0)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = activation_loss + tv_loss + l2_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Post-processing\n",
    "        with torch.no_grad():\n",
    "            input_img.data.clamp_(0, 255)\n",
    "            \n",
    "            # Track best activation\n",
    "            current_activation = -activation_loss.item()\n",
    "            if current_activation > best_activation:\n",
    "                best_activation = current_activation\n",
    "                best_img = processed_img.clone()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': loss.item(), 'act': current_activation})\n",
    "    \n",
    "    # Clean up hooks\n",
    "    hook.remove()\n",
    "    sae_hook.remove()\n",
    "    \n",
    "    # Process the final image\n",
    "    result_img = best_img / 255.0  # Normalize to [0,1]\n",
    "    \n",
    "    # If we used decorrelation, apply it to the final image for visualization\n",
    "    if use_decorrelation:\n",
    "        # Apply whitening\n",
    "        whitened_img = apply_whitening(result_img, whitening_matrix, mean_color)\n",
    "        \n",
    "        # Apply unwhitening to get back to normal color space\n",
    "        # This is the key step for visualization - we want to see what the decorrelated\n",
    "        # optimization produced in the original color space\n",
    "        unwhitened_img = apply_unwhitening(whitened_img, unwhitening_matrix, mean_color)\n",
    "        \n",
    "        # Ensure values are in [0,1]\n",
    "        result_img = torch.clamp(unwhitened_img, 0, 1)\n",
    "    \n",
    "    # Convert to numpy for display\n",
    "    result = result_img.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "    \n",
    "    return result, best_activation\n",
    "\n",
    "# Create a figure with 2x2 subplots for standard visualization\n",
    "fig1, axes1 = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes1 = axes1.flatten()\n",
    "\n",
    "# Create a figure with 2x2 subplots for decorrelated visualization\n",
    "fig2, axes2 = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes2 = axes2.flatten()\n",
    "\n",
    "# Create visualizations directory if it doesn't exist\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "\n",
    "# Visualize each feature with both standard and decorrelated approaches\n",
    "for i, feature_idx in enumerate(feature_indices):\n",
    "    print(f\"\\nVisualizing feature {feature_idx} ({i+1}/{len(feature_indices)})\")\n",
    "    \n",
    "    # Standard visualization (no decorrelation)\n",
    "    print(\"Standard visualization...\")\n",
    "    vis_standard, act_standard = visualize_feature(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        layer_name=layer_name,\n",
    "        feature_idx=feature_idx,\n",
    "        num_steps=2560,\n",
    "        use_decorrelation=False\n",
    "    )\n",
    "    \n",
    "    # Decorrelated visualization\n",
    "    print(\"Decorrelated visualization...\")\n",
    "    vis_decorrelated, act_decorrelated = visualize_feature(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        layer_name=layer_name,\n",
    "        feature_idx=feature_idx,\n",
    "        num_steps=2560,\n",
    "        use_decorrelation=True,\n",
    "        color_matrices_path=\"color_matrices.pt\"\n",
    "    )\n",
    "    \n",
    "    # Display and save standard visualization\n",
    "    axes1[i].imshow(vis_standard)\n",
    "    axes1[i].set_title(f'Standard - Feature {feature_idx} (Act: {act_standard:.2f})')\n",
    "    axes1[i].axis('off')\n",
    "    plt.imsave(f'visualizations/standard_feature_{feature_idx}.png', vis_standard)\n",
    "    \n",
    "    # Display and save decorrelated visualization  \n",
    "    axes2[i].imshow(vis_decorrelated)\n",
    "    axes2[i].set_title(f'Decorrelated - Feature {feature_idx} (Act: {act_decorrelated:.2f})')\n",
    "    axes2[i].axis('off')\n",
    "    plt.imsave(f'visualizations/decorrelated_feature_{feature_idx}.png', vis_decorrelated)\n",
    "\n",
    "# Add overall titles\n",
    "fig1.suptitle('Standard Visualization', fontsize=16)\n",
    "fig2.suptitle('Decorrelated Visualization', fontsize=16)\n",
    "\n",
    "# Save and display combined plots\n",
    "plt.figure(fig1.number)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/standard_features_combined.png')\n",
    "\n",
    "plt.figure(fig2.number)\n",
    "plt.tight_layout() \n",
    "plt.savefig('visualizations/decorrelated_features_combined.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
