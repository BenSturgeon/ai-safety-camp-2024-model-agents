{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.helpers import load_interpretable_model\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import visualization utilities\n",
    "from feature_vis_impala import (\n",
    "    total_variation, \n",
    "    jitter, \n",
    "    random_scale, \n",
    "    random_rotate\n",
    ")\n",
    "\n",
    "# Import SAE-related modules\n",
    "from sae_cnn import ConvSAE\n",
    "from extract_sae_features import replace_layer_with_sae\n",
    "from feature_vis_sae import load_sae_from_checkpoint\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = load_interpretable_model()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load SAE model\n",
    "sae_checkpoint_path = \"../checkpoints/sae_checkpoint_step_4500000.pt\"  # Update with your checkpoint path\n",
    "sae = load_sae_from_checkpoint(sae_checkpoint_path, device)\n",
    "layer_name = \"conv4a\"  # Update with your target layer\n",
    "layer_number = 8\n",
    "feature_idx = 1  # Change to the feature you want to visualize\n",
    "\n",
    "# Function to visualize a feature with optional color decorrelation\n",
    "def visualize_feature(model, sae, layer_name, feature_idx, num_steps=1000, \n",
    "                      use_decorrelation=False, color_matrices_path=None):\n",
    "    \"\"\"\n",
    "    Visualize what maximally activates a specific SAE feature\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        sae: The SAE model\n",
    "        layer_name: Name of the layer containing the feature\n",
    "        feature_idx: Index of the feature to visualize\n",
    "        num_steps: Number of optimization steps\n",
    "        use_decorrelation: Whether to use color decorrelation\n",
    "        color_matrices_path: Path to color matrices file\n",
    "        \n",
    "    Returns:\n",
    "        visualization_image: numpy array of the visualization (H, W, C)\n",
    "    \"\"\"\n",
    "    # Set up color matrices if using decorrelation\n",
    "    if use_decorrelation and color_matrices_path and os.path.exists(color_matrices_path):\n",
    "        print(f\"Loading color matrices from {color_matrices_path}\")\n",
    "        data = torch.load(color_matrices_path, map_location=device)\n",
    "        whitening_matrix = data['whitening_matrix'].to(device)\n",
    "        unwhitening_matrix = data['unwhitening_matrix'].to(device)\n",
    "        mean_color = data['mean_color'].to(device)\n",
    "    else:\n",
    "        use_decorrelation = False\n",
    "        print(\"Not using color decorrelation\")\n",
    "    \n",
    "    sae = sae[0]\n",
    "    # Attach SAE to the model\n",
    "    sae_hook = replace_layer_with_sae(model, sae, layer_number)\n",
    "    \n",
    "    # Set up hooks to capture activations\n",
    "    activations = {}\n",
    "    \n",
    "    def sae_activation_hook(module, input, output):\n",
    "        # For ConvSAE, the 3rd return value contains the activations\n",
    "        if isinstance(output, tuple) and len(output) >= 3:\n",
    "            activations['sae_features'] = output[2]\n",
    "        return output\n",
    "    \n",
    "    # Register hook on the SAE\n",
    "    hook = sae.register_forward_hook(sae_activation_hook)\n",
    "    \n",
    "    # Initialize image\n",
    "    padded_size = 64 + 8  # 4 pixels on each side\n",
    "    input_img = torch.randint(0, 256, (1, 3, padded_size, padded_size), \n",
    "                            device=device, \n",
    "                            dtype=torch.float32).requires_grad_(True)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.Adam([input_img], lr=0.08)\n",
    "    \n",
    "    # Parameters for transformations\n",
    "    scales = [1.0, 0.975, 1.025, 0.95, 1.05]\n",
    "    angles = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "    \n",
    "    best_activation = float('-inf')\n",
    "    best_img = None\n",
    "    \n",
    "    # Optimization loop\n",
    "    pbar = tqdm(range(num_steps))\n",
    "    for step in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Create a copy for transformations\n",
    "        processed_img = input_img.clone()\n",
    "        \n",
    "        # Apply transformations\n",
    "        ox, oy = np.random.randint(-8, 9, 2)\n",
    "        processed_img = jitter(processed_img, ox, oy)\n",
    "        processed_img = random_scale(processed_img, scales)\n",
    "        processed_img = random_rotate(processed_img, angles)\n",
    "        ox, oy = np.random.randint(-4, 5, 2)\n",
    "        processed_img = jitter(processed_img, ox, oy)\n",
    "        \n",
    "        # Crop padding\n",
    "        processed_img = processed_img[:, :, 4:-4, 4:-4]\n",
    "        \n",
    "        # Ensure values are in [0, 255]\n",
    "        processed_img.data.clamp_(0, 255)\n",
    "        \n",
    "        # Normalize to [0,1] for model input\n",
    "        normalized_img = processed_img / 255.0\n",
    "        \n",
    "        # Forward pass\n",
    "        _ = model(normalized_img)\n",
    "        \n",
    "        # Get the activation for the target feature\n",
    "        if 'sae_features' in activations:\n",
    "            feature_activation = activations['sae_features'][0, feature_idx]\n",
    "            activation_loss = -feature_activation.mean()  # negative because we want to maximize\n",
    "        else:\n",
    "            activation_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Calculate regularization losses\n",
    "        tv_loss = 1e-3 * total_variation(input_img / 255.0)\n",
    "        l2_loss = 1e-3 * torch.norm(input_img / 255.0)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = activation_loss + tv_loss + l2_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Post-processing\n",
    "        with torch.no_grad():\n",
    "            input_img.data.clamp_(0, 255)\n",
    "            \n",
    "            # Track best activation\n",
    "            current_activation = -activation_loss.item()\n",
    "            if current_activation > best_activation:\n",
    "                best_activation = current_activation\n",
    "                best_img = processed_img.clone()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': loss.item(), 'act': current_activation})\n",
    "    \n",
    "    # Clean up hooks\n",
    "    hook.remove()\n",
    "    sae_hook.remove()\n",
    "    \n",
    "    # If we used decorrelation, apply it to the final image\n",
    "    if use_decorrelation and best_img is not None:\n",
    "        # This would be where we apply decorrelation, but we'll skip it for now\n",
    "        # to avoid the range issues\n",
    "        pass\n",
    "    \n",
    "    # Convert to numpy for display\n",
    "    result = (best_img.detach().cpu().squeeze().permute(1, 2, 0) / 255.0).numpy()\n",
    "    \n",
    "    return result, best_activation\n",
    "\n",
    "# Run the visualization\n",
    "vis, activation = visualize_feature(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    layer_name=layer_name,\n",
    "    feature_idx=feature_idx,\n",
    "    num_steps=5000,  # Reduce for faster results\n",
    "    use_decorrelation=False,  # Start with this off to ensure it works\n",
    "    color_matrices_path=\"color_matrices.pt\"\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(vis)\n",
    "plt.title(f'SAE Feature {feature_idx} (Act: {activation:.2f})')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.helpers import load_interpretable_model\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import visualization utilities\n",
    "from feature_vis_impala import (\n",
    "    total_variation, \n",
    "    jitter, \n",
    "    random_scale, \n",
    "    random_rotate\n",
    ")\n",
    "\n",
    "# Import SAE-related modules\n",
    "from sae_cnn import ConvSAE\n",
    "from extract_sae_features import replace_layer_with_sae\n",
    "from feature_vis_sae import load_sae_from_checkpoint\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = load_interpretable_model()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load SAE model with adjusted path and access\n",
    "sae_checkpoint_path = \"../checkpoints/sae_checkpoint_step_4500000.pt\"  # Adjusted with ..\n",
    "sae_result = load_sae_from_checkpoint(sae_checkpoint_path, device)\n",
    "layer_name = \"conv4a\"  # Update with your target layer\n",
    "layer_number = 8\n",
    "\n",
    "# List of feature indices to visualize - all 128 features\n",
    "feature_indices = list(range(128))  # Visualize all 128 features\n",
    "\n",
    "# Function to apply whitening to an image tensor\n",
    "def apply_whitening(x, whitening_matrix, mean_color=None):\n",
    "    \"\"\"Apply whitening transformation to an image tensor\"\"\"\n",
    "    # Store original shape\n",
    "    original_shape = x.shape\n",
    "    \n",
    "    # Reshape to (B*H*W, C)\n",
    "    x_flat = x.permute(0, 2, 3, 1).reshape(-1, 3)\n",
    "    \n",
    "    # Center if mean_color is provided\n",
    "    if mean_color is not None:\n",
    "        x_flat = x_flat - mean_color\n",
    "    \n",
    "    # Apply whitening\n",
    "    x_whitened = torch.mm(x_flat, whitening_matrix.t())\n",
    "    \n",
    "    # Reshape back to original shape\n",
    "    x_whitened = x_whitened.reshape(original_shape[0], original_shape[2], original_shape[3], 3).permute(0, 3, 1, 2)\n",
    "    \n",
    "    return x_whitened\n",
    "\n",
    "# Function to apply unwhitening to an image tensor\n",
    "def apply_unwhitening(x, unwhitening_matrix, mean_color=None):\n",
    "    \"\"\"Apply unwhitening transformation to an image tensor\"\"\"\n",
    "    # Store original shape\n",
    "    original_shape = x.shape\n",
    "    \n",
    "    # Reshape to (B*H*W, C)\n",
    "    x_flat = x.permute(0, 2, 3, 1).reshape(-1, 3)\n",
    "    \n",
    "    # Apply unwhitening\n",
    "    x_unwhitened = torch.mm(x_flat, unwhitening_matrix.t())\n",
    "    \n",
    "    # Add back mean if provided\n",
    "    if mean_color is not None:\n",
    "        x_unwhitened = x_unwhitened + mean_color\n",
    "    \n",
    "    # Reshape back to original shape\n",
    "    x_unwhitened = x_unwhitened.reshape(original_shape[0], original_shape[2], original_shape[3], 3).permute(0, 3, 1, 2)\n",
    "    \n",
    "    return x_unwhitened\n",
    "\n",
    "# Function to visualize a feature with optional color decorrelation\n",
    "def visualize_feature(model, sae, layer_name, feature_idx, num_steps=1000, \n",
    "                      use_decorrelation=False, color_matrices_path=None):\n",
    "    \"\"\"\n",
    "    Visualize what maximally activates a specific SAE feature\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        sae: The SAE model\n",
    "        layer_name: Name of the layer containing the feature\n",
    "        feature_idx: Index of the feature to visualize\n",
    "        num_steps: Number of optimization steps\n",
    "        use_decorrelation: Whether to use color decorrelation\n",
    "        color_matrices_path: Path to color matrices file\n",
    "        \n",
    "    Returns:\n",
    "        visualization_image: numpy array of the visualization (H, W, C)\n",
    "    \"\"\"\n",
    "    # Set up color matrices if using decorrelation\n",
    "    if use_decorrelation and color_matrices_path and os.path.exists(color_matrices_path):\n",
    "        print(f\"Loading color matrices from {color_matrices_path}\")\n",
    "        data = torch.load(color_matrices_path, map_location=device)\n",
    "        whitening_matrix = data['whitening_matrix'].to(device)\n",
    "        unwhitening_matrix = data['unwhitening_matrix'].to(device)\n",
    "        mean_color = data['mean_color'].to(device)\n",
    "    else:\n",
    "        use_decorrelation = False\n",
    "        whitening_matrix = torch.eye(3, device=device)\n",
    "        unwhitening_matrix = torch.eye(3, device=device)\n",
    "        mean_color = torch.zeros(3, device=device)\n",
    "        print(\"Not using color decorrelation\")\n",
    "    \n",
    "    # Attach SAE to the model\n",
    "    sae = sae[0]\n",
    "    sae_hook = replace_layer_with_sae(model, sae, layer_number)\n",
    "    \n",
    "    # Set up hooks to capture activations\n",
    "    activations = {}\n",
    "    \n",
    "    def sae_activation_hook(module, input, output):\n",
    "        # For ConvSAE, the 3rd return value contains the activations\n",
    "        if isinstance(output, tuple) and len(output) >= 3:\n",
    "            activations['sae_features'] = output[2]\n",
    "        return output\n",
    "    \n",
    "    # Register hook on the SAE\n",
    "    hook = sae.register_forward_hook(sae_activation_hook)\n",
    "    \n",
    "    # Initialize image\n",
    "    padded_size = 64 + 8  # 4 pixels on each side\n",
    "    input_img = torch.randint(0, 256, (1, 3, padded_size, padded_size), \n",
    "                            device=device, \n",
    "                            dtype=torch.float32).requires_grad_(True)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.Adam([input_img], lr=0.08)\n",
    "    \n",
    "    # Parameters for transformations\n",
    "    scales = [1.0, 0.975, 1.025, 0.95, 1.05]\n",
    "    angles = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "    \n",
    "    best_activation = float('-inf')\n",
    "    best_img = None\n",
    "    \n",
    "    # Optimization loop\n",
    "    pbar = tqdm(range(num_steps))\n",
    "    for step in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Create a copy for transformations\n",
    "        processed_img = input_img.clone()\n",
    "        \n",
    "        # Apply transformations\n",
    "        ox, oy = np.random.randint(-8, 9, 2)\n",
    "        processed_img = jitter(processed_img, ox, oy)\n",
    "        processed_img = random_scale(processed_img, scales)\n",
    "        processed_img = random_rotate(processed_img, angles)\n",
    "        ox, oy = np.random.randint(-4, 5, 2)\n",
    "        processed_img = jitter(processed_img, ox, oy)\n",
    "        \n",
    "        # Crop padding\n",
    "        processed_img = processed_img[:, :, 4:-4, 4:-4]\n",
    "        \n",
    "        # Ensure values are in [0, 255]\n",
    "        processed_img.data.clamp_(0, 255)\n",
    "        \n",
    "        # Normalize to [0,1] for model input\n",
    "        normalized_img = processed_img / 255.0\n",
    "        \n",
    "        # Apply whitening if using decorrelation\n",
    "        # Note: We optimize in the original space but do the forward pass in whitened space\n",
    "        if use_decorrelation:\n",
    "            # Apply whitening\n",
    "            whitened_img = apply_whitening(normalized_img, whitening_matrix, mean_color)\n",
    "            \n",
    "            # Rescale to [0,1] range for the model\n",
    "            # This is a key step to avoid the out-of-range error\n",
    "            whitened_min = whitened_img.min()\n",
    "            whitened_max = whitened_img.max()\n",
    "            rescaled_img = (whitened_img - whitened_min) / (whitened_max - whitened_min)\n",
    "            \n",
    "            # Use the rescaled whitened image for the forward pass\n",
    "            model_input = rescaled_img\n",
    "        else:\n",
    "            # Use the normalized image directly\n",
    "            model_input = normalized_img\n",
    "        \n",
    "        # Forward pass\n",
    "        _ = model(model_input)\n",
    "        \n",
    "        # Get the activation for the target feature\n",
    "        if 'sae_features' in activations:\n",
    "            feature_activation = activations['sae_features'][0, feature_idx]\n",
    "            activation_loss = -feature_activation.mean()  # negative because we want to maximize\n",
    "        else:\n",
    "            activation_loss = torch.tensor(0.0, device=device)\n",
    "        \n",
    "        # Calculate regularization losses\n",
    "        tv_loss = 1e-3 * total_variation(input_img / 255.0)\n",
    "        l2_loss = 1e-3 * torch.norm(input_img / 255.0)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = activation_loss + tv_loss + l2_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Post-processing\n",
    "        with torch.no_grad():\n",
    "            input_img.data.clamp_(0, 255)\n",
    "            \n",
    "            # Track best activation\n",
    "            current_activation = -activation_loss.item()\n",
    "            if current_activation > best_activation:\n",
    "                best_activation = current_activation\n",
    "                best_img = processed_img.clone()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({'loss': loss.item(), 'act': current_activation})\n",
    "    \n",
    "    # Clean up hooks\n",
    "    hook.remove()\n",
    "    sae_hook.remove()\n",
    "    \n",
    "    # Process the final image\n",
    "    result_img = best_img / 255.0  # Normalize to [0,1]\n",
    "    \n",
    "    # If we used decorrelation, apply it to the final image for visualization\n",
    "    if use_decorrelation:\n",
    "        # Apply whitening\n",
    "        whitened_img = apply_whitening(result_img, whitening_matrix, mean_color)\n",
    "        \n",
    "        # Apply unwhitening to get back to normal color space\n",
    "        # This is the key step for visualization - we want to see what the decorrelated\n",
    "        # optimization produced in the original color space\n",
    "        unwhitened_img = apply_unwhitening(whitened_img, unwhitening_matrix, mean_color)\n",
    "        \n",
    "        # Ensure values are in [0,1]\n",
    "        result_img = torch.clamp(unwhitened_img, 0, 1)\n",
    "    \n",
    "    # Convert to numpy for display\n",
    "    result = result_img.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "    \n",
    "    return result, best_activation\n",
    "\n",
    "# Create visualizations directory if it doesn't exist\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "\n",
    "# Process features in batches of 8\n",
    "batch_size = 8\n",
    "num_batches = len(feature_indices) // batch_size\n",
    "if len(feature_indices) % batch_size != 0:\n",
    "    num_batches += 1\n",
    "\n",
    "# Process each batch\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(feature_indices))\n",
    "    batch_features = feature_indices[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"\\nProcessing batch {batch_idx+1}/{num_batches} (features {start_idx}-{end_idx-1})\")\n",
    "    \n",
    "    # Create figure for standard visualization\n",
    "    fig1, axes1 = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes1 = axes1.flatten()\n",
    "    \n",
    "    # Create figure for decorrelated visualization\n",
    "    fig2, axes2 = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes2 = axes2.flatten()\n",
    "    \n",
    "    # Visualize each feature in this batch\n",
    "    for i, feature_idx in enumerate(batch_features):\n",
    "        print(f\"\\nVisualizing feature {feature_idx} ({i+1}/{len(batch_features)})\")\n",
    "        \n",
    "        # Standard visualization (no decorrelation)\n",
    "        print(\"Standard visualization...\")\n",
    "        vis_standard, act_standard = visualize_feature(\n",
    "            model=model,\n",
    "            sae=sae_result,\n",
    "            layer_name=layer_name,\n",
    "            feature_idx=feature_idx,\n",
    "            num_steps=2560,\n",
    "            use_decorrelation=False\n",
    "        )\n",
    "        \n",
    "        # Decorrelated visualization\n",
    "        print(\"Decorrelated visualization...\")\n",
    "        vis_decorrelated, act_decorrelated = visualize_feature(\n",
    "            model=model,\n",
    "            sae=sae_result,\n",
    "            layer_name=layer_name,\n",
    "            feature_idx=feature_idx,\n",
    "            num_steps=2560,\n",
    "            use_decorrelation=True,\n",
    "            color_matrices_path=\"color_matrices.pt\"\n",
    "        )\n",
    "        \n",
    "        # Display and save standard visualization\n",
    "        axes1[i].imshow(vis_standard)\n",
    "        axes1[i].set_title(f'Standard - Feature {feature_idx} (Act: {act_standard:.2f})')\n",
    "        axes1[i].axis('off')\n",
    "        plt.imsave(f'visualizations/standard_feature_{feature_idx}.png', vis_standard)\n",
    "        \n",
    "        # Display and save decorrelated visualization  \n",
    "        axes2[i].imshow(vis_decorrelated)\n",
    "        axes2[i].set_title(f'Decorrelated - Feature {feature_idx} (Act: {act_decorrelated:.2f})')\n",
    "        axes2[i].axis('off')\n",
    "        plt.imsave(f'visualizations/decorrelated_feature_{feature_idx}.png', vis_decorrelated)\n",
    "    \n",
    "    # Add overall titles\n",
    "    fig1.suptitle(f'Standard Visualization (Batch {batch_idx+1}/{num_batches})', fontsize=16)\n",
    "    fig2.suptitle(f'Decorrelated Visualization (Batch {batch_idx+1}/{num_batches})', fontsize=16)\n",
    "    \n",
    "    # Save batch figures\n",
    "    plt.figure(fig1.number)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'visualizations/standard_batch_{batch_idx+1}.png')\n",
    "    \n",
    "    plt.figure(fig2.number)\n",
    "    plt.tight_layout() \n",
    "    plt.savefig(f'visualizations/decorrelated_batch_{batch_idx+1}.png')\n",
    "    \n",
    "    # Close figures to free memory\n",
    "    plt.close(fig1)\n",
    "    plt.close(fig2)\n",
    "\n",
    "print(\"All visualizations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import visualization utilities\n",
    "from feature_vis_impala import (\n",
    "    total_variation, \n",
    "    jitter, \n",
    "    random_scale, \n",
    "    random_rotate\n",
    ")\n",
    "\n",
    "# Import SAE-related modules\n",
    "from sae_cnn import ConvSAE, generate_batch_activations_parallel\n",
    "from extract_sae_features import replace_layer_with_sae\n",
    "from feature_vis_sae import load_sae_from_checkpoint\n",
    "from utils.helpers import load_interpretable_model, ModelActivations\n",
    "\n",
    "# Import color decorrelation functions\n",
    "from color_decorrelation import (\n",
    "    apply_whitening,\n",
    "    apply_unwhitening,\n",
    "    load_color_matrices\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = load_interpretable_model()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load SAE model\n",
    "sae_checkpoint_path = \"../checkpoints/sae_checkpoint_step_4500000.pt\"\n",
    "sae_result = load_sae_from_checkpoint(sae_checkpoint_path, device)\n",
    "sae = sae_result[0]  # Access the SAE with sae[0]\n",
    "layer_name = \"conv4a\"  # Update with your target layer\n",
    "layer_number = 8\n",
    "\n",
    "# Load color matrices\n",
    "color_matrices_path = \"color_matrices.pt\"\n",
    "if os.path.exists(color_matrices_path):\n",
    "    cov_matrix, whitening_matrix, unwhitening_matrix, mean_color = load_color_matrices(color_matrices_path, device)\n",
    "    use_decorrelation = True\n",
    "else:\n",
    "    whitening_matrix = torch.eye(3, device=device)\n",
    "    unwhitening_matrix = torch.eye(3, device=device)\n",
    "    mean_color = torch.zeros(3, device=device)\n",
    "    use_decorrelation = False\n",
    "\n",
    "# Function to extract activation patches\n",
    "def extract_activation_patches(activation_map, observation, patch_size=16, stride=8):\n",
    "    \"\"\"\n",
    "    Extract patches around high-activation regions in the activation map.\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    act_h, act_w = activation_map.shape\n",
    "    \n",
    "    # Ensure observation is in (64,64,3) format\n",
    "    if observation.shape[0] == 64 and observation.shape[2] == 3:\n",
    "        # Already in (64,64,3) format\n",
    "        obs_np = observation.detach().cpu().numpy()\n",
    "    elif observation.shape[0] == 3:\n",
    "        # In (3,64,64) format, transpose to (64,64,3)\n",
    "        obs_np = observation.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    else:\n",
    "        # Try to reshape to (64,64,3)\n",
    "        obs_np = observation.detach().cpu().numpy()\n",
    "        if obs_np.shape[0] == 64:\n",
    "            obs_np = obs_np.transpose(0, 2, 1)  # (64,3,64) -> (64,64,3)\n",
    "    \n",
    "    obs_h, obs_w = obs_np.shape[:2]\n",
    "    \n",
    "    # Scale factors between activation map and original image\n",
    "    scale_h = obs_h / act_h\n",
    "    scale_w = obs_w / act_w\n",
    "    \n",
    "    # Ensure patch size isn't larger than the image\n",
    "    patch_size = min(patch_size, obs_h, obs_w)\n",
    "    \n",
    "    # Convert patch_size and stride to activation map space\n",
    "    act_patch_size = max(1, int(patch_size / scale_h))\n",
    "    act_stride = max(1, int(stride / scale_h))\n",
    "    \n",
    "    # Find regions of high activation\n",
    "    flat_activations = activation_map.flatten()\n",
    "    top_k = 10  # Number of high activation points to consider\n",
    "    top_indices = np.argsort(flat_activations)[-top_k:]\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        # Convert flat index back to 2D coordinates\n",
    "        i, j = idx // act_w, idx % act_w\n",
    "        \n",
    "        # Convert to image space coordinates\n",
    "        img_i = int(i * scale_h)\n",
    "        img_j = int(j * scale_w)\n",
    "        \n",
    "        # Calculate patch boundaries\n",
    "        half_size = patch_size // 2\n",
    "        start_i = max(0, img_i - half_size)\n",
    "        start_j = max(0, img_j - half_size)\n",
    "        end_i = min(obs_h, img_i + half_size)\n",
    "        end_j = min(obs_w, img_j + half_size)\n",
    "        \n",
    "        # Extract patch from numpy array\n",
    "        patch = obs_np[start_i:end_i, start_j:end_j, :]\n",
    "        \n",
    "        # Get activation score for this region\n",
    "        act_start_i = max(0, i - act_patch_size//2)\n",
    "        act_start_j = max(0, j - act_patch_size//2)\n",
    "        act_end_i = min(act_h, i + act_patch_size//2)\n",
    "        act_end_j = min(act_w, j + act_patch_size//2)\n",
    "        region_score = activation_map[act_start_i:act_end_i, act_start_j:act_end_j].mean().item()\n",
    "        \n",
    "        # Resize patch to standard size if needed\n",
    "        if patch.shape[:2] != (patch_size, patch_size):\n",
    "            try:\n",
    "                patch_pil = Image.fromarray((patch * 255).astype(np.uint8))\n",
    "                patch_pil = patch_pil.resize((patch_size, patch_size), Image.BILINEAR)\n",
    "                patch = np.array(patch_pil).astype(np.float32) / 255.0\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to resize patch: {e}, patch shape: {patch.shape}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert back to tensor in the original format\n",
    "        patch_tensor = torch.from_numpy(patch).permute(2, 0, 1)  # HWC -> CHW\n",
    "        patches.append((region_score, patch_tensor, (img_i, img_j)))\n",
    "    \n",
    "    # Sort by activation score\n",
    "    patches.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "# Function to gather max activating samples\n",
    "def gather_max_activating_samples(model, sae, layer_number, feature_indices, \n",
    "                                 iterations=10, batch_size=32, num_envs=8, \n",
    "                                 episode_length=150, top_k=4, diversity_weight=2.0, \n",
    "                                 patch_size=16, use_decorrelation=False):\n",
    "    \"\"\"\n",
    "    Gather patches from the environment that maximally activate each SAE feature.\n",
    "    \"\"\"\n",
    "    # Set up model activations\n",
    "    model_activations = ModelActivations(model)\n",
    "    \n",
    "    # Attach SAE to the model\n",
    "    sae_hook = replace_layer_with_sae(model, sae, layer_number)\n",
    "    \n",
    "    # Set up hooks to capture SAE activations\n",
    "    sae_activations = {}\n",
    "    \n",
    "    def sae_activation_hook(module, input, output):\n",
    "        if isinstance(output, tuple) and len(output) >= 3:\n",
    "            sae_activations['features'] = output[2]\n",
    "        return output\n",
    "    \n",
    "    # Register hook on the SAE\n",
    "    hook = sae.register_forward_hook(sae_activation_hook)\n",
    "    \n",
    "    best_samples = {}\n",
    "    total_samples = 0\n",
    "\n",
    "    def cosine_similarity(patch1, patch2):\n",
    "        \"\"\"Compute cosine similarity between two patches\"\"\"\n",
    "        vec1 = patch1.flatten()\n",
    "        vec2 = patch2.flatten()\n",
    "        norm1 = np.linalg.norm(vec1) + 1e-8\n",
    "        norm2 = np.linalg.norm(vec2) + 1e-8\n",
    "        sim = np.dot(vec1, vec2) / (norm1 * norm2)\n",
    "        return sim\n",
    "\n",
    "    # Initialize best_samples dictionary\n",
    "    for c in feature_indices:\n",
    "        best_samples[c] = []\n",
    "\n",
    "    # Create environment\n",
    "    venv = gym.make('procgen:procgen-heist-v0')\n",
    "    \n",
    "    try:\n",
    "        for it in range(iterations):\n",
    "            print(f\"\\nIteration {it+1}/{iterations}\")\n",
    "            \n",
    "            # Collect batch of observations\n",
    "            observations = []\n",
    "            for _ in range(batch_size):\n",
    "                obs = venv.reset()\n",
    "                observations.append(obs)\n",
    "            \n",
    "            # Convert to tensor\n",
    "            batch_obs = torch.tensor(np.array(observations), dtype=torch.float32).to(device)\n",
    "            batch_obs = batch_obs.permute(0, 3, 1, 2)  # NHWC -> NCHW\n",
    "            \n",
    "            # Apply whitening if using decorrelation\n",
    "            if use_decorrelation:\n",
    "                # Normalize to [0,1]\n",
    "                normalized_obs = batch_obs / 255.0\n",
    "                \n",
    "                # Apply whitening\n",
    "                whitened_obs = apply_whitening(normalized_obs, whitening_matrix, mean_color)\n",
    "                \n",
    "                # Rescale to [0,1] range for the model\n",
    "                whitened_min = whitened_obs.min()\n",
    "                whitened_max = whitened_obs.max()\n",
    "                model_input = (whitened_obs - whitened_min) / (whitened_max - whitened_min)\n",
    "            else:\n",
    "                # Just normalize\n",
    "                model_input = batch_obs / 255.0\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                _ = model(model_input)\n",
    "            \n",
    "            # Get SAE activations\n",
    "            if 'features' not in sae_activations:\n",
    "                print(\"No SAE activations captured!\")\n",
    "                continue\n",
    "                \n",
    "            batch_acts = sae_activations['features']\n",
    "            \n",
    "            print(f\"Batch activations shape: {batch_acts.shape}\")\n",
    "            print(f\"Batch observations shape: {batch_obs.shape}\")\n",
    "            \n",
    "            # Process each sample in the batch\n",
    "            for b in range(batch_acts.shape[0]):\n",
    "                total_samples += 1\n",
    "                for c in feature_indices:\n",
    "                    # Get activation map for this feature\n",
    "                    activation_map = batch_acts[b, c].detach().cpu().numpy()\n",
    "                    observation = batch_obs[b]\n",
    "                    \n",
    "                    # Extract patches around high activation regions\n",
    "                    patches = extract_activation_patches(activation_map, observation, \n",
    "                                                      patch_size=patch_size)\n",
    "                    \n",
    "                    if not patches:\n",
    "                        continue\n",
    "                    \n",
    "                    # Process top patches\n",
    "                    for raw_score, patch, center in patches[:top_k*2]:  # Get more candidates for diversity\n",
    "                        # For the first sample, just use raw score\n",
    "                        if len(best_samples[c]) == 0:\n",
    "                            effective_score = raw_score\n",
    "                        else:\n",
    "                            # Compute average similarity to existing samples\n",
    "                            candidate_img = patch.detach().cpu().numpy()\n",
    "                            similarities = []\n",
    "                            for _, _, stored_patch, _ in best_samples[c]:\n",
    "                                stored_img = stored_patch.detach().cpu().numpy()\n",
    "                                sim = cosine_similarity(candidate_img, stored_img)\n",
    "                                similarities.append(sim)\n",
    "                            \n",
    "                            avg_similarity = np.mean(similarities)\n",
    "                            # Reward dissimilarity\n",
    "                            diversity_bonus = diversity_weight * (1 - avg_similarity)\n",
    "                            effective_score = raw_score + diversity_bonus\n",
    "                        \n",
    "                        # Update best samples\n",
    "                        if len(best_samples[c]) < top_k:\n",
    "                            best_samples[c].append((effective_score, raw_score, patch.clone(), center))\n",
    "                        else:\n",
    "                            min_effective_score = min(best_samples[c], key=lambda x: x[0])[0]\n",
    "                            if effective_score > min_effective_score:\n",
    "                                min_idx = np.argmin([s[0] for s in best_samples[c]])\n",
    "                                best_samples[c][min_idx] = (effective_score, raw_score, patch.clone(), center)\n",
    "\n",
    "            print(f\"Total samples processed: {total_samples}\")\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        hook.remove()\n",
    "        sae_hook.remove()\n",
    "        model_activations.clear_hooks()\n",
    "        venv.close()\n",
    "\n",
    "    # Sort by effective score and keep top_k\n",
    "    for c in best_samples:\n",
    "        best_samples[c] = sorted(best_samples[c], key=lambda x: x[0], reverse=True)\n",
    "        best_samples[c] = best_samples[c][:top_k]\n",
    "    \n",
    "    return best_samples\n",
    "\n",
    "# Function to visualize max activations\n",
    "def visualize_max_activations(best_samples, target_layer_name, use_decorrelation=False):\n",
    "    \"\"\"Visualize the patches that maximally activate each feature.\"\"\"\n",
    "    print(f\"Visualizing samples for {len(best_samples)} features\")\n",
    "    \n",
    "    # Create directory for saving visualizations\n",
    "    os.makedirs('max_activations', exist_ok=True)\n",
    "    \n",
    "    for c, samples in best_samples.items():\n",
    "        num_samples = len(samples)\n",
    "        if num_samples == 0:\n",
    "            print(f\"Skipping feature {c} - no samples found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Feature {c}: Found {num_samples} samples\")\n",
    "        fig, axs = plt.subplots(1, num_samples, figsize=(3 * num_samples, 3))\n",
    "        if num_samples == 1:\n",
    "            axs = [axs]\n",
    "        \n",
    "        for i, (effective_score, raw_score, patch, center) in enumerate(samples):\n",
    "            img = patch.detach().cpu().numpy()  # Already in CHW format\n",
    "            img = img.transpose(1, 2, 0)  # Convert to HWC for plotting\n",
    "            \n",
    "            axs[i].imshow(img)\n",
    "            axs[i].set_title(f\"Raw: {raw_score:.2f}\\nEff: {effective_score:.2f}\\nPos: {center}\")\n",
    "            axs[i].axis('off')\n",
    "            \n",
    "            # Save individual patch\n",
    "            plt.imsave(f'max_activations/feature_{c}_patch_{i}.png', img)\n",
    "            \n",
    "        # Add title with decorrelation info\n",
    "        title = f\"Feature {c}: Max Activating Patches for {target_layer_name}\"\n",
    "        if use_decorrelation:\n",
    "            title += \" (Decorrelated)\"\n",
    "        fig.suptitle(title)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'max_activations/feature_{c}_patches.png')\n",
    "        plt.close(fig)\n",
    "\n",
    "# Function to visualize feature with decorrelation\n",
    "def visualize_feature_with_decorrelation(model, sae, layer_name, feature_idx, \n",
    "                                        num_steps=1000, use_decorrelation=False):\n",
    "    \"\"\"Generate a visualization of what maximally activates a specific SAE feature.\"\"\"\n",
    "    # Attach SAE to the model\n",
    "    sae_hook = replace_layer_with_sae(model, sae, layer_number)\n",
    "    \n",
    "    # Set up hooks to capture activations\n",
    "    activations = {}\n",
    "    \n",
    "    def sae_activation_hook(module, input, output):\n",
    "        if isinstance(output, tuple) and len(output) >= 3:\n",
    "            activations['sae_features'] = output[2]\n",
    "        return output\n",
    "    \n",
    "    # Register hook on the SAE\n",
    "    hook = sae.register_forward_hook(sae_activation_hook)\n",
    "    \n",
    "    # Initialize image\n",
    "    padded_size = 64 + 8  # 4 pixels on each side\n",
    "    input_img = torch.randint(0, 256, (1, 3, padded_size, padded_size), \n",
    "                            device=device, \n",
    "                            dtype=torch.float32).requires_grad_(True)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.Adam([input_img], lr=0.08)\n",
    "    \n",
    "    # Parameters for transformations\n",
    "    scales = [1.0, 0.975, 1.025, 0.95, 1.05]\n",
    "    angles = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "    \n",
    "    best_activation = float('-inf')\n",
    "    best_img = None\n",
    "    \n",
    "    try:\n",
    "        # Optimization loop\n",
    "        pbar = tqdm(range(num_steps))\n",
    "        for step in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Create a copy for transformations\n",
    "            processed_img = input_img.clone()\n",
    "            \n",
    "            # Apply transformations\n",
    "            ox, oy = np.random.randint(-8, 9, 2)\n",
    "            processed_img = jitter(processed_img, ox, oy)\n",
    "            processed_img = random_scale(processed_img, scales)\n",
    "            processed_img = random_rotate(processed_img, angles)\n",
    "            ox, oy = np.random.randint(-4, 5, 2)\n",
    "            processed_img = jitter(processed_img, ox, oy)\n",
    "            \n",
    "            # Crop padding\n",
    "            processed_img = processed_img[:, :, 4:-4, 4:-4]\n",
    "            \n",
    "            # Ensure values are in [0, 255]\n",
    "            processed_img.data.clamp_(0, 255)\n",
    "            \n",
    "            # Normalize to [0,1] for model input\n",
    "            normalized_img = processed_img / 255.0\n",
    "            \n",
    "            # Apply whitening if using decorrelation\n",
    "            if use_decorrelation:\n",
    "                # Apply whitening\n",
    "                whitened_img = apply_whitening(normalized_img, whitening_matrix, mean_color)\n",
    "                \n",
    "                # Rescale to [0,1] range for the model\n",
    "                whitened_min = whitened_img.min()\n",
    "                whitened_max = whitened_img.max()\n",
    "                model_input = (whitened_img - whitened_min) / (whitened_max - whitened_min)\n",
    "            else:\n",
    "                # Use the normalized image directly\n",
    "                model_input = normalized_img\n",
    "            \n",
    "            # Forward pass\n",
    "            _ = model(model_input)\n",
    "            \n",
    "            # Get the activation for the target feature\n",
    "            if 'sae_features' in activations:\n",
    "                feature_activation = activations['sae_features'][0, feature_idx]\n",
    "                activation_loss = -feature_activation.mean()  # negative because we want to maximize\n",
    "            else:\n",
    "                activation_loss = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            # Calculate regularization losses\n",
    "            tv_loss = 1e-3 * total_variation(input_img / 255.0)\n",
    "            l2_loss = 1e-3 * torch.norm(input_img / 255.0)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = activation_loss + tv_loss + l2_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Post-processing\n",
    "            with torch.no_grad():\n",
    "                input_img.data.clamp_(0, 255)\n",
    "                \n",
    "                # Track best activation\n",
    "                current_activation = -activation_loss.item()\n",
    "                if current_activation > best_activation:\n",
    "                    best_activation = current_activation\n",
    "                    best_img = processed_img.clone()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': loss.item(), 'act': current_activation})\n",
    "        \n",
    "        # Process the final image\n",
    "        result_img = best_img / 255.0  # Normalize to [0,1]\n",
    "        \n",
    "        # If we used decorrelation, apply it to the final image for visualization\n",
    "        if use_decorrelation:\n",
    "            # Apply whitening\n",
    "            whitened_img = apply_whitening(result_img, whitening_matrix, mean_color)\n",
    "            \n",
    "            # Apply unwhitening to get back to normal color space\n",
    "            unwhitened_img = apply_unwhitening(whitened_img, unwhitening_matrix, mean_color)\n",
    "            \n",
    "            # Ensure values are in [0,1]\n",
    "            result_img = torch.clamp(unwhitened_img, 0, 1)\n",
    "        \n",
    "        # Convert to numpy for display\n",
    "        result = result_img.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        \n",
    "        return result, best_activation\n",
    "    \n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        hook.remove()\n",
    "        sae_hook.remove()\n",
    "\n",
    "# Main execution\n",
    "# Select features to visualize\n",
    "feature_indices = list(range(8))  # Start with first 8 features\n",
    "\n",
    "# Create directory for saving visualizations\n",
    "os.makedirs('visualizations', exist_ok=True)\n",
    "\n",
    "# First, generate synthetic visualizations for each feature\n",
    "print(\"Generating synthetic visualizations...\")\n",
    "for feature_idx in feature_indices:\n",
    "    print(f\"\\nVisualizing feature {feature_idx}\")\n",
    "    \n",
    "    # Standard visualization\n",
    "    print(\"Standard visualization...\")\n",
    "    vis_standard, act_standard = visualize_feature_with_decorrelation(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        layer_name=layer_name,\n",
    "        feature_idx=feature_idx,\n",
    "        num_steps=1000,\n",
    "        use_decorrelation=False\n",
    "    )\n",
    "    \n",
    "    # Decorrelated visualization\n",
    "    print(\"Decorrelated visualization...\")\n",
    "    vis_decorrelated, act_decorrelated = visualize_feature_with_decorrelation(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        layer_name=layer_name,\n",
    "        feature_idx=feature_idx,\n",
    "        num_steps=1000,\n",
    "        use_decorrelation=True\n",
    "    )\n",
    "    \n",
    "    # Save individual visualizations\n",
    "    plt.imsave(f'visualizations/standard_feature_{feature_idx}.png', vis_standard)\n",
    "    plt.imsave(f'visualizations/decorrelated_feature_{feature_idx}.png', vis_decorrelated)\n",
    "\n",
    "# Now, gather max activating samples from the environment\n",
    "print(\"\\nGathering max activating samples...\")\n",
    "best_samples_standard = gather_max_activating_samples(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    layer_number=layer_number,\n",
    "    feature_indices=feature_indices,\n",
    "    iterations=5,\n",
    "    batch_size=16,\n",
    "    top_k=4,\n",
    "    patch_size=32,\n",
    "    use_decorrelation=False\n",
    ")\n",
    "\n",
    "best_samples_decorrelated = gather_max_activating_samples(\n",
    "    model=model,\n",
    "    sae=sae,\n",
    "    layer_number=layer_number,\n",
    "    feature_indices=feature_indices,\n",
    "    iterations=5,\n",
    "    batch_size=16,\n",
    "    top_k=4,\n",
    "    patch_size=32,\n",
    "    use_decorrelation=True\n",
    ")\n",
    "\n",
    "# Visualize the max activating samples\n",
    "print(\"\\nVisualizing max activating samples...\")\n",
    "visualize_max_activations(best_samples_standard, layer_name, use_decorrelation=False)\n",
    "visualize_max_activations(best_samples_decorrelated, layer_name, use_decorrelation=True)\n",
    "\n",
    "print(\"All visualizations completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.helpers import load_interpretable_model\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Import visualization utilities\n",
    "from feature_vis_impala import (\n",
    "    total_variation, \n",
    "    jitter, \n",
    "    random_scale, \n",
    "    random_rotate\n",
    ")\n",
    "\n",
    "# Import SAE-related modules\n",
    "from sae_cnn import ConvSAE\n",
    "from extract_sae_features import replace_layer_with_sae\n",
    "from feature_vis_sae import load_sae_from_checkpoint\n",
    "\n",
    "# Import color decorrelation functions\n",
    "from color_decorrelation import (\n",
    "    apply_whitening,\n",
    "    apply_unwhitening,\n",
    "    load_color_matrices\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = load_interpretable_model()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load SAE model\n",
    "sae_checkpoint_path = \"../checkpoints/sae_checkpoint_step_4500000.pt\"\n",
    "sae_result = load_sae_from_checkpoint(sae_checkpoint_path, device)\n",
    "sae = sae_result[0]  # Access the SAE model\n",
    "layer_name = \"conv4a\"  # Target layer\n",
    "layer_number = 8\n",
    "\n",
    "# Load color matrices\n",
    "color_matrices_path = \"color_matrices.pt\"\n",
    "if os.path.exists(color_matrices_path):\n",
    "    cov_matrix, whitening_matrix, unwhitening_matrix, mean_color = load_color_matrices(color_matrices_path, device)\n",
    "    print(\"Loaded color matrices successfully\")\n",
    "else:\n",
    "    print(\"Color matrices not found, using identity matrices\")\n",
    "    whitening_matrix = torch.eye(3, device=device)\n",
    "    unwhitening_matrix = torch.eye(3, device=device)\n",
    "    mean_color = torch.zeros(3, device=device)\n",
    "\n",
    "def generate_synthetic_image(model, sae, feature_idx, num_steps=1000, use_decorrelation=False):\n",
    "    \"\"\"\n",
    "    Generate a synthetic image that maximally activates a specific SAE feature.\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        sae: The SAE model\n",
    "        feature_idx: Index of the feature to visualize\n",
    "        num_steps: Number of optimization steps\n",
    "        use_decorrelation: Whether to use color decorrelation\n",
    "        \n",
    "    Returns:\n",
    "        image: The optimized image as a tensor (1, 3, 64, 64)\n",
    "        activation: The activation value achieved\n",
    "    \"\"\"\n",
    "    print(f\"Generating synthetic image for feature {feature_idx}\")\n",
    "    print(f\"Using decorrelation: {use_decorrelation}\")\n",
    "    \n",
    "    # Attach SAE to the model\n",
    "    sae_hook = replace_layer_with_sae(model, sae, layer_number)\n",
    "    \n",
    "    # Set up hooks to capture activations\n",
    "    activations = {}\n",
    "    \n",
    "    def sae_activation_hook(module, input, output):\n",
    "        if isinstance(output, tuple) and len(output) >= 3:\n",
    "            activations['sae_features'] = output[2]\n",
    "        return output\n",
    "    \n",
    "    # Register hook on the SAE\n",
    "    hook = sae.register_forward_hook(sae_activation_hook)\n",
    "    \n",
    "    try:\n",
    "        # Initialize a random image with padding\n",
    "        padded_size = 64 + 8  # 4 pixels padding on each side\n",
    "        input_img = torch.randint(0, 256, (1, 3, padded_size, padded_size), \n",
    "                                device=device, \n",
    "                                dtype=torch.float32).requires_grad_(True)\n",
    "        \n",
    "        # Set up optimizer\n",
    "        optimizer = torch.optim.Adam([input_img], lr=0.08)\n",
    "        \n",
    "        # Parameters for transformations\n",
    "        scales = [1.0, 0.975, 1.025, 0.95, 1.05]\n",
    "        angles = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "        \n",
    "        best_activation = float('-inf')\n",
    "        best_img = None\n",
    "        \n",
    "        # Optimization loop\n",
    "        pbar = tqdm(range(num_steps))\n",
    "        for step in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Create a copy for transformations\n",
    "            processed_img = input_img.clone()\n",
    "            \n",
    "            # Apply transformations\n",
    "            ox, oy = np.random.randint(-4, 5, 2)\n",
    "            processed_img = jitter(processed_img, ox, oy)\n",
    "            processed_img = random_scale(processed_img, scales)\n",
    "            processed_img = random_rotate(processed_img, angles)\n",
    "            ox, oy = np.random.randint(-2, 3, 2)\n",
    "            processed_img = jitter(processed_img, ox, oy)\n",
    "            \n",
    "            # Crop padding\n",
    "            processed_img = processed_img[:, :, 4:-4, 4:-4]\n",
    "            \n",
    "            # Ensure values are in [0, 255]\n",
    "            processed_img.data.clamp_(0, 255)\n",
    "            \n",
    "            # Normalize to [0,1] for model input\n",
    "            normalized_img = processed_img / 255.0\n",
    "            \n",
    "            # Apply whitening if using decorrelation\n",
    "            if use_decorrelation:\n",
    "                # Apply whitening\n",
    "                whitened_img = apply_whitening(normalized_img, whitening_matrix, mean_color)\n",
    "                \n",
    "                # Rescale to [0,1] range for the model\n",
    "                whitened_min = whitened_img.min()\n",
    "                whitened_max = whitened_img.max()\n",
    "                model_input = (whitened_img - whitened_min) / (whitened_max - whitened_min)\n",
    "            else:\n",
    "                # Use the normalized image directly\n",
    "                model_input = normalized_img\n",
    "            \n",
    "            # Forward pass\n",
    "            _ = model(model_input)\n",
    "            \n",
    "            # Get the activation for the target feature\n",
    "            if 'sae_features' in activations:\n",
    "                feature_activation = activations['sae_features'][0, feature_idx]\n",
    "                activation_loss = -feature_activation.mean()  # negative because we want to maximize\n",
    "            else:\n",
    "                activation_loss = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            # Calculate regularization losses\n",
    "            tv_loss = 1e-3 * total_variation(input_img / 255.0)\n",
    "            l2_loss = 1e-3 * torch.norm(input_img / 255.0)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = activation_loss + tv_loss + l2_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Post-processing\n",
    "            with torch.no_grad():\n",
    "                input_img.data.clamp_(0, 255)\n",
    "                \n",
    "                # Track best activation\n",
    "                current_activation = -activation_loss.item()\n",
    "                if current_activation > best_activation:\n",
    "                    best_activation = current_activation\n",
    "                    best_img = processed_img.clone()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': loss.item(), 'act': current_activation})\n",
    "        \n",
    "        # Process the final image\n",
    "        result_img = best_img / 255.0  # Normalize to [0,1]\n",
    "        \n",
    "        # If we used decorrelation, apply it to the final image for visualization\n",
    "        if use_decorrelation:\n",
    "            # Apply whitening\n",
    "            whitened_img = apply_whitening(result_img, whitening_matrix, mean_color)\n",
    "            \n",
    "            # Apply unwhitening to get back to normal color space\n",
    "            unwhitened_img = apply_unwhitening(whitened_img, unwhitening_matrix, mean_color)\n",
    "            \n",
    "            # Ensure values are in [0,1]\n",
    "            result_img = torch.clamp(unwhitened_img, 0, 1)\n",
    "        \n",
    "        return result_img, best_activation\n",
    "        \n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        hook.remove()\n",
    "        sae_hook.remove()\n",
    "\n",
    "def get_feature_activation_map(model, sae, image, feature_idx):\n",
    "    \"\"\"\n",
    "    Get the activation map for a specific feature when running the image through the model.\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        sae: The SAE model\n",
    "        image: Input image tensor (1, 3, H, W)\n",
    "        feature_idx: Index of the feature to get activations for\n",
    "        \n",
    "    Returns:\n",
    "        activation_map: 2D numpy array of activations\n",
    "    \"\"\"\n",
    "    # Attach SAE to the model\n",
    "    sae_hook = replace_layer_with_sae(model, sae, layer_number)\n",
    "    \n",
    "    # Set up hooks to capture activations\n",
    "    activations = {}\n",
    "    \n",
    "    def sae_activation_hook(module, input, output):\n",
    "        if isinstance(output, tuple) and len(output) >= 3:\n",
    "            activations['sae_features'] = output[2]\n",
    "        return output\n",
    "    \n",
    "    # Register hook on the SAE\n",
    "    hook = sae.register_forward_hook(sae_activation_hook)\n",
    "    \n",
    "    try:\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            _ = model(image)\n",
    "        \n",
    "        # Get the activation map for the target feature\n",
    "        if 'sae_features' in activations:\n",
    "            feature_map = activations['sae_features'][0, feature_idx]\n",
    "            return feature_map.detach().cpu().numpy()\n",
    "        else:\n",
    "            raise ValueError(\"No activations captured\")\n",
    "    \n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        hook.remove()\n",
    "        sae_hook.remove()\n",
    "\n",
    "def find_high_activation_regions(activation_map, top_k=3):\n",
    "    \"\"\"\n",
    "    Find regions with highest activation in the activation map.\n",
    "    \n",
    "    Args:\n",
    "        activation_map: 2D numpy array of activations\n",
    "        top_k: Number of top regions to return\n",
    "        \n",
    "    Returns:\n",
    "        regions: List of dictionaries with region information\n",
    "    \"\"\"\n",
    "    act_h, act_w = activation_map.shape\n",
    "    flat_activations = activation_map.flatten()\n",
    "    \n",
    "    # Find top-k activation points\n",
    "    top_indices = np.argsort(flat_activations)[-top_k:]\n",
    "    \n",
    "    regions = []\n",
    "    for idx in top_indices:\n",
    "        # Convert flat index to 2D coordinates\n",
    "        i, j = idx // act_w, idx % act_w\n",
    "        \n",
    "        # Get activation score\n",
    "        score = flat_activations[idx]\n",
    "        \n",
    "        regions.append({\n",
    "            'act_coords': (i, j),\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "    # Sort by score (highest first)\n",
    "    regions.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    return regions\n",
    "\n",
    "def map_activation_to_image_coords(region, act_shape, img_shape):\n",
    "    \"\"\"\n",
    "    Map coordinates from activation space to image space.\n",
    "    \n",
    "    Args:\n",
    "        region: Dictionary with region information\n",
    "        act_shape: Shape of the activation map (H, W)\n",
    "        img_shape: Shape of the image (H, W)\n",
    "        \n",
    "    Returns:\n",
    "        img_coords: Tuple (i, j) of image coordinates\n",
    "    \"\"\"\n",
    "    act_h, act_w = act_shape\n",
    "    img_h, img_w = img_shape\n",
    "    \n",
    "    # Scale factors\n",
    "    scale_h = img_h / act_h\n",
    "    scale_w = img_w / act_w\n",
    "    \n",
    "    # Get activation coordinates\n",
    "    act_i, act_j = region['act_coords']\n",
    "    \n",
    "    # Convert to image coordinates\n",
    "    img_i = int(act_i * scale_h)\n",
    "    img_j = int(act_j * scale_w)\n",
    "    \n",
    "    return (img_i, img_j)\n",
    "\n",
    "def extract_patch(image, center_coords, patch_size=24):\n",
    "    \"\"\"\n",
    "    Extract a patch from the image centered at the given coordinates.\n",
    "    \n",
    "    Args:\n",
    "        image: Image tensor (1, 3, H, W)\n",
    "        center_coords: Tuple (i, j) of center coordinates\n",
    "        patch_size: Size of the patch to extract\n",
    "        \n",
    "    Returns:\n",
    "        patch: Extracted patch tensor (1, 3, patch_size, patch_size)\n",
    "    \"\"\"\n",
    "    img_i, img_j = center_coords\n",
    "    half_size = patch_size // 2\n",
    "    \n",
    "    # Calculate patch boundaries\n",
    "    start_i = max(0, img_i - half_size)\n",
    "    start_j = max(0, img_j - half_size)\n",
    "    end_i = min(image.shape[2], img_i + half_size)\n",
    "    end_j = min(image.shape[3], img_j + half_size)\n",
    "    \n",
    "    # Extract patch\n",
    "    patch = image[:, :, start_i:end_i, start_j:end_j]\n",
    "    \n",
    "    # Resize to patch_size if needed\n",
    "    if patch.shape[2] != patch_size or patch.shape[3] != patch_size:\n",
    "        patch = F.interpolate(patch, size=(patch_size, patch_size), mode='bilinear', align_corners=False)\n",
    "    \n",
    "    return patch\n",
    "\n",
    "def optimize_patch(model, sae, feature_idx, initial_patch, center_coords, num_steps=2000, use_decorrelation=False):\n",
    "    \"\"\"\n",
    "    Optimize a patch to maximally activate a specific feature.\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        sae: The SAE model\n",
    "        feature_idx: Index of the feature to optimize for\n",
    "        initial_patch: Initial patch tensor (1, 3, patch_size, patch_size)\n",
    "        center_coords: Tuple (i, j) of center coordinates in the full image\n",
    "        num_steps: Number of optimization steps\n",
    "        use_decorrelation: Whether to use color decorrelation\n",
    "        \n",
    "    Returns:\n",
    "        optimized_patch: The optimized patch tensor (1, 3, patch_size, patch_size)\n",
    "    \"\"\"\n",
    "    print(f\"Optimizing patch for feature {feature_idx} at coordinates {center_coords}\")\n",
    "    \n",
    "    # Get patch size\n",
    "    patch_size = initial_patch.shape[2]\n",
    "    \n",
    "    # Create a padded version for transformations\n",
    "    padded_size = patch_size + 8\n",
    "    padded_patch = F.pad(initial_patch, (4, 4, 4, 4), mode='reflect')\n",
    "    padded_patch = padded_patch.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Attach SAE to the model\n",
    "    sae_hook = replace_layer_with_sae(model, sae, layer_number)\n",
    "    \n",
    "    # Set up hooks to capture activations\n",
    "    activations = {}\n",
    "    \n",
    "    def sae_activation_hook(module, input, output):\n",
    "        if isinstance(output, tuple) and len(output) >= 3:\n",
    "            activations['sae_features'] = output[2]\n",
    "        return output\n",
    "    \n",
    "    # Register hook on the SAE\n",
    "    hook = sae.register_forward_hook(sae_activation_hook)\n",
    "    \n",
    "    # Calculate activation map coordinates\n",
    "    img_h, img_w = 64, 64\n",
    "    act_h, act_w = 8, 8  # Assuming 8x8 feature maps for conv4a\n",
    "    scale_h = img_h / act_h\n",
    "    scale_w = img_w / act_w\n",
    "    \n",
    "    act_i = int(center_coords[0] / scale_h)\n",
    "    act_j = int(center_coords[1] / scale_w)\n",
    "    \n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.Adam([padded_patch], lr=0.08)\n",
    "    \n",
    "    # Parameters for transformations\n",
    "    scales = [1.0, 0.975, 1.025, 0.95, 1.05]\n",
    "    angles = [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]\n",
    "    \n",
    "    best_activation = float('-inf')\n",
    "    best_patch = None\n",
    "    \n",
    "    try:\n",
    "        # Optimization loop\n",
    "        pbar = tqdm(range(num_steps))\n",
    "        for step in pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Create a copy for transformations\n",
    "            processed_patch = padded_patch.clone()\n",
    "\n",
    "            # Apply transformations\n",
    "            ox, oy = np.random.randint(-4, 5, 2)\n",
    "            processed_patch = jitter(processed_patch, ox, oy)\n",
    "            processed_patch = random_scale(processed_patch, scales)\n",
    "            processed_patch = random_rotate(processed_patch, angles)\n",
    "            ox, oy = np.random.randint(-2, 3, 2)\n",
    "            processed_patch = jitter(processed_patch, ox, oy)\n",
    "\n",
    "            # Crop padding\n",
    "            processed_patch = processed_patch[:, :, 4:-4, 4:-4]\n",
    "\n",
    "            # Clamp values to [0, 1]\n",
    "            processed_patch.data.clamp_(0, 1)\n",
    "\n",
    "            # Explicitly enforce size consistency after transformations\n",
    "            processed_patch = F.interpolate(\n",
    "                processed_patch,\n",
    "                size=(patch_size, patch_size),\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # Create a full-sized image with the patch\n",
    "            full_img = torch.zeros((1, 3, 64, 64), device=device)\n",
    "\n",
    "            # Calculate where to place the patch\n",
    "            img_i, img_j = center_coords\n",
    "            half_size = patch_size // 2\n",
    "\n",
    "            # Ensure the patch fits within the image bounds\n",
    "            start_i = max(0, img_i - half_size)\n",
    "            start_j = max(0, img_j - half_size)\n",
    "            end_i = min(64, start_i + patch_size)\n",
    "            end_j = min(64, start_j + patch_size)\n",
    "\n",
    "            # Calculate the actual size of the region\n",
    "            region_height = end_i - start_i\n",
    "            region_width = end_j - start_j\n",
    "\n",
    "            # Ensure patch matches region size exactly\n",
    "            if (region_height != patch_size) or (region_width != patch_size):\n",
    "                patch_to_place = F.interpolate(\n",
    "                    processed_patch,\n",
    "                    size=(region_height, region_width),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False\n",
    "                )\n",
    "            else:\n",
    "                patch_to_place = processed_patch\n",
    "\n",
    "            # Place the patch in the full image\n",
    "            full_img[:, :, start_i:end_i, start_j:end_j] = patch_to_place\n",
    "\n",
    "            # Forward pass\n",
    "            _ = model(full_img)\n",
    "\n",
    "            # Get activation and compute loss\n",
    "            if 'sae_features' in activations:\n",
    "                feature_map = activations['sae_features'][0, feature_idx]\n",
    "                radius = 1\n",
    "                act_start_i = max(0, act_i - radius)\n",
    "                act_start_j = max(0, act_j - radius)\n",
    "                act_end_i = min(act_h, act_i + radius + 1)\n",
    "                act_end_j = min(act_w, act_j + radius + 1)\n",
    "                region_activation = feature_map[act_start_i:act_end_i, act_start_j:act_end_j]\n",
    "                activation_loss = -region_activation.mean()\n",
    "            else:\n",
    "                activation_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            # Regularization losses\n",
    "            tv_loss = 1e-3 * total_variation(padded_patch)\n",
    "            l2_loss = 1e-3 * torch.norm(padded_patch)\n",
    "\n",
    "            # Total loss\n",
    "            loss = activation_loss + tv_loss + l2_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Post-processing\n",
    "            with torch.no_grad():\n",
    "                padded_patch.data.clamp_(0, 1)\n",
    "\n",
    "                current_activation = -activation_loss.item()\n",
    "                if current_activation > best_activation:\n",
    "                    best_activation = current_activation\n",
    "                    best_patch = processed_patch.clone()\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': loss.item(), 'act': current_activation})\n",
    "        \n",
    "        # Process the final patch\n",
    "        result_patch = best_patch.clone()\n",
    "        \n",
    "        # If we used decorrelation, apply it to the final patch for visualization\n",
    "        if use_decorrelation:\n",
    "            # Create a full image with the patch\n",
    "            full_img = torch.zeros((1, 3, 64, 64), device=device)\n",
    "            \n",
    "            # Resize the patch to match the region exactly\n",
    "            patch_to_place = F.interpolate(\n",
    "                result_patch, \n",
    "                size=(region_height, region_width), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "            \n",
    "            # Place the patch in the full image\n",
    "            full_img[:, :, start_i:end_i, start_j:end_j] = patch_to_place\n",
    "            \n",
    "            # Apply whitening\n",
    "            whitened_img = apply_whitening(full_img, whitening_matrix, mean_color)\n",
    "            \n",
    "            # Apply unwhitening to get back to normal color space\n",
    "            unwhitened_img = apply_unwhitening(whitened_img, unwhitening_matrix, mean_color)\n",
    "            \n",
    "            # Extract the patch again\n",
    "            result_patch = unwhitened_img[:, :, start_i:end_i, start_j:end_j]\n",
    "            \n",
    "            # Ensure values are in [0,1]\n",
    "            result_patch = torch.clamp(result_patch, 0, 1)\n",
    "        \n",
    "        # Resize to standard patch size if needed\n",
    "        if result_patch.shape[2] != patch_size or result_patch.shape[3] != patch_size:\n",
    "            result_patch = F.interpolate(\n",
    "                result_patch, \n",
    "                size=(patch_size, patch_size), \n",
    "                mode='bilinear', \n",
    "                align_corners=False\n",
    "            )\n",
    "        \n",
    "        return result_patch\n",
    "        \n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        hook.remove()\n",
    "        sae_hook.remove()\n",
    "\n",
    "def generate_and_optimize_patches(model, sae, feature_idx, num_steps=1000, use_decorrelation=False):\n",
    "    \"\"\"\n",
    "    Generate synthetic images and optimize specific regions that maximally activate a feature.\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        sae: The SAE model\n",
    "        feature_idx: Index of the feature to visualize\n",
    "        num_steps: Total number of optimization steps\n",
    "        use_decorrelation: Whether to use color decorrelation\n",
    "        \n",
    "    Returns:\n",
    "        patches: List of tuples (patch, score, coordinates)\n",
    "    \"\"\"\n",
    "    # Step 1: Generate initial full-size synthetic image\n",
    "    print(f\"Step 1: Generating initial synthetic image for feature {feature_idx}\")\n",
    "    initial_image, _ = generate_synthetic_image(model, sae, feature_idx, num_steps=num_steps//2, use_decorrelation=use_decorrelation)\n",
    "    \n",
    "    # Step 2: Identify high-activation regions\n",
    "    print(f\"Step 2: Identifying high-activation regions\")\n",
    "    activation_map = get_feature_activation_map(model, sae, initial_image, feature_idx)\n",
    "    high_activation_regions = find_high_activation_regions(activation_map, top_k=3)\n",
    "    \n",
    "    print(f\"Found {len(high_activation_regions)} high-activation regions\")\n",
    "    for i, region in enumerate(high_activation_regions):\n",
    "        print(f\"  Region {i+1}: coords={region['act_coords']}, score={region['score']:.4f}\")\n",
    "    \n",
    "    # Step 3: Extract and optimize patches for each high-activation region\n",
    "    print(f\"Step 3: Extracting and optimizing patches\")\n",
    "    optimized_patches = []\n",
    "    for i, region in enumerate(high_activation_regions):\n",
    "        # Extract coordinates in image space\n",
    "        img_coords = map_activation_to_image_coords(region, activation_map.shape, (64, 64))\n",
    "        print(f\"  Region {i+1}: activation coords={region['act_coords']}, image coords={img_coords}\")\n",
    "        \n",
    "        # Extract initial patch\n",
    "        initial_patch = extract_patch(initial_image, img_coords, patch_size=24)\n",
    "        \n",
    "        # Optimize just this patch\n",
    "        optimized_patch = optimize_patch(model, sae, feature_idx, initial_patch, img_coords, \n",
    "                                       num_steps=num_steps//2, use_decorrelation=use_decorrelation)\n",
    "        \n",
    "        optimized_patches.append((optimized_patch, region['score'], img_coords))\n",
    "    \n",
    "    return optimized_patches\n",
    "\n",
    "# Create directory for saving visualizations\n",
    "os.makedirs('synthetic_patches', exist_ok=True)\n",
    "\n",
    "# Select features to visualize\n",
    "feature_indices = list(range(8))  # Start with first 8 features\n",
    "\n",
    "# Generate and optimize patches for each feature\n",
    "for feature_idx in feature_indices:\n",
    "    print(f\"\\nProcessing feature {feature_idx}\")\n",
    "    \n",
    "    # Standard visualization (no decorrelation)\n",
    "    print(\"Generating standard patches...\")\n",
    "    standard_patches = generate_and_optimize_patches(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        feature_idx=feature_idx,\n",
    "        num_steps=1000,\n",
    "        use_decorrelation=False\n",
    "    )\n",
    "    \n",
    "    # Decorrelated visualization\n",
    "    print(\"Generating decorrelated patches...\")\n",
    "    decorrelated_patches = generate_and_optimize_patches(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        feature_idx=feature_idx,\n",
    "        num_steps=1000,\n",
    "        use_decorrelation=True\n",
    "    )\n",
    "    \n",
    "    # Create a figure to display the results\n",
    "    num_patches = len(standard_patches)\n",
    "    fig, axes = plt.subplots(2, num_patches, figsize=(4*num_patches, 8))\n",
    "    \n",
    "    # Display standard patches\n",
    "    for i, (patch, score, coords) in enumerate(standard_patches):\n",
    "        patch_np = patch.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        patch_np = np.clip(patch_np, 0, 1)\n",
    "        \n",
    "        axes[0, i].imshow(patch_np)\n",
    "        axes[0, i].set_title(f'Standard\\nScore: {score:.2f}\\nPos: {coords}')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Save individual patch\n",
    "        plt.imsave(f'synthetic_patches/feature_{feature_idx}_standard_patch_{i}.png', patch_np)\n",
    "    \n",
    "    # Display decorrelated patches\n",
    "    for i, (patch, score, coords) in enumerate(decorrelated_patches):\n",
    "        patch_np = patch.detach().cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "        patch_np = np.clip(patch_np, 0, 1)\n",
    "        \n",
    "        axes[1, i].imshow(patch_np)\n",
    "        axes[1, i].set_title(f'Decorrelated\\nScore: {score:.2f}\\nPos: {coords}')\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Save individual patch\n",
    "        plt.imsave(f'synthetic_patches/feature_{feature_idx}_decorrelated_patch_{i}.png', patch_np)\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle(f'Feature {feature_idx} Synthetic Patches', fontsize=16)\n",
    "    \n",
    "    # Save the combined figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'synthetic_patches/feature_{feature_idx}_patches.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "print(\"All synthetic patches generated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import necessary functions\n",
    "from utils.helpers import load_interpretable_model\n",
    "from sae_cnn import ConvSAE\n",
    "from extract_sae_features import replace_layer_with_sae\n",
    "from feature_vis_sae import load_sae_from_checkpoint\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = load_interpretable_model()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load SAE model - explicitly set layer_number instead of trying to get it from the function\n",
    "sae_checkpoint_path = \"../checkpoints/sae_checkpoint_step_4500000.pt\"\n",
    "sae = load_sae_from_checkpoint(sae_checkpoint_path, device)[0]  # Just get the SAE\n",
    "layer_name = \"conv4a\"  # Target layer\n",
    "layer_number = 8  # Explicitly set layer number\n",
    "\n",
    "def cosine_similarity(patch1, patch2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two patches.\n",
    "    \n",
    "    Args:\n",
    "        patch1: First patch tensor\n",
    "        patch2: Second patch tensor\n",
    "        \n",
    "    Returns:\n",
    "        similarity: Cosine similarity value\n",
    "    \"\"\"\n",
    "    # Flatten patches\n",
    "    vec1 = patch1.flatten()\n",
    "    vec2 = patch2.flatten()\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    norm1 = torch.norm(vec1) + 1e-8\n",
    "    norm2 = torch.norm(vec2) + 1e-8\n",
    "    sim = torch.dot(vec1, vec2) / (norm1 * norm2)\n",
    "    \n",
    "    return sim.item()\n",
    "\n",
    "def extract_activation_patches(activation_map, observation, patch_size=24, top_k=5):\n",
    "    \"\"\"\n",
    "    Extract patches around high-activation regions in the activation map.\n",
    "    \n",
    "    Args:\n",
    "        activation_map: 2D tensor of activations for a specific feature\n",
    "        observation: The corresponding input image (3, 64, 64)\n",
    "        patch_size: Size of patches to extract\n",
    "        top_k: Number of top activations to consider\n",
    "        \n",
    "    Returns:\n",
    "        patches: List of (score, patch, coordinates) tuples\n",
    "    \"\"\"\n",
    "    patches = []\n",
    "    act_h, act_w = activation_map.shape\n",
    "    \n",
    "    # Convert observation to numpy for easier handling\n",
    "    if observation.shape[0] == 3:  # CHW format\n",
    "        obs_np = observation.detach().cpu().numpy().transpose(1, 2, 0)  # Convert to HWC\n",
    "    else:\n",
    "        obs_np = observation.detach().cpu().numpy()\n",
    "    \n",
    "    # Ensure observation is in correct format\n",
    "    if obs_np.shape != (64, 64, 3):\n",
    "        print(f\"Warning: Unexpected observation shape: {obs_np.shape}\")\n",
    "        return patches\n",
    "    \n",
    "    # Scale factors between activation map and original image\n",
    "    scale_h = 64 / act_h\n",
    "    scale_w = 64 / act_w\n",
    "    \n",
    "    # Find regions of high activation\n",
    "    flat_activations = activation_map.flatten()\n",
    "    top_indices = np.argsort(flat_activations.detach().cpu().numpy())[-top_k:]\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        # Convert flat index to 2D coordinates\n",
    "        i, j = idx // act_w, idx % act_w\n",
    "        \n",
    "        # Convert to image space coordinates\n",
    "        img_i = int(i * scale_h)\n",
    "        img_j = int(j * scale_w)\n",
    "        \n",
    "        # Calculate patch boundaries\n",
    "        half_size = patch_size // 2\n",
    "        start_i = max(0, img_i - half_size)\n",
    "        start_j = max(0, img_j - half_size)\n",
    "        end_i = min(64, img_i + half_size)\n",
    "        end_j = min(64, img_j + half_size)\n",
    "        \n",
    "        # Extract patch\n",
    "        patch = obs_np[start_i:end_i, start_j:end_j, :]\n",
    "        \n",
    "        # Get activation score for this region\n",
    "        act_start_i = max(0, i - 1)\n",
    "        act_start_j = max(0, j - 1)\n",
    "        act_end_i = min(act_h, i + 2)\n",
    "        act_end_j = min(act_w, j + 2)\n",
    "        region_score = activation_map[act_start_i:act_end_i, act_start_j:act_end_j].mean().item()\n",
    "        \n",
    "        # Resize patch to standard size if needed\n",
    "        if patch.shape[:2] != (patch_size, patch_size):\n",
    "            try:\n",
    "                patch_pil = Image.fromarray((patch * 255).astype(np.uint8))\n",
    "                patch_pil = patch_pil.resize((patch_size, patch_size), Image.BILINEAR)\n",
    "                patch = np.array(patch_pil).astype(np.float32) / 255.0\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to resize patch: {e}, patch shape: {patch.shape}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert back to tensor\n",
    "        patch_tensor = torch.from_numpy(patch).permute(2, 0, 1)  # HWC -> CHW\n",
    "        patches.append((region_score, patch_tensor, (img_i, img_j)))\n",
    "    \n",
    "    # Sort by activation score\n",
    "    patches.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "def generate_batch_observations(num_samples=100, env_name='procgen:procgen-heist-v0'):\n",
    "    \"\"\"\n",
    "    Generate a batch of observations from the environment.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of observations to collect\n",
    "        env_name: Name of the environment\n",
    "        \n",
    "    Returns:\n",
    "        observations: List of observation tensors\n",
    "    \"\"\"\n",
    "    print(f\"Collecting {num_samples} observations from {env_name}\")\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    observations = []\n",
    "    \n",
    "    with tqdm(total=num_samples) as pbar:\n",
    "        while len(observations) < num_samples:\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            while not done and len(observations) < num_samples:\n",
    "                # Convert observation to tensor\n",
    "                obs_tensor = torch.from_numpy(obs).permute(2, 0, 1).float() / 255.0\n",
    "                observations.append(obs_tensor)\n",
    "                \n",
    "                # Take a random action\n",
    "                action = env.action_space.sample()\n",
    "                obs, _, done, _ = env.step(action)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                if len(observations) >= num_samples:\n",
    "                    break\n",
    "    \n",
    "    env.close()\n",
    "    return observations\n",
    "\n",
    "def get_sae_feature_activations(model, sae, observations, feature_idx):\n",
    "    \"\"\"\n",
    "    Get activations for a specific SAE feature across multiple observations.\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        sae: The SAE model\n",
    "        observations: List of observation tensors\n",
    "        feature_idx: Index of the feature to get activations for\n",
    "        \n",
    "    Returns:\n",
    "        activations: List of (activation_map, observation) tuples\n",
    "    \"\"\"\n",
    "    print(f\"Getting activations for feature {feature_idx}\")\n",
    "    \n",
    "    # Attach SAE to the model\n",
    "    sae_hook = replace_layer_with_sae(model, sae, layer_number)\n",
    "    \n",
    "    # Set up hooks to capture activations\n",
    "    all_activations = []\n",
    "    \n",
    "    def sae_activation_hook(module, input, output):\n",
    "        if isinstance(output, tuple) and len(output) >= 3:\n",
    "            # Store the activation for the specific feature\n",
    "            all_activations.append(output[2][0, feature_idx])\n",
    "        return output\n",
    "    \n",
    "    # Register hook on the SAE\n",
    "    hook = sae.register_forward_hook(sae_activation_hook)\n",
    "    \n",
    "    try:\n",
    "        # Process each observation\n",
    "        results = []\n",
    "        for obs in tqdm(observations):\n",
    "            # Add batch dimension\n",
    "            obs_batch = obs.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                _ = model(obs_batch)\n",
    "            \n",
    "            # Get the activation for this observation\n",
    "            if all_activations:\n",
    "                activation_map = all_activations.pop()\n",
    "                results.append((activation_map, obs))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    finally:\n",
    "        # Clean up hooks\n",
    "        hook.remove()\n",
    "        sae_hook.remove()\n",
    "\n",
    "def find_max_activating_patches(model, sae, feature_idx, num_samples=100, patch_size=24, top_k=5, diversity_weight=2.0):\n",
    "    \"\"\"\n",
    "    Find diverse patches that maximally activate a specific SAE feature.\n",
    "    \n",
    "    Args:\n",
    "        model: The base model\n",
    "        sae: The SAE model\n",
    "        feature_idx: Index of the feature to find patches for\n",
    "        num_samples: Number of observations to process\n",
    "        patch_size: Size of patches to extract\n",
    "        top_k: Number of top patches to return\n",
    "        diversity_weight: Weight for diversity bonus\n",
    "        \n",
    "    Returns:\n",
    "        max_patches: List of (effective_score, raw_score, patch, coordinates) tuples\n",
    "    \"\"\"\n",
    "    # Generate observations\n",
    "    observations = generate_batch_observations(num_samples)\n",
    "    \n",
    "    # Get activations for the feature\n",
    "    activations = get_sae_feature_activations(model, sae, observations, feature_idx)\n",
    "    \n",
    "    # Extract patches from high-activation regions\n",
    "    all_patches = []\n",
    "    for activation_map, obs in activations:\n",
    "        patches = extract_activation_patches(activation_map, obs, patch_size, top_k=3)\n",
    "        all_patches.extend(patches)\n",
    "    \n",
    "    # Sort by activation score\n",
    "    all_patches.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Select diverse patches\n",
    "    selected_patches = []\n",
    "    \n",
    "    # Process candidate patches\n",
    "    for raw_score, patch, coords in all_patches:\n",
    "        # For the first patch, just use raw score\n",
    "        if len(selected_patches) == 0:\n",
    "            effective_score = raw_score\n",
    "            selected_patches.append((effective_score, raw_score, patch, coords))\n",
    "            continue\n",
    "        \n",
    "        # Compute similarity to existing selected patches\n",
    "        similarities = []\n",
    "        for _, _, existing_patch, _ in selected_patches:\n",
    "            sim = cosine_similarity(patch, existing_patch)\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        # Calculate average similarity\n",
    "        avg_similarity = sum(similarities) / len(similarities)\n",
    "        \n",
    "        # Apply diversity bonus\n",
    "        diversity_bonus = diversity_weight * (1.0 - avg_similarity)\n",
    "        effective_score = raw_score + diversity_bonus\n",
    "        \n",
    "        # Add to selected patches if better than existing or if we need more\n",
    "        if len(selected_patches) < top_k:\n",
    "            selected_patches.append((effective_score, raw_score, patch, coords))\n",
    "        else:\n",
    "            # Find the patch with the lowest effective score\n",
    "            min_idx = min(range(len(selected_patches)), key=lambda i: selected_patches[i][0])\n",
    "            if effective_score > selected_patches[min_idx][0]:\n",
    "                selected_patches[min_idx] = (effective_score, raw_score, patch, coords)\n",
    "        \n",
    "        # Sort by effective score\n",
    "        selected_patches.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Keep only top_k\n",
    "        if len(selected_patches) > top_k:\n",
    "            selected_patches = selected_patches[:top_k]\n",
    "    \n",
    "    return selected_patches\n",
    "\n",
    "def visualize_max_activations(feature_idx, max_patches):\n",
    "    \"\"\"\n",
    "    Visualize patches that maximally activate a specific feature.\n",
    "    \n",
    "    Args:\n",
    "        feature_idx: Index of the feature\n",
    "        max_patches: List of (effective_score, raw_score, patch, coordinates) tuples\n",
    "    \"\"\"\n",
    "    # Create directory for saving visualizations\n",
    "    os.makedirs('max_activations', exist_ok=True)\n",
    "    \n",
    "    # Create a figure\n",
    "    num_patches = len(max_patches)\n",
    "    fig, axes = plt.subplots(1, num_patches, figsize=(4*num_patches, 4))\n",
    "    \n",
    "    if num_patches == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Display each patch\n",
    "    for i, (effective_score, raw_score, patch, coords) in enumerate(max_patches):\n",
    "        # Convert patch to numpy for display\n",
    "        patch_np = patch.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "        patch_np = np.clip(patch_np, 0, 1)\n",
    "        \n",
    "        # Display patch\n",
    "        axes[i].imshow(patch_np)\n",
    "        axes[i].set_title(f'Raw: {raw_score:.2f}\\nEff: {effective_score:.2f}\\nPos: {coords}')\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Save individual patch\n",
    "        plt.imsave(f'max_activations/feature_{feature_idx}_patch_{i}.png', patch_np)\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle(f'Feature {feature_idx} - Max Activating Patches', fontsize=16)\n",
    "    \n",
    "    # Save the combined figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'max_activations/feature_{feature_idx}_patches.png')\n",
    "    \n",
    "    # Display figure inline\n",
    "    plt.show()\n",
    "\n",
    "# Select features to visualize\n",
    "feature_indices = list(range(8))  # Start with first 8 features\n",
    "\n",
    "# Find max activating patches for each feature\n",
    "for feature_idx in feature_indices:\n",
    "    print(f\"\\nProcessing feature {feature_idx}\")\n",
    "    \n",
    "    # Find max activating patches with diversity\n",
    "    max_patches = find_max_activating_patches(\n",
    "        model=model,\n",
    "        sae=sae,\n",
    "        feature_idx=feature_idx,\n",
    "        num_samples=100,\n",
    "        patch_size=24,\n",
    "        top_k=4,\n",
    "        diversity_weight=2.0  # Adjust this to control diversity vs. raw activation\n",
    "    )\n",
    "    \n",
    "    # Visualize patches\n",
    "    visualize_max_activations(feature_idx, max_patches)\n",
    "\n",
    "print(\"All max activating patches found and visualized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
