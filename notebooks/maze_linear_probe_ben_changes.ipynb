{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Probe Experiments\n",
    "A first try at generating data in a procgen maze environment, and using that to construct a linear probe to detect a \"cheese signal\" in the neural network layers of a pre-trained RL agent. \n",
    "\n",
    "## Initialize Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from procgen import ProcgenGym3Env\n",
    "from procgen_tools import maze\n",
    "from procgen_tools.models import load_policy\n",
    "from procgen_tools import maze as maze_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "save_dir = Path(\"maze_linear_probe_data\")\n",
    "if not save_dir.is_dir():\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gym environment will be created through `procgen-tools`. It provides a wrapper around the original procgen environment to make it compatible with gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prev_level_seed', 'prev_level_complete', 'level_seed', 'rgb'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "wrapped_venv = maze_api.create_venv(\n",
    "    num=1, start_level=int(seed), num_levels=0, num_threads = 4\n",
    ")\n",
    "wrapped_venv.env.get_info()[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CEnv lib_path=/Users/bensturgeon/Library/Caches/pypoetry/virtualenvs/ai-safety-camp-Y8XZewIj-py3.10/lib/python3.10/site-packages/procgen/data/prebuilt/libenv.dylib options={'center_agent': True, 'use_generated_assets': False, 'use_monochrome_assets': False, 'restrict_themes': False, 'use_backgrounds': True, 'paint_vel_info': False, 'distribution_mode': 1, 'env_name': 'maze', 'num_levels': 0, 'start_level': 42, 'num_actions': 15, 'use_sequential_levels': False, 'debug_mode': 0, 'rand_seed': 322487239, 'num_threads': 4, 'render_human': True, 'resource_root': '/Users/bensturgeon/Library/Caches/pypoetry/virtualenvs/ai-safety-camp-Y8XZewIj-py3.10/lib/python3.10/site-packages/procgen/data/assets/'}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the procgen env directly like so\n",
    "wrapped_venv.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Agent\n",
    "Download one of the agents from the [trained model files](https://drive.google.com/drive/folders/1Ig7bzRlieyYFcdKL_PM-guSWR8WryDOL). I used `maze_I/model_rand_region_5` without knowing about the performance of this agent. Do not forget to rename the model file or change the filename below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = load_policy('model_rand_region_15.pth', action_size=15, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_episode(model, maze_environment,logger, argmax=True, max_time_steps=256):\n",
    "    model.eval()  # Switch off gradient tracking and other training time mechanisms\n",
    "    obs = maze_environment.reset()\n",
    "    logger.update_environment(maze_environment) \n",
    "    for step in range(max_time_steps):\n",
    "        out, _ = model(torch.FloatTensor(obs))\n",
    "        act = out.probs.argmax(dim=-1).numpy() if argmax else out.sample().numpy()\n",
    "        obs, rew, done, info = maze_environment.step(act)\n",
    "        logger.log(model, step, act, obs, rew, done)  # Updated call\n",
    "        if done:\n",
    "            logger.done()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Model Activations\n",
    "Activations can be [logged with `circrl`](https://github.com/montemac/circrl/tree/main). The hook manager logs a single activation by default. Since we want to have activations spread over the entire eposide we will use a custom hook manager that can be reset between episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_layer_names(model):\n",
    "    return [name for name, module in model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'embedder', 'embedder.block1', 'embedder.block1.conv', 'embedder.block1.maxpool']\n"
     ]
    }
   ],
   "source": [
    "names = get_model_layer_names(policy)\n",
    "print(names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a hook to log the activations of the corresponding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before logging\n",
      "dict_keys([])\n",
      "After logging\n",
      "(Categorical(logits: torch.Size([1, 15])), tensor([5.3809], grad_fn=<ViewBackward0>))\n",
      "dict_keys(['embedder.block1.conv', 'embedder.block1', 'embedder'])\n"
     ]
    }
   ],
   "source": [
    "from circrl import hooks\n",
    "\n",
    "policy_hook = hooks.HookManager(\n",
    "    model=policy,\n",
    "    cache=names[1:4]\n",
    ")\n",
    "\n",
    "print(\"Before logging\")\n",
    "print(policy_hook.cache_results.keys())\n",
    "\n",
    "# Run an environment update in the policy hook context to start logging\n",
    "with policy_hook:\n",
    "    obs = wrapped_venv.reset()\n",
    "    output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"After logging\")\n",
    "print(output)\n",
    "print(policy_hook.cache_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful! In the default log setting the `cache_results` gets overwritten every time the policy is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log size after a single execution:\n",
      "\ttorch.Size([1, 256])\n",
      "Log size after a multiple executions:\n",
      "\ttorch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "with policy_hook:\n",
    "    obs = wrapped_venv.reset()\n",
    "    output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"log size after a single execution:\")\n",
    "print(f\"\\t{policy_hook.cache_results[names[1]].size()}\")\n",
    "\n",
    "with policy_hook:    \n",
    "    obs = wrapped_venv.reset()\n",
    "    for _ in range(10):\n",
    "        output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"Log size after a multiple executions:\")\n",
    "print(f\"\\t{policy_hook.cache_results[names[1]].size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Training Data\n",
    "To gather training data we need to run multiple episodes and store the activations of the model alongside the environment parameters. Procgen-tools provides some convenience functions to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_maze_training_data(model, n_episodes,logger, on_episode_step, on_episode_done ):\n",
    "    max_seed = int(1e9)\n",
    "    seeds = np.random.default_rng().choice(max_seed, size=n_episodes, replace=False)\n",
    "\n",
    "    for seed in tqdm(seeds):\n",
    "        wrapped_venv = maze_api.create_venv(\n",
    "            num=1, start_level=int(seed), num_levels=0, num_threads = 4\n",
    "        )  # Convenience functions\n",
    "        \n",
    "        run_episode(model, wrapped_venv,logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a system to log activations and environment variables persistently. It would be best to avoid having to edit code related to model execution when logging to avoid introducting bugs. We can use something like the following logging setup to log episodic data to disk at the end of every episode. It combines the previous logging function with hooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class ProbeDataLogger:\n",
    "    def __init__(self, model, save_file_path=\"\"):\n",
    "        self._init_logs()\n",
    "\n",
    "        self.model = model\n",
    "        layer_names = get_model_layer_names(model)\n",
    "        self.model_hook = hooks.HookManager(\n",
    "            model=model,\n",
    "            cache=names\n",
    "        )\n",
    "        save_file_path = Path(save_file_path)\n",
    "        self._file_path = save_file_path\n",
    "        self._file_extension = \".pkl\"\n",
    "        self._current_ep = 0\n",
    "        self.env = None\n",
    "    \n",
    "    def update_environment(self, env):\n",
    "        \"\"\"Update the logger to use the current environment instance.\"\"\"\n",
    "        self.env = env\n",
    "\n",
    "    def log(self, model, environment, step, act, obs, rew):\n",
    "        self.observations.append(obs)\n",
    "        self.rewards.append(rew)\n",
    "        self.activations.append(self.model_hook.cache_results)\n",
    "\n",
    "        # Using these functions only works because the environment is wrapped in\n",
    "        # the training data generation method\n",
    "        state = maze_api.state_from_venv(self.env)\n",
    "        grid = maze_api.get_grid(state.state_vals)\n",
    "        self.cheese_positions.append(maze_api.get_cheese_pos(grid))\n",
    "\n",
    "    def done(self):\n",
    "        self._current_ep += 1\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"Flush data in log buffer to file and reset buffer\"\"\"\n",
    "        save_path = self._file_path / Path(\"ep_\" + str(self._current_ep) + str(self._file_extension))\n",
    "        print(save_path)\n",
    "        self._save_logs_to_file(save_path)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._init_logs()\n",
    "\n",
    "    def _save_logs_to_file(self, file_path):\n",
    "        write_mode = 'wb' \n",
    "\n",
    "        obs = torch.tensor(self.observations)\n",
    "        rew = torch.tensor(self.rewards)\n",
    "        cheese_positions = torch.tensor(self.cheese_positions)\n",
    "        \n",
    "        with open(file_path, write_mode) as file:\n",
    "            pickle.dump(self.observations, file)\n",
    "            pickle.dump(self.rewards, file)\n",
    "            pickle.dump(self.activations, file)\n",
    "            pickle.dump(self.cheese_positions, file)\n",
    "\n",
    "    def _init_logs(self):\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "        self.activations = []\n",
    "        self.cheese_positions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load back the data. Including the loading functionality in the logging class might be convenient, but here I will simply recover it manually as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/58/f16y6j7x0j5d0vv3pvq80c680000gn/T/ipykernel_2665/2808487301.py:51: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  obs = torch.tensor(self.observations)\n",
      "/var/folders/58/f16y6j7x0j5d0vv3pvq80c680000gn/T/ipykernel_2665/2808487301.py:52: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  rew = torch.tensor(self.rewards)\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/ep_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/ep_2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/ep_3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "frames =[]\n",
    "logs = ProbeDataLogger(policy, save_file_path=save_dir)\n",
    "with logs.model_hook:\n",
    "    generate_maze_training_data(policy, 3,logs, logs.log, logs.done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded activations: 16\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_logs(file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - file_path (str): The path to the pickle file.\n",
    "    \n",
    "    Returns:\n",
    "    - observations (list): Environment observations.\n",
    "    - rewards (list): Environment rewards.\n",
    "    - activations (list): The list of activations unpickled from the file.\n",
    "    - cheese_positions (list): Cheese position tensors in x, y.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Unpickle in the order they were saved: observations, rewards, activations\n",
    "        observations = pickle.load(file)\n",
    "        rewards = pickle.load(file)\n",
    "        activations = pickle.load(file)\n",
    "        cheese_positions = pickle.load(file)\n",
    "    \n",
    "    return observations, rewards, activations, cheese_positions\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = save_dir / 'ep_1.pkl'\n",
    "_, _, activations, _ = load_logs(file_path)\n",
    "print(\"Loaded activations:\", len(activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Linear Probe\n",
    "Training a linear probe requires choosing what layers we are probing, and with respect to what target. That in turn determines the structure of the probe.\n",
    "\n",
    "### Generate Training Data\n",
    "\n",
    "In this case we will probe an early convolutional layer in the network for cheese position signal as an example. First we load a model and generate some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log directory\n",
    "import os\n",
    "\n",
    "log_dir = save_dir / Path(\"logs\")\n",
    "\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and create logger\n",
    "probe_policy = load_policy('model_rand_region_15.pth', action_size=15, device=torch.device('cpu'))\n",
    "logger = ProbeDataLogger(probe_policy, save_file_path=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/var/folders/58/f16y6j7x0j5d0vv3pvq80c680000gn/T/ipykernel_2665/2808487301.py:52: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  rew = torch.tensor(self.rewards)\n",
      "  3%|▎         | 1/30 [00:01<00:30,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:26,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:02<00:20,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:03<00:22,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_4.pkl\n",
      "maze_linear_probe_data/logs/ep_5.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:03<00:12,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_6.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:04<00:15,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_7.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:13<01:04,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_8.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:13<00:33,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_9.pkl\n",
      "maze_linear_probe_data/logs/ep_10.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:14<00:20,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_11.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:15<00:20,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_12.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:16<00:16,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_13.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:16<00:13,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_14.pkl\n",
      "maze_linear_probe_data/logs/ep_15.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:17<00:07,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_16.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:24<00:18,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_17.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:24<00:13,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_18.pkl\n",
      "maze_linear_probe_data/logs/ep_19.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:25<00:04,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_20.pkl\n",
      "maze_linear_probe_data/logs/ep_21.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:26<00:05,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_22.pkl\n",
      "maze_linear_probe_data/logs/ep_23.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:27<00:02,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_24.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:29<00:02,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_25.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:36<00:01,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_26.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:37<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_27.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "with logger.model_hook:\n",
    "    generate_maze_training_data(probe_policy, 30, logger, logger.log, logger.done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probe Training\n",
    "\n",
    "We need to load the episode data, process it into a format suitable for the training process and then train [a linear probe](https://github.com/montemac/circrl/blob/33534f2f78547b38172e3a4f0d682bc8b5b46b4f/src/circrl/probing.py#L47). For this particular version we can process the episodic data into files of numpy arrays or pytorch tensors. We'll choose pytorch tensors here. \n",
    "\n",
    "Alternatively the data could be batched in files to reduce read/write overhead. It might be worth doing if compute bottlenecks become an obvious concern.\n",
    "\n",
    "#### Regression Probe\n",
    "\n",
    "Using the Ridge Regressor provided by SKLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def fit_linear_probe(model: Ridge, layer_name: str, inputs: np.ndarray, targets: np.ndarray, random_seed: int = 42):\n",
    "    test_size = 0.2\n",
    "    inputs_train, inputs_test, targets_train, targets_test = train_test_split(\n",
    "        inputs, targets, test_size=test_size, random_state=random_seed\n",
    "    )\n",
    "\n",
    "    model.fit(inputs_train, targets_train)\n",
    "    return {\n",
    "        \"train_score\": model.score(inputs_train, targets_train),\n",
    "        \"test_score\": model.score(inputs_test, targets_test),\n",
    "        \"x_train\": inputs_train,\n",
    "        \"y_train\": targets_train,\n",
    "        \"x_test\": inputs_test,\n",
    "        \"y_test\": targets_test,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.49999999999989886\n",
      "test score: 0.5\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "mdl = Ridge(random_state=seed)\n",
    "layer_name = \"embedder.block1.conv\"\n",
    "data_path = save_dir / \"logs\" / \"ep_1.pkl\"\n",
    "\n",
    "_, _, layer_activations, cheese_positions = load_logs(data_path)\n",
    "assert layer_name in layer_activations[0].keys(), \"Cannot find layer '{layer_name}' in activations\"\n",
    "\n",
    "inputs = torch.stack([act[layer_name] for act in layer_activations], dim=0)\n",
    "inputs = inputs.detach().numpy()\n",
    "# Explicit choice to use all the activations as input instead of sampling (might need to reduce input feature space significantly to prevent overfitting)\n",
    "inputs = np.reshape(inputs, (inputs.shape[0], -1))\n",
    "\n",
    "targets = torch.tensor(cheese_positions)\n",
    "targets = targets.detach().numpy()\n",
    "\n",
    "results = fit_linear_probe(mdl, layer_name, inputs, targets, seed)\n",
    "print(f\"train score: {results['train_score']}\")\n",
    "print(f\"test score: {results['test_score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_probe(model: Ridge, layer_name: str, episode_data_dir: Path, random_seed: int = 42):\n",
    "    all_inputs = []\n",
    "    all_targets = []\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    # Aggregate data from all files first\n",
    "    for data_file in episode_data_dir.glob('*.pkl'):\n",
    "        _, _, layer_activations, cheese_positions = load_logs(data_file)\n",
    "        assert layer_name in layer_activations[0].keys(), \"Cannot find layer '{layer_name}' in activations\"\n",
    "        \n",
    "        in_tmp = torch.stack([act[layer_name] for act in layer_activations], dim=0).detach().numpy()\n",
    "        in_tmp = np.reshape(in_tmp, (in_tmp.shape[0], -1))\n",
    "        tar_tmp = torch.tensor(cheese_positions).detach().numpy()\n",
    "\n",
    "        all_inputs.append(in_tmp)\n",
    "        all_targets.append(tar_tmp)\n",
    "\n",
    "\n",
    "\n",
    "    # Concatenate all collected data\n",
    "    inputs = np.concatenate(all_inputs, axis=0)\n",
    "    targets = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    # Now, train the model once with all the data\n",
    "    results = fit_linear_probe(model, layer_name, inputs, targets, random_seed)\n",
    "    train_scores.append(results['train_score'])\n",
    "    test_scores.append(results['test_score'])\n",
    "\n",
    "    # Calculate and print overall performance\n",
    "    mean_train_score = np.mean(train_scores)\n",
    "    mean_test_score = np.mean(test_scores)\n",
    "    unique_targets = np.unique(targets)\n",
    "    print(f\"Mean train score: {mean_train_score}\")\n",
    "    print(f\"Mean test score: {mean_test_score}\")\n",
    "    print(f\"Number of unique targets: {len(unique_targets)}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train score: 0.9743477722776905\n",
      "Mean test score: 0.9690213593690625\n",
      "Number of unique targets: 24\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "mdl = Ridge(random_state=seed)\n",
    "layer_name = \"embedder.block1.conv\"\n",
    "data_path = save_dir / \"logs\"\n",
    "\n",
    "model = train_linear_probe(mdl, layer_name, data_path, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
