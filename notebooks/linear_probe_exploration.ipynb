{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building procgen..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bensturgeon/mambaforge/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "/Users/bensturgeon/mambaforge/lib/python3.10/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import heist\n",
    "import helpers\n",
    "import torch.distributions\n",
    "import torch\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from helpers import generate_action, load_model\n",
    "from procgen import ProcgenGym3Env\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import typing\n",
    "import math\n",
    "\n",
    "from procgen import ProcgenGym3Env\n",
    "import struct\n",
    "import typing\n",
    "from typing import Tuple, Dict, Callable, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from src.policies_impala import ImpalaCNN\n",
    "from procgen_tools.procgen_wrappers import VecExtractDictObs, TransposeFrame, ScaledFloatFrame\n",
    "\n",
    "from gym3 import ToBaselinesVecEnv\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import src.probing as probing\n",
    "\n",
    "categories  = {\n",
    "        \"gem\": [],\n",
    "        \"blue_key\": [],\n",
    "        \"green_key\": [],\n",
    "        \"red_key\": [],\n",
    "        \"blue_lock\": [],\n",
    "        \"green_lock\": [],\n",
    "        \"red_lock\": []\n",
    "    }\n",
    "\n",
    "# Create classified dataset\n",
    "classified_dataset = heist.create_classified_dataset(num_samples_per_category=900, num_levels=0)\n",
    "\n",
    "# Load the model\n",
    "model = helpers.load_interpretable_model(model_path=\"../model_interpretable.pt\")\n",
    "\n",
    "layer_paths = helpers.get_model_layer_names(model)\n",
    "# Create activation dataset\n",
    "activation_dataset = helpers.create_activation_dataset(classified_dataset, model, layer_paths, categories)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_paths= {\n",
    "    1: 'conv1a',\n",
    "    2: 'pool1',\n",
    "    3: 'conv2a',\n",
    "    4: 'conv2b',\n",
    "    5: 'pool2',\n",
    "    6: 'conv3a',\n",
    "    7: 'pool3',\n",
    "    8: 'conv4a',\n",
    "    9: 'pool4',\n",
    "    10: 'fc1',\n",
    "    11: 'fc2',\n",
    "    12: 'fc3',\n",
    "    13: 'value_fc',\n",
    "    14: 'dropout_conv',\n",
    "    15: 'dropout_fc'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'x_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m layer_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfc1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Example 1: Per-category linear probe\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m category_results \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_probe_per_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m category, result \u001b[38;5;129;01min\u001b[39;00m category_results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m, in \u001b[0;36mlinear_probe_per_category\u001b[0;34m(activation_dataset, layer_path, model_type, test_size, random_state, **regression_kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m y \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcat([y_category, y_rest])\n\u001b[1;32m     48\u001b[0m result \u001b[38;5;241m=\u001b[39m linear_probe(\n\u001b[1;32m     49\u001b[0m     X,\n\u001b[1;32m     50\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mregression_kwargs,\n\u001b[1;32m     55\u001b[0m )\n\u001b[0;32m---> 57\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     58\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m accuracy_score(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_test\u001b[39m\u001b[38;5;124m\"\u001b[39m], y_pred)\n\u001b[1;32m     60\u001b[0m category_results[category] \u001b[38;5;241m=\u001b[39m result\n",
      "\u001b[0;31mKeyError\u001b[0m: 'x_test'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bensturgeon/Library/Caches/pypoetry/virtualenvs/ai-safety-camp-Y8XZewIj-py3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bensturgeon/Library/Caches/pypoetry/virtualenvs/ai-safety-camp-Y8XZewIj-py3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bensturgeon/Library/Caches/pypoetry/virtualenvs/ai-safety-camp-Y8XZewIj-py3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.962698\n",
      "1    0.975397\n",
      "2    0.865873\n",
      "3    0.880159\n",
      "4    0.842857\n",
      "5    0.846825\n",
      "6    0.921429\n",
      "Name: test_accuracy, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bensturgeon/Library/Caches/pypoetry/virtualenvs/ai-safety-camp-Y8XZewIj-py3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/bensturgeon/Library/Caches/pypoetry/virtualenvs/ai-safety-camp-Y8XZewIj-py3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "results_dataframe, probes = probing.linear_probe_per_category_using_probes(activation_dataset, 'fc1', model_type='classifier', test_size=0.2, random_state=42)\n",
    "print(results_dataframe[\"test_accuracy\"])\n",
    "# results_dataframe = probing.linear_probe_per_category_using_probes(activation_dataset, 'fc2', model_type='classifier', test_size=0.2, random_state=42)\n",
    "# print(results_dataframe[\"test_accuracy\"])\n",
    "# results_dataframe = probing.linear_probe_per_category_using_probes(activation_dataset, 'fc3', model_type='classifier', test_size=0.2, random_state=42)\n",
    "# print(results_dataframe[\"test_accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9779653070792311\n",
      "0.9855746859004187\n",
      "0.9243170622480967\n",
      "0.9303826648224989\n",
      "0.9121561668145519\n",
      "0.9147149801148917\n",
      "0.9534555712270804\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "new_classification_data = results_dataframe[\"classification_report\"]\n",
    "\n",
    "for key,val in new_classification_data.items():\n",
    "    print(val[\"0\"][\"f1-score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for abs(): 'LogisticRegression'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m weights \u001b[38;5;241m=\u001b[39m probes \u001b[38;5;66;03m# 7 objectives, 100 neurons\u001b[39;00m\n\u001b[1;32m     68\u001b[0m objectives \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue_key\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen_key\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred_key\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue_lock\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen_lock\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred_lock\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 70\u001b[0m top_neurons, shared_neurons, unique_neurons \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_probe_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectives\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# print(\"Top 5 neurons for each objective:\")\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# for i, obj in enumerate(objectives):\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#     print(f\"{obj}: {top_neurons[i][:5]}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# # Usage (you would need your actual model and test data)\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# # ablation_study(model, x_test, y_test, top_neurons, objectives)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36manalyze_probe_weights\u001b[0;34m(weights, objectives)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Analyze and visualize the weights of a linear probe.\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1. Magnitude Analysis\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m importance \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m top_neurons \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(importance, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:, :\u001b[38;5;241m10\u001b[39m]  \u001b[38;5;66;03m# Top 10 neurons for each objective\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 2. Visualization\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for abs(): 'LogisticRegression'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def analyze_probe_weights(weights, objectives):\n",
    "    \"\"\"Analyze and visualize the weights of a linear probe.\"\"\"\n",
    "    \n",
    "    # 1. Magnitude Analysis\n",
    "    importance = np.abs(weights)\n",
    "    top_neurons = np.argsort(importance, axis=1)[:, ::-1][:, :10]  # Top 10 neurons for each objective\n",
    "    \n",
    "    # 2. Visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(weights, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title('Probe Weights for Each Objective')\n",
    "    plt.xlabel('Dense Layer Neurons')\n",
    "    plt.ylabel('Objectives')\n",
    "    plt.yticks(range(len(objectives)), objectives)\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Per-Objective Analysis\n",
    "    for i, objective in enumerate(objectives):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(range(len(weights[i])), weights[i])\n",
    "        plt.title(f'Weight Distribution for {objective}')\n",
    "        plt.xlabel('Neuron Index')\n",
    "        plt.ylabel('Weight')\n",
    "        plt.show()\n",
    "    \n",
    "    # 4. Shared vs. Unique Neurons\n",
    "    threshold = np.percentile(np.abs(weights), 95)  # Top 5% of weights\n",
    "    important_neurons = np.abs(weights) > threshold\n",
    "    shared_neurons = np.sum(important_neurons, axis=0) > 1\n",
    "    unique_neurons = np.sum(important_neurons, axis=0) == 1\n",
    "    \n",
    "    print(f\"Number of shared important neurons: {np.sum(shared_neurons)}\")\n",
    "    print(f\"Number of unique important neurons: {np.sum(unique_neurons)}\")\n",
    "    \n",
    "    # 5. Correlation Analysis\n",
    "    corr_matrix = np.corrcoef(weights)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', xticklabels=objectives, yticklabels=objectives)\n",
    "    plt.title('Correlation between Objective Weight Vectors')\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Dimensionality Reduction\n",
    "    pca = PCA(n_components=2)\n",
    "    weights_2d = pca.fit_transform(weights)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(weights_2d[:, 0], weights_2d[:, 1])\n",
    "    for i, obj in enumerate(objectives):\n",
    "        plt.annotate(obj, (weights_2d[i, 0], weights_2d[i, 1]))\n",
    "    plt.title('PCA of Weight Vectors')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.show()\n",
    "    \n",
    "    return top_neurons, shared_neurons, unique_neurons\n",
    "\n",
    "# Usage example\n",
    "# Assuming you have trained a linear probe and have the weights and objectives\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "weights = probes # 7 objectives, 100 neurons\n",
    "objectives = ['gem', 'blue_key', 'green_key', 'red_key', 'blue_lock', 'green_lock', 'red_lock']\n",
    "\n",
    "top_neurons, shared_neurons, unique_neurons = analyze_probe_weights(weights, objectives)\n",
    "\n",
    "# print(\"Top 5 neurons for each objective:\")\n",
    "# for i, obj in enumerate(objectives):\n",
    "#     print(f\"{obj}: {top_neurons[i][:5]}\")\n",
    "\n",
    "# print(\"\\nIndices of shared important neurons:\", np.where(shared_neurons)[0])\n",
    "# print(\"Indices of unique important neurons:\", np.where(unique_neurons)[0])\n",
    "\n",
    "# # 7. Activation Pattern Analysis (you would need to implement this with your actual data)\n",
    "# def analyze_activation_patterns(activations, top_neurons, objectives):\n",
    "#     \"\"\"Analyze activation patterns of top neurons for each objective.\"\"\"\n",
    "#     for i, obj in enumerate(objectives):\n",
    "#         plt.figure(figsize=(12, 5))\n",
    "#         for j, neuron in enumerate(top_neurons[i][:5]):\n",
    "#             plt.plot(activations[:, neuron], label=f'Neuron {neuron}')\n",
    "#         plt.title(f'Activation Patterns of Top 5 Neurons for {obj}')\n",
    "#         plt.xlabel('Sample')\n",
    "#         plt.ylabel('Activation')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "# # Usage (you would need your actual activation data)\n",
    "# # activations = ... # Shape: (n_samples, n_neurons)\n",
    "# # analyze_activation_patterns(activations, top_neurons, objectives)\n",
    "\n",
    "# # 8. Ablation Studies (you would need to implement this with your actual model and data)\n",
    "# def ablation_study(model, x_test, y_test, top_neurons, objectives):\n",
    "#     \"\"\"Perform ablation study by zeroing out top neurons for each objective.\"\"\"\n",
    "#     base_score = model.score(x_test, y_test)\n",
    "#     for i, obj in enumerate(objectives):\n",
    "#         x_test_ablated = x_test.copy()\n",
    "#         x_test_ablated[:, top_neurons[i][:10]] = 0  # Zero out top 10 neurons\n",
    "#         ablated_score = model.score(x_test_ablated, y_test)\n",
    "#         print(f\"Ablation result for {obj}:\")\n",
    "#         print(f\"  Base score: {base_score:.4f}\")\n",
    "#         print(f\"  Ablated score: {ablated_score:.4f}\")\n",
    "#         print(f\"  Performance drop: {base_score - ablated_score:.4f}\")\n",
    "\n",
    "# # Usage (you would need your actual model and test data)\n",
    "# # ablation_study(model, x_test, y_test, top_neurons, objectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-safety-camp-Y8XZewIj-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
