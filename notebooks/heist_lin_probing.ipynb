{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch.distributions\n",
    "import torch\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from procgen import ProcgenGym3Env\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import typing\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, Dict, Callable, List, Optional\n",
    "from dataclasses import dataclass\n",
    "# from src.policies_modified import ImpalaCNN\n",
    "from procgen_tools.procgen_wrappers import VecExtractDictObs, TransposeFrame, ScaledFloatFrame\n",
    "from gym3 import ToBaselinesVecEnv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Import custom modules\n",
    "from src.utils import heist\n",
    "from src.utils import helpers\n",
    "from helpers import generate_action, load_model\n",
    "\n",
    "# Reload modules automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_plot_probes(\n",
    "    objective_activations_dataset: Dict[str, Dict[str, torch.Tensor]], \n",
    "    layers_to_probe: Optional[List[str]] = None,\n",
    "    plot_confusion_matrices: bool = True,\n",
    "    plot_auroc_scores: bool = True,\n",
    "    plot_accuracies: bool = True\n",
    "):\n",
    "    '''\n",
    "    Train probes on each layer to predict current objective from activations.\n",
    "\n",
    "    Args:\n",
    "    - objective_vectors: dict of dict of objective vectors, where each key is the name of the objective\n",
    "    (e.g. gem or red_lock) and each value is a dict whose keys are layer names and values are activations for that layer\n",
    "    that correspond to input images where the player is trying to get to that objective.\n",
    "    - layers_to_probe: list of layers to probe. If None, all layers will be probed.\n",
    "    - plot_confusion_matrices: whether to plot confusion matrices.\n",
    "    - plot_auroc_scores: whether to plot AUROC scores.\n",
    "    - plot_accuracies: whether to plot accuracies.\n",
    "\n",
    "    Returns:\n",
    "    - Accuracies: dict of dict of accuracies, where each key is the name of the objective, and each value is a dict\n",
    "    whose keys are layer names and values are the accuracy of the probe on that layer.\n",
    "    '''\n",
    "    accuracies = {}\n",
    "    class_accuracies = {}\n",
    "    auroc_scores = {}\n",
    "    class_aurocs = {}\n",
    "\n",
    "    # Ensure that 'gem' is a key and extract the layers from 'gem'\n",
    "    assert 'gem' in objective_activations_dataset, \"'gem' must be a key in the objective_activations_dataset\"\n",
    "    first_key = next(iter(objective_activations_dataset))\n",
    "    \n",
    "    for layer in objective_activations_dataset[first_key].keys():\n",
    "        # Skip layers not in layers_to_probe\n",
    "        if layers_to_probe is not None and layer not in layers_to_probe:\n",
    "            continue\n",
    "        \n",
    "        # Get dataset of activation, objective pairs\n",
    "        activation_data = []\n",
    "        labels = []\n",
    "        for objective in objective_activations_dataset.keys():\n",
    "            activations = torch.stack(objective_activations_dataset[objective][layer])  # Normally tuple\n",
    "            activations = activations.view(activations.shape[0], -1)\n",
    "            activation_data.append(activations)\n",
    "            labels += [objective] * activations.shape[0]\n",
    "        \n",
    "        # Combine all activation data and labels into single tensors/lists\n",
    "        combined_activations = torch.cat(activation_data)\n",
    "        combined_labels = np.array(labels)\n",
    "\n",
    "        # Create train and test sets\n",
    "        train_data, test_data, train_labels, test_labels = train_test_split(combined_activations, combined_labels, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Train logistic regression model\n",
    "        probe = LogisticRegression(random_state=42, max_iter=40000)\n",
    "        probe.fit(train_data, train_labels)\n",
    "\n",
    "        # Predict on test set\n",
    "        predictions = probe.predict(test_data)\n",
    "        accuracy = accuracy_score(test_labels, predictions)\n",
    "        accuracies[layer] = accuracy\n",
    "\n",
    "        # Calculate AUROC score for multiclass classification\n",
    "        label_binarized_test = label_binarize(test_labels, classes=list(objective_activations_dataset.keys()))\n",
    "        decision_scores = probe.decision_function(test_data)\n",
    "        auroc = roc_auc_score(label_binarized_test, decision_scores, multi_class='ovr')\n",
    "        auroc_scores[layer] = auroc\n",
    "\n",
    "        # Calculate AUROC score for each class\n",
    "        class_aurocs[layer] = {}\n",
    "        for i, objective in enumerate(objective_activations_dataset.keys()):\n",
    "            if np.any(label_binarized_test[:, i]):\n",
    "                class_aurocs[layer][objective] = roc_auc_score(label_binarized_test[:, i], decision_scores[:, i])\n",
    "            else:\n",
    "                class_aurocs[layer][objective] = float('nan')  # Handle missing AUROC entries\n",
    "\n",
    "        # Calculate accuracy for each class\n",
    "        report = classification_report(test_labels, predictions, output_dict=True)\n",
    "        class_accuracies[layer] = {objective: report[objective]['precision'] for objective in objective_activations_dataset.keys() if objective in report}\n",
    "        \n",
    "        print(f'Layer: {layer}, Overall Accuracy: {accuracy}, AUROC: {auroc}')\n",
    "        for objective in class_accuracies[layer]:\n",
    "            auroc_str = class_aurocs[layer].get(objective, \"N/A\")  # Handle missing AUROC entries\n",
    "            print(f'  Objective: {objective}, Accuracy: {class_accuracies[layer][objective]}, AUROC: {auroc_str}')\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        if plot_confusion_matrices:\n",
    "            cm = confusion_matrix(test_labels, predictions, labels=list(objective_activations_dataset.keys()))\n",
    "            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(objective_activations_dataset.keys()))\n",
    "            disp.plot(cmap=plt.cm.Blues)\n",
    "            plt.title(f'Confusion Matrix for Layer {layer}')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "        # Plot class accuracies and AUROC for the layer\n",
    "        if plot_accuracies:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.barplot(x=list(class_accuracies[layer].keys()), y=list(class_accuracies[layer].values()), label='Accuracy')\n",
    "            plt.title(f'Class Accuracies for Layer {layer}')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xlabel('Objective')\n",
    "            plt.ylim(0, 1)  # Set y-axis limit from 0 to 1\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "        if plot_auroc_scores:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.barplot(x=list(class_aurocs[layer].keys()), y=list(class_aurocs[layer].values()), label='AUROC', alpha=0.5)\n",
    "            plt.title(f'Class AUROCs for Layer {layer}')\n",
    "            plt.ylabel('AUROC')\n",
    "            plt.xlabel('Objective')\n",
    "            plt.ylim(0, 1)  # Set y-axis limit from 0 to 1\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "\n",
    "    # Plot overall accuracies and AUROC scores\n",
    "    if plot_accuracies or plot_auroc_scores:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        x = list(accuracies.keys())\n",
    "        accuracy_values = list(accuracies.values())\n",
    "        auroc_values = list(auroc_scores.values())\n",
    "\n",
    "        bar_width = 0.35\n",
    "        x_indices = np.arange(len(x))\n",
    "\n",
    "        if plot_accuracies:\n",
    "            plt.bar(x_indices, accuracy_values, width=bar_width, label='Accuracy')\n",
    "        if plot_auroc_scores:\n",
    "            plt.bar(x_indices + bar_width, auroc_values, width=bar_width, label='AUROC')\n",
    "\n",
    "        plt.title('Probe Accuracies and AUROC by Layer')\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Layer')\n",
    "        plt.ylim(0, 1)  # Set y-axis limit from 0 to 1\n",
    "        plt.xticks(x_indices + bar_width / 2, x, rotation=45)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and setup environment\n",
    "difficulty = 'easy'\n",
    "# model = helpers.load_model(model_path=f\"../model_{difficulty}.pt\")\n",
    "model = helpers.load_interpretable_model(model_path=\"../model_interpretable.pt\")\n",
    "model_activations = helpers.ModelActivations(model)\n",
    "layer_paths = ['conv1a', 'conv2a', 'conv2b','conv3a', 'conv4a', 'pool4', 'fc1', 'fc2','fc3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv1a', 'conv2a', 'conv2b', 'conv3a', 'conv4a', 'pool4', 'fc1', 'fc2', 'fc3']\n"
     ]
    }
   ],
   "source": [
    "objective_activations_dataset = helpers.get_objective_activations(model_activations, layer_paths, 5000)\n",
    "del objective_activations_dataset['empty_maze'] # Get rid of empty maze dataset\n",
    "\n",
    "print(list(objective_activations_dataset['gem'].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot_probes(objective_activations_dataset, ['conv1a', 'conv2a', 'conv2b','conv3a', 'conv4a', 'pool4', 'fc1', 'fc2','fc3'], plot_confusion_matrices=True, plot_auroc_scores=True, plot_accuracies=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "procgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
