{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Probe Experiments\n",
    "A first try at generating data in a procgen maze environment, and using that to construct a linear probe to detect a \"cheese signal\" in the neural network layers of a pre-trained RL agent. \n",
    "\n",
    "## Initialize Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from procgen import ProcgenGym3Env\n",
    "from procgen_tools import maze\n",
    "from procgen_tools.models import load_policy\n",
    "from procgen_tools import maze as maze_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "save_dir = Path(\"maze_linear_probe_data\")\n",
    "if not save_dir.is_dir():\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gym environment will be created through `procgen-tools`. It provides a wrapper around the original procgen environment to make it compatible with gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prev_level_seed', 'prev_level_complete', 'level_seed', 'rgb'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 4242\n",
    "wrapped_venv = maze_api.create_venv(\n",
    "    num=1, start_level=int(seed), num_levels=0, num_threads = 4\n",
    ")\n",
    "wrapped_venv.env.get_info()[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CEnv lib_path=/home/gearspark/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/procgen/data/prebuilt/libenv.so options={'center_agent': True, 'use_generated_assets': False, 'use_monochrome_assets': False, 'restrict_themes': False, 'use_backgrounds': True, 'paint_vel_info': False, 'distribution_mode': 1, 'env_name': 'maze', 'num_levels': 0, 'start_level': 4242, 'num_actions': 15, 'use_sequential_levels': False, 'debug_mode': 0, 'rand_seed': 968779440, 'num_threads': 4, 'render_human': True, 'resource_root': '/home/gearspark/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/procgen/data/assets/'}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the procgen env directly like so\n",
    "wrapped_venv.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Agent\n",
    "Download one of the agents from the [trained model files](https://drive.google.com/drive/folders/1Ig7bzRlieyYFcdKL_PM-guSWR8WryDOL). I used `maze_I/model_rand_region_5` without knowing about the performance of this agent. Do not forget to rename the model file or change the filename below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = load_policy('../model_rand_region_15.pth', action_size=15, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_step_log(model, environment, step, act, obs, rew, done, info):\n",
    "    \"\"\"Just a dummy log method\"\"\"\n",
    "\n",
    "def dummy_done_log():\n",
    "    \"\"\"Just a dummy log method\"\"\"\n",
    "\n",
    "\n",
    "def run_episode(model, maze_environment, argmax=True, max_time_steps=256, on_step=dummy_step_log, on_done=dummy_done_log):\n",
    "    model.eval()  # Switch off gradient tracking and other training time mechanisms\n",
    "    obs = maze_environment.reset()\n",
    "\n",
    "    for step in range(max_time_steps):\n",
    "        out, _ = model(torch.FloatTensor(obs))\n",
    "        if argmax:\n",
    "            act = out.probs.argmax(dim=-1).numpy()\n",
    "        else:\n",
    "            act = out.sample().numpy()\n",
    "        obs, rew, done, info = maze_environment.step(act)\n",
    "        on_step(model, maze_environment, step, act, obs, rew, done, info)\n",
    "        if done:\n",
    "            break\n",
    "    on_done()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test episode\n",
    "run_episode(policy, wrapped_venv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: manual reset ignored\n"
     ]
    }
   ],
   "source": [
    "# Define a custom logging function\n",
    "class CustomLogger:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def log(self, model, environment, step, act, obs, rew, done, info):\n",
    "        self.observations.append(obs)\n",
    "        self.rewards.append(float(rew[0]))\n",
    "\n",
    "    def reset_logs(self):\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "logger = CustomLogger()\n",
    "run_episode(policy, wrapped_venv, on_step=logger.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n obs: 244, n rewards: 244, total reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "obs = logger.observations\n",
    "rews = logger.rewards\n",
    "print(f\"n obs: {len(obs)}, n rewards: {len(rews)}, total reward: {sum(rews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Model Activations\n",
    "Activations can be [logged with `circrl`](https://github.com/montemac/circrl/tree/main). The hook manager logs a single activation by default. Since we want to have activations spread over the entire eposide we will use a custom hook manager that can be reset between episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_layer_names(model):\n",
    "    return [name for name, module in model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'embedder', 'embedder.block1', 'embedder.block1.conv', 'embedder.block1.maxpool']\n"
     ]
    }
   ],
   "source": [
    "names = get_model_layer_names(policy)\n",
    "print(names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a hook to log the activations of the corresponding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before logging\n",
      "dict_keys([])\n",
      "After logging\n",
      "(Categorical(logits: torch.Size([1, 15])), tensor([8.7205], grad_fn=<ViewBackward0>))\n",
      "dict_keys(['embedder.block1.conv', 'embedder.block1', 'embedder'])\n"
     ]
    }
   ],
   "source": [
    "from circrl import hooks\n",
    "\n",
    "policy_hook = hooks.HookManager(\n",
    "    model=policy,\n",
    "    cache=names[1:4]\n",
    ")\n",
    "\n",
    "print(\"Before logging\")\n",
    "print(policy_hook.cache_results.keys())\n",
    "\n",
    "# Run an environment update in the policy hook context to start logging\n",
    "with policy_hook:\n",
    "    obs = wrapped_venv.reset()\n",
    "    output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"After logging\")\n",
    "print(output)\n",
    "print(policy_hook.cache_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful! In the default log setting the `cache_results` gets overwritten every time the policy is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log size after a single execution:\n",
      "\ttorch.Size([1, 256])\n",
      "Log size after a multiple executions:\n",
      "\ttorch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "with policy_hook:\n",
    "    obs = wrapped_venv.reset()\n",
    "    output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"log size after a single execution:\")\n",
    "print(f\"\\t{policy_hook.cache_results[names[1]].size()}\")\n",
    "\n",
    "with policy_hook:    \n",
    "    obs = wrapped_venv.reset()\n",
    "    for _ in range(10):\n",
    "        output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"Log size after a multiple executions:\")\n",
    "print(f\"\\t{policy_hook.cache_results[names[1]].size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Training Data\n",
    "To gather training data we need to run multiple episodes and store the activations of the model alongside the environment parameters. Procgen-tools provides some convenience functions to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_maze_training_data(model, n_episodes, on_episode_step, on_episode_done):\n",
    "    max_seed = int(1e9)\n",
    "    seeds = np.random.default_rng().choice(max_seed, size=n_episodes, replace=False)\n",
    "\n",
    "    for seed in tqdm(seeds):\n",
    "        wrapped_venv = maze_api.create_venv(\n",
    "            num=1, start_level=int(seed), num_levels=0, num_threads = 4\n",
    "        )  # Convenience functions\n",
    "\n",
    "        run_episode(model, wrapped_venv, on_step=on_episode_step, on_done=on_episode_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_maze_training_data(policy, 1, dummy_step_log, dummy_done_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a system to log activations and environment variables persistently. It would be best to avoid having to edit code related to model execution when logging to avoid introducting bugs. We can use something like the following logging setup to log episodic data to disk at the end of every episode. It combines the previous logging function with hooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class ProbeDataLogger:\n",
    "    def __init__(self, model, save_file_path=\"\"):\n",
    "        self._init_logs()\n",
    "\n",
    "        self.model = model\n",
    "        layer_names = get_model_layer_names(model)\n",
    "        self.model_hook = hooks.HookManager(\n",
    "            model=model,\n",
    "            cache=names\n",
    "        )\n",
    "        save_file_path = Path(save_file_path)\n",
    "        self._file_path = save_file_path\n",
    "        self._file_extension = \".pkl\"\n",
    "        self._current_ep = 0\n",
    "\n",
    "    def log(self, model, environment, step, act, obs, rew, done, info):\n",
    "        self.observations.append(obs)\n",
    "        self.rewards.append(rew)\n",
    "        self.activations.append(self.model_hook.cache_results)\n",
    "\n",
    "        # Using these functions only works because the environment is wrapped in\n",
    "        # the training data generation method\n",
    "        state = maze_api.state_from_venv(wrapped_venv)\n",
    "        grid = maze_api.get_grid(state.state_vals)\n",
    "        self.cheese_positions.append(maze_api.get_cheese_pos(grid))\n",
    "\n",
    "    def done(self):\n",
    "        self._current_ep += 1\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"Flush data in log buffer to file and reset buffer\"\"\"\n",
    "        save_path = self._file_path / Path(\"ep_\" + str(self._current_ep) + str(self._file_extension))\n",
    "        print(save_path)\n",
    "        self._save_logs_to_file(save_path)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._init_logs()\n",
    "\n",
    "    def _save_logs_to_file(self, file_path):\n",
    "        write_mode = 'wb' \n",
    "\n",
    "        obs = torch.tensor(self.observations)\n",
    "        rew = torch.tensor(self.rewards)\n",
    "        cheese_positions = torch.tensor(self.cheese_positions)\n",
    "        \n",
    "        with open(file_path, write_mode) as file:\n",
    "            pickle.dump(self.observations, file)\n",
    "            pickle.dump(self.rewards, file)\n",
    "            pickle.dump(self.activations, file)\n",
    "            pickle.dump(self.cheese_positions, file)\n",
    "\n",
    "    def _init_logs(self):\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "        self.activations = []\n",
    "        self.cheese_positions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/ep_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:05<00:02,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/ep_2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/ep_3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logs = ProbeDataLogger(policy, save_file_path=save_dir)\n",
    "with logs.model_hook:\n",
    "    generate_maze_training_data(policy, 3, logs.log, logs.done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load back the data. Including the loading functionality in the logging class might be convenient, but here I will simply recover it manually as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded activations: 3\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_logs(file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - file_path (str): The path to the pickle file.\n",
    "    \n",
    "    Returns:\n",
    "    - observations (list): Environment observations.\n",
    "    - rewards (list): Environment rewards.\n",
    "    - activations (list): The list of activations unpickled from the file.\n",
    "    - cheese_positions (list): Cheese position tensors in x, y.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Unpickle in the order they were saved: observations, rewards, activations\n",
    "        observations = pickle.load(file)\n",
    "        rewards = pickle.load(file)\n",
    "        activations = pickle.load(file)\n",
    "        cheese_positions = pickle.load(file)\n",
    "    \n",
    "    return observations, rewards, activations, cheese_positions\n",
    "\n",
    "# Example usage\n",
    "file_path = save_dir / 'ep_1.pkl'\n",
    "_, _, activations, _ = load_logs(file_path)\n",
    "print(\"Loaded activations:\", len(activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Linear Probe\n",
    "Training a linear probe requires choosing what layers we are probing, and with respect to what target. That in turn determines the structure of the probe.\n",
    "\n",
    "### Generate Training Data\n",
    "\n",
    "In this case we will probe a early convolutional layer in the network for cheese position signal as an example. First we load a model and generate some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log directory\n",
    "import os\n",
    "\n",
    "log_dir = save_dir / Path(\"logs\")\n",
    "\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and create logger\n",
    "probe_policy = load_policy('model_rand_region_15.pth', action_size=15, device=torch.device('cpu'))\n",
    "logger = ProbeDataLogger(probe_policy, save_file_path=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:06,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:16,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:01<00:14,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:02<00:13,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_4.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:02<00:09,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_5.pkl\n",
      "maze_linear_probe_data/logs/ep_6.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:03<00:13,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_7.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:04<00:14,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_8.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:05<00:12,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_8.pkl\n",
      "maze_linear_probe_data/logs/ep_9.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:05<00:12,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_10.pkl\n",
      "maze_linear_probe_data/logs/ep_11.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:06<00:09,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_12.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:07<00:10,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_13.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:07<00:08,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_14.pkl\n",
      "maze_linear_probe_data/logs/ep_15.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:08<00:06,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_16.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:08<00:05,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_17.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:09<00:05,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_18.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:10<00:07,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_18.pkl\n",
      "maze_linear_probe_data/logs/ep_19.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:11<00:06,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_20.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:16<00:17,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_21.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:16<00:11,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_22.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:17<00:08,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_23.pkl\n",
      "maze_linear_probe_data/logs/ep_24.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:17<00:03,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_25.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:21<00:06,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_26.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:22<00:03,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_27.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:23<00:02,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_28.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:23<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_28.pkl\n",
      "maze_linear_probe_data/logs/ep_29.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:24<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_30.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "with logger.model_hook:\n",
    "    generate_maze_training_data(probe_policy, 30, logger.log, logger.done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing For Probe Training\n",
    "\n",
    "We need to load the episode data, process it into a format suitable for the training process and then train [a linear probe](https://github.com/montemac/circrl/blob/33534f2f78547b38172e3a4f0d682bc8b5b46b4f/src/circrl/probing.py#L47). For this particular version we can process the episodic data into files of numpy arrays or pytorch tensors. We'll choose pytorch tensors here. \n",
    "\n",
    "Alternatively the data could be batched in files to reduce read/write overhead. It might be worth doing if compute bottlenecks become an obvious concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = save_dir / Path(\"logs\")\n",
    "target = save_dir / Path(\"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
