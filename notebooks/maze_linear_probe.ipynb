{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Probe Experiments\n",
    "A first try at generating data in a procgen maze environment, and using that to construct a linear probe to detect a \"cheese signal\" in the neural network layers of a pre-trained RL agent. \n",
    "\n",
    "## Initialize Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from procgen import ProcgenGym3Env\n",
    "from procgen_tools import maze\n",
    "from procgen_tools.models import load_policy\n",
    "from procgen_tools import maze as maze_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import io\n",
    "\n",
    "save_dir = Path(\"maze_linear_probe_data\")\n",
    "if not save_dir.is_dir():\n",
    "    os.mkdir(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gym environment will be created through `procgen-tools`. It provides a wrapper around the original procgen environment to make it compatible with gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prev_level_seed', 'prev_level_complete', 'level_seed', 'rgb'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 4242\n",
    "wrapped_venv = maze_api.create_venv(\n",
    "    num=1, start_level=int(seed), num_levels=0, num_threads = 4\n",
    ")\n",
    "wrapped_venv.env.get_info()[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CEnv lib_path=/home/gearspark/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/procgen/data/prebuilt/libenv.so options={'center_agent': True, 'use_generated_assets': False, 'use_monochrome_assets': False, 'restrict_themes': False, 'use_backgrounds': True, 'paint_vel_info': False, 'distribution_mode': 1, 'env_name': 'maze', 'num_levels': 0, 'start_level': 4242, 'num_actions': 15, 'use_sequential_levels': False, 'debug_mode': 0, 'rand_seed': 1737695600, 'num_threads': 4, 'render_human': True, 'resource_root': '/home/gearspark/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/procgen/data/assets/'}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the procgen env directly like so\n",
    "wrapped_venv.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Agent\n",
    "Download one of the agents from the [trained model files](https://drive.google.com/drive/folders/1Ig7bzRlieyYFcdKL_PM-guSWR8WryDOL). I used `maze_I/model_rand_region_5` without knowing about the performance of this agent. Do not forget to rename the model file or change the filename below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = load_policy('model_rand_region_15.pth', action_size=15, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_step_log(model, environment, step, act, obs, rew, done, info):\n",
    "    \"\"\"Just a dummy log method\"\"\"\n",
    "\n",
    "def dummy_done_log():\n",
    "    \"\"\"Just a dummy log method\"\"\"\n",
    "\n",
    "\n",
    "def run_episode(model, maze_environment, argmax=True, max_time_steps=256, on_step=dummy_step_log, on_done=dummy_done_log):\n",
    "    model.eval()  # Switch off gradient tracking and other training time mechanisms\n",
    "    obs = maze_environment.reset()\n",
    "\n",
    "    for step in range(max_time_steps):\n",
    "        out, _ = model(torch.FloatTensor(obs))\n",
    "        if argmax:\n",
    "            act = out.probs.argmax(dim=-1).numpy()\n",
    "        else:\n",
    "            act = out.sample().numpy()\n",
    "        obs, rew, done, info = maze_environment.step(act)\n",
    "        on_step(model, maze_environment, step, act, obs, rew, done, info)\n",
    "        if done:\n",
    "            break\n",
    "    on_done()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test episode\n",
    "run_episode(policy, wrapped_venv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom logging function\n",
    "class CustomLogger:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def log(self, model, environment, step, act, obs, rew, done, info):\n",
    "        self.observations.append(obs)\n",
    "        self.rewards.append(float(rew[0]))\n",
    "\n",
    "    def reset_logs(self):\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "logger = CustomLogger()\n",
    "run_episode(policy, wrapped_venv, on_step=logger.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n obs: 17, n rewards: 17, total reward: 10.0\n"
     ]
    }
   ],
   "source": [
    "obs = logger.observations\n",
    "rews = logger.rewards\n",
    "print(f\"n obs: {len(obs)}, n rewards: {len(rews)}, total reward: {sum(rews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Model Activations\n",
    "Activations can be [logged with `circrl`](https://github.com/montemac/circrl/tree/main). The hook manager logs a single activation by default. Since we want to have activations spread over the entire eposide we will use a custom hook manager that can be reset between episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_layer_names(model):\n",
    "    return [name for name, module in model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'embedder', 'embedder.block1', 'embedder.block1.conv', 'embedder.block1.maxpool']\n"
     ]
    }
   ],
   "source": [
    "names = get_model_layer_names(policy)\n",
    "print(names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a hook to log the activations of the corresponding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before logging\n",
      "dict_keys([])\n",
      "After logging\n",
      "(Categorical(logits: torch.Size([1, 15])), tensor([8.1030], grad_fn=<ViewBackward0>))\n",
      "dict_keys(['embedder.block1.conv', 'embedder.block1', 'embedder'])\n"
     ]
    }
   ],
   "source": [
    "from circrl import hooks\n",
    "\n",
    "policy_hook = hooks.HookManager(\n",
    "    model=policy,\n",
    "    cache=names[1:4]\n",
    ")\n",
    "\n",
    "print(\"Before logging\")\n",
    "print(policy_hook.cache_results.keys())\n",
    "\n",
    "# Run an environment update in the policy hook context to start logging\n",
    "with policy_hook:\n",
    "    obs = wrapped_venv.reset()\n",
    "    output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"After logging\")\n",
    "print(output)\n",
    "print(policy_hook.cache_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful! In the default log setting the `cache_results` gets overwritten every time the policy is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log size after a single execution:\n",
      "\ttorch.Size([1, 256])\n",
      "Log size after a multiple executions:\n",
      "\ttorch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "with policy_hook:\n",
    "    obs = wrapped_venv.reset()\n",
    "    output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"log size after a single execution:\")\n",
    "print(f\"\\t{policy_hook.cache_results[names[1]].size()}\")\n",
    "\n",
    "with policy_hook:    \n",
    "    obs = wrapped_venv.reset()\n",
    "    for _ in range(10):\n",
    "        output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"Log size after a multiple executions:\")\n",
    "print(f\"\\t{policy_hook.cache_results[names[1]].size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Training Data\n",
    "To gather training data we need to run multiple episodes and store the activations of the model alongside the environment parameters. Procgen-tools provides some convenience functions to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_maze_training_data(model, n_episodes, on_episode_step, on_episode_done):\n",
    "    max_seed = int(1e9)\n",
    "    seeds = np.random.default_rng().choice(max_seed, size=n_episodes, replace=False)\n",
    "\n",
    "    for seed in tqdm(seeds):\n",
    "        wrapped_venv = maze_api.create_venv(\n",
    "            num=1, start_level=int(seed), num_levels=0, num_threads = 4\n",
    "        )  # Convenience functions\n",
    "\n",
    "        run_episode(model, wrapped_venv, on_step=on_episode_step, on_done=on_episode_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_maze_training_data(policy, 1, dummy_step_log, dummy_done_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a system to log activations and environment variables persistently. It would be best to avoid having to edit code related to model execution when logging to avoid introducting bugs. We can use something like the following logging setup to log episodic data to disk at the end of every episode. It combines the previous logging function with hooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class ProbeDataLogger:\n",
    "    def __init__(self, model, save_file_path=\"\"):\n",
    "        self._init_logs()\n",
    "\n",
    "        self.model = model\n",
    "        layer_names = get_model_layer_names(model)\n",
    "        self.model_hook = hooks.HookManager(\n",
    "            model=model,\n",
    "            cache=names\n",
    "        )\n",
    "        save_file_path = Path(save_file_path)\n",
    "        self._file_path = save_file_path\n",
    "        self._file_extension = \".pkl\"\n",
    "        self._current_ep = 0\n",
    "\n",
    "    def log(self, model, environment, step, act, obs, rew, done, info):\n",
    "        self.observations.append(obs)\n",
    "        self.rewards.append(rew)\n",
    "        self.activations.append(self.model_hook.cache_results)\n",
    "\n",
    "        # Using these functions only works because the environment is wrapped in\n",
    "        # the training data generation method\n",
    "        state = maze_api.state_from_venv(wrapped_venv)\n",
    "        grid = maze_api.get_grid(state.state_vals)\n",
    "        self.cheese_positions.append(maze_api.get_cheese_pos(grid))\n",
    "\n",
    "    def done(self):\n",
    "        self._current_ep += 1\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"Flush data in log buffer to file and reset buffer\"\"\"\n",
    "        save_path = self._file_path / Path(\"ep_\" + str(self._current_ep) + str(self._file_extension))\n",
    "        print(save_path)\n",
    "        self._save_logs_to_file(save_path)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._init_logs()\n",
    "\n",
    "    def _save_logs_to_file(self, file_path):\n",
    "        write_mode = 'wb' \n",
    "\n",
    "        obs = torch.tensor(self.observations)\n",
    "        rew = torch.tensor(self.rewards)\n",
    "        cheese_positions = torch.tensor(self.cheese_positions)\n",
    "        \n",
    "        with open(file_path, write_mode) as file:\n",
    "            pickle.dump(self.observations, file)\n",
    "            pickle.dump(self.rewards, file)\n",
    "            pickle.dump(self.activations, file)\n",
    "            pickle.dump(self.cheese_positions, file)\n",
    "\n",
    "    def _init_logs(self):\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "        self.activations = []\n",
    "        self.cheese_positions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/tmp/ipykernel_57399/25385720.py:46: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  obs = torch.tensor(self.observations)\n",
      " 67%|██████▋   | 2/3 [00:00<00:00,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/ep_1.pkl\n",
      "maze_linear_probe_data/ep_2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/ep_3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logs = ProbeDataLogger(policy, save_file_path=save_dir)\n",
    "with logs.model_hook:\n",
    "    generate_maze_training_data(policy, 3, logs.log, logs.done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load back the data. Including the loading functionality in the logging class might be convenient, but here I will simply recover it manually as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded activations: 32\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_logs(file_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    - file_path (str): The path to the pickle file.\n",
    "    \n",
    "    Returns:\n",
    "    - observations (list): Environment observations.\n",
    "    - rewards (list): Environment rewards.\n",
    "    - activations (list): The list of activations unpickled from the file.\n",
    "    - cheese_positions (list): Cheese position tensors in x, y.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Unpickle in the order they were saved: observations, rewards, activations\n",
    "        observations = pickle.load(file)\n",
    "        rewards = pickle.load(file)\n",
    "        activations = pickle.load(file)\n",
    "        cheese_positions = pickle.load(file)\n",
    "    \n",
    "    return observations, rewards, activations, cheese_positions\n",
    "\n",
    "# Example usage\n",
    "file_path = save_dir / 'ep_1.pkl'\n",
    "_, _, activations, _ = load_logs(file_path)\n",
    "print(\"Loaded activations:\", len(activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Linear Probe\n",
    "Training a linear probe requires choosing what layers we are probing, and with respect to what target. That in turn determines the structure of the probe.\n",
    "\n",
    "### Generate Training Data\n",
    "\n",
    "In this case we will probe a early convolutional layer in the network for cheese position signal as an example. First we load a model and generate some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log directory\n",
    "import os\n",
    "\n",
    "log_dir = save_dir / Path(\"logs\")\n",
    "\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and create logger\n",
    "probe_policy = load_policy('model_rand_region_15.pth', action_size=15, device=torch.device('cpu'))\n",
    "logger = ProbeDataLogger(probe_policy, save_file_path=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:11,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_1.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:17,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_2.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:02<00:26,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_3.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:03<00:13,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_4.pkl\n",
      "maze_linear_probe_data/logs/ep_5.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:04<00:21,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_6.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:05<00:11,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_7.pkl\n",
      "maze_linear_probe_data/logs/ep_8.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:05<00:07,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_9.pkl\n",
      "maze_linear_probe_data/logs/ep_10.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:06<00:10,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_11.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:07<00:08,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_12.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:07<00:08,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_13.pkl\n",
      "maze_linear_probe_data/logs/ep_14.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:08<00:06,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_15.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:08<00:05,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_16.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:09<00:06,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_17.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:10<00:07,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_18.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:11<00:07,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_19.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:16<00:21,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_20.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:22<00:27,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_21.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:27<00:18,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_22.pkl\n",
      "maze_linear_probe_data/logs/ep_23.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:28<00:12,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_24.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:28<00:04,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_25.pkl\n",
      "maze_linear_probe_data/logs/ep_26.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:29<00:03,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_27.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:30<00:02,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_28.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:31<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_29.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:32<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maze_linear_probe_data/logs/ep_30.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "with logger.model_hook:\n",
    "    generate_maze_training_data(probe_policy, 30, logger.log, logger.done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probe Training\n",
    "\n",
    "We need to load the episode data, process it into a format suitable for the training process and then train [a linear probe](https://github.com/montemac/circrl/blob/33534f2f78547b38172e3a4f0d682bc8b5b46b4f/src/circrl/probing.py#L47). For this particular version we can process the episodic data into files of numpy arrays or pytorch tensors. We'll choose pytorch tensors here. \n",
    "\n",
    "Alternatively the data could be batched in files to reduce read/write overhead. It might be worth doing if compute bottlenecks become an obvious concern.\n",
    "\n",
    "#### Regression Probe\n",
    "\n",
    "Using the Ridge Regressor provided by SKLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_regression_episode(model: Ridge, layer_name: str, episode_data_path: Path, random_seed: int = 42):\n",
    "    _, _, layer_activations, cheese_positions = load_logs(episode_data_path)\n",
    "    assert layer_name in layer_activations[0].keys(), \"Cannot find layer '{layer_name}' in activations\"\n",
    "    \n",
    "    test_size = 0.2\n",
    "    inputs = torch.stack([act[layer_name] for act in layer_activations], dim=0)\n",
    "    inputs = inputs.detach().numpy()\n",
    "    targets = torch.tensor(cheese_positions)\n",
    "    targets = targets.detach().numpy()\n",
    "\n",
    "    inputs_train, inputs_test, targets_train, targets_test = train_test_split(\n",
    "        inputs, targets, test_size=test_size, random_state=random_seed\n",
    "    )\n",
    "\n",
    "    model.fit(inputs_train, targets_train)\n",
    "    result = {\n",
    "        \"train_score\": model.score(inputs_train, targets_train),\n",
    "        \"test_score\": model.score(inputs_test, targets_test),\n",
    "        \"x_train\": inputs_train,\n",
    "        \"y_train\": targets_train,\n",
    "        \"x_test\": inputs_test,\n",
    "        \"y_test\": targets_test,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 5. Ridge expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb Cell 35\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m layer_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39membedder.block1.conv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data_path \u001b[39m=\u001b[39m save_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mep_1.pkl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m mdl \u001b[39m=\u001b[39m train_regression_episode(mdl, layer_name, data_path, seed)\n",
      "\u001b[1;32m/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m inputs_train, inputs_test, targets_train, targets_test \u001b[39m=\u001b[39m train_test_split(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     inputs, targets, test_size\u001b[39m=\u001b[39mtest_size, random_state\u001b[39m=\u001b[39mrandom_seed\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(inputs_train, targets_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m result \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain_score\u001b[39m\u001b[39m\"\u001b[39m: model\u001b[39m.\u001b[39mscore(inputs_train, targets_train),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m: model\u001b[39m.\u001b[39mscore(inputs_test, targets_test),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gearspark/Projects/ai-safety-camp-2024-model-agents/notebooks/maze_linear_probe.ipynb#X46sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m }\n",
      "File \u001b[0;32m~/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:1153\u001b[0m, in \u001b[0;36mRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit Ridge regression model.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \n\u001b[1;32m   1135\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m _accept_sparse \u001b[39m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[39m.\u001b[39missparse(X), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msolver)\n\u001b[0;32m-> 1153\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m   1154\u001b[0m     X,\n\u001b[1;32m   1155\u001b[0m     y,\n\u001b[1;32m   1156\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m_accept_sparse,\n\u001b[1;32m   1157\u001b[0m     dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[1;32m   1158\u001b[0m     multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1159\u001b[0m     y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1161\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/sklearn/utils/validation.py:1192\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1188\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1189\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     )\n\u001b[0;32m-> 1192\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1193\u001b[0m     X,\n\u001b[1;32m   1194\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1195\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1196\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1197\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1198\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1199\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1200\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1201\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1202\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1203\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1204\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1205\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1206\u001b[0m )\n\u001b[1;32m   1208\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1210\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/sklearn/utils/validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    993\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    994\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    995\u001b[0m     )\n\u001b[1;32m    996\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nd \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m--> 997\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m         \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1000\u001b[0m     )\n\u001b[1;32m   1002\u001b[0m \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m   1003\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1004\u001b[0m         array,\n\u001b[1;32m   1005\u001b[0m         input_name\u001b[39m=\u001b[39minput_name,\n\u001b[1;32m   1006\u001b[0m         estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[1;32m   1007\u001b[0m         allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1008\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 5. Ridge expected <= 2."
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "mdl = Ridge(random_state=seed)\n",
    "layer_name = \"embedder.block1.conv\"\n",
    "data_path = save_dir / \"logs\" / \"ep_1.pkl\"\n",
    "\n",
    "mdl = train_regression_episode(mdl, layer_name, data_path, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
