{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Probe Experiments\n",
    "A first try at generating data in a procgen maze environment, and using that to construct a linear probe to detect a \"cheese signal\" in the neural network layers of a pre-trained RL agent. \n",
    "\n",
    "## Initialize Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from procgen import ProcgenGym3Env\n",
    "from procgen_tools import maze\n",
    "from procgen_tools.models import load_policy\n",
    "from procgen_tools import maze as maze_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gym environment will be created through `procgen-tools`. It provides a wrapper around the original procgen environment to make it compatible with gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prev_level_seed', 'prev_level_complete', 'level_seed', 'rgb'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 4242\n",
    "wrapped_venv = maze_api.create_venv(\n",
    "    num=1, start_level=int(seed), num_levels=0, num_threads = 4\n",
    ")\n",
    "wrapped_venv.env.get_info()[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CEnv lib_path=/home/gearspark/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/procgen/data/prebuilt/libenv.so options={'center_agent': True, 'use_generated_assets': False, 'use_monochrome_assets': False, 'restrict_themes': False, 'use_backgrounds': True, 'paint_vel_info': False, 'distribution_mode': 1, 'env_name': 'maze', 'num_levels': 0, 'start_level': 4242, 'num_actions': 15, 'use_sequential_levels': False, 'debug_mode': 0, 'rand_seed': 149530471, 'num_threads': 4, 'render_human': True, 'resource_root': '/home/gearspark/Projects/ai-safety-camp-2024-model-agents/venv/lib/python3.10/site-packages/procgen/data/assets/'}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the procgen env directly like so\n",
    "wrapped_venv.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Agent\n",
    "Download one of the agents from the [trained model files](https://drive.google.com/drive/folders/1Ig7bzRlieyYFcdKL_PM-guSWR8WryDOL). I used `maze_I/model_rand_region_5` without knowing about the performance of this agent. Do not forget to rename the model file or change the filename below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = load_policy('model_rand.pth', action_size=15, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_step_log(model, environment, step, act, obs, rew, done, info):\n",
    "    \"\"\"Just a dummy log method\"\"\"\n",
    "\n",
    "def dummy_done_log():\n",
    "    \"\"\"Just a dummy log method\"\"\"\n",
    "\n",
    "\n",
    "def run_episode(model, maze_environment, argmax=True, max_time_steps=256, on_step=dummy_step_log, on_done=dummy_done_log):\n",
    "    model.eval()  # Switch off gradient tracking and other training time mechanisms\n",
    "    obs = maze_environment.reset()\n",
    "\n",
    "    for step in range(max_time_steps):\n",
    "        out, _ = model(torch.FloatTensor(obs))\n",
    "        if argmax:\n",
    "            act = out.probs.argmax(dim=-1).numpy()\n",
    "        else:\n",
    "            act = out.sample().numpy()\n",
    "        obs, rew, done, info = maze_environment.step(act)\n",
    "        on_step(model, maze_environment, step, act, obs, rew, done, info)\n",
    "        if done:\n",
    "            break\n",
    "    on_done()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a test episode\n",
    "run_episode(policy, wrapped_venv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: manual reset ignored\n"
     ]
    }
   ],
   "source": [
    "# Define a custom logging function\n",
    "class CustomLogger:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def log(self, model, environment, step, act, obs, rew, done, info):\n",
    "        self.observations.append(obs)\n",
    "        self.rewards.append(float(rew[0]))\n",
    "\n",
    "    def reset_logs(self):\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "logger = CustomLogger()\n",
    "run_episode(policy, wrapped_venv, on_step=logger.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n obs: 244, n rewards: 244, total reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "obs = logger.observations\n",
    "rews = logger.rewards\n",
    "print(f\"n obs: {len(obs)}, n rewards: {len(rews)}, total reward: {sum(rews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Model Activations\n",
    "Activations can be logged with `circrl`. The hook manager logs a single activation by default. Since we want to have activations spread over the entire eposide we will use a custom hook manager that can be reset between episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_layer_names(model):\n",
    "    return [name for name, module in model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'embedder', 'embedder.block1', 'embedder.block1.conv', 'embedder.block1.maxpool']\n"
     ]
    }
   ],
   "source": [
    "names = get_model_layer_names(policy)\n",
    "print(names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a hook to log the activations of the corresponding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before logging\n",
      "dict_keys([])\n",
      "After logging\n",
      "(Categorical(logits: torch.Size([1, 15])), tensor([8.4563], grad_fn=<ViewBackward0>))\n",
      "dict_keys(['embedder.block1.conv', 'embedder.block1', 'embedder'])\n"
     ]
    }
   ],
   "source": [
    "from circrl import hooks\n",
    "\n",
    "policy_hook = hooks.HookManager(\n",
    "    model=policy,\n",
    "    cache=names[1:4]\n",
    ")\n",
    "\n",
    "print(\"Before logging\")\n",
    "print(policy_hook.cache_results.keys())\n",
    "\n",
    "# Run an environment update in the policy hook context to start logging\n",
    "with policy_hook:\n",
    "    obs = wrapped_venv.reset()\n",
    "    output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"After logging\")\n",
    "print(output)\n",
    "print(policy_hook.cache_results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful! In the default log setting the `cache_results` gets overwritten every time the policy is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log size after a single execution:\n",
      "\ttorch.Size([1, 256])\n",
      "Log size after a multiple executions:\n",
      "\ttorch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "with policy_hook:\n",
    "    obs = wrapped_venv.reset()\n",
    "    output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"log size after a single execution:\")\n",
    "print(f\"\\t{policy_hook.cache_results[names[1]].size()}\")\n",
    "\n",
    "with policy_hook:    \n",
    "    obs = wrapped_venv.reset()\n",
    "    for _ in range(10):\n",
    "        output = policy(torch.FloatTensor(obs))\n",
    "\n",
    "print(\"Log size after a multiple executions:\")\n",
    "print(f\"\\t{policy_hook.cache_results[names[1]].size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Training Data\n",
    "To gather training data we need to run multiple episodes and store the activations of the model alongside the environment parameters. Procgen-tools provides some convenience functions to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_maze_training_data(model, n_episodes, on_episode_step, on_episode_done):\n",
    "    max_seed = int(1e9)\n",
    "    seeds = np.random.default_rng().choice(max_seed, size=n_episodes, replace=False)\n",
    "\n",
    "    for seed in tqdm(seeds):\n",
    "        wrapped_venv = maze_api.create_venv(\n",
    "            num=1, start_level=int(seed), num_levels=0, num_threads = 4\n",
    "        )  # Convenience functions\n",
    "        state = maze_api.state_from_venv(wrapped_venv)\n",
    "        grid = maze_api.get_grid(state.state_vals)\n",
    "        cheese_pos = maze_api.get_cheese_pos(grid)\n",
    "\n",
    "        run_episode(model, wrapped_venv, on_step=on_episode_step, on_done=on_episode_done)\n",
    "\n",
    "    print(cheese_pos)\n",
    "    print(len(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 21)\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_maze_training_data(policy, 1, dummy_step_log, dummy_done_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a system to log activations and environment variables persistently. It would be best to avoid having to edit code related to model execution when logging to avoid introducting bugs. We can use something like the following logging setup to log episodic data to disk at the end of every episode. It combines the previous logging function with hooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class ProbeDataLogger:\n",
    "    def __init__(self, model, save_file_path=\"\"):\n",
    "        self._init_logs()\n",
    "\n",
    "        self.model = model\n",
    "        layer_names = get_model_layer_names(model)\n",
    "        self.model_hook = hooks.HookManager(\n",
    "            model=model,\n",
    "            cache=names\n",
    "        )\n",
    "        self._file_name = save_file_path + \"probe_logs\"\n",
    "        self._file_extension = \".pkl\"\n",
    "        self._current_ep = 0\n",
    "\n",
    "    def log(self, model, environment, step, act, obs, rew, done, info):\n",
    "        self.observations.append(obs)\n",
    "        self.rewards.append(rew)\n",
    "        self.activations.append(self.model_hook.cache_results)\n",
    "\n",
    "    def done(self):\n",
    "        self._current_ep += 1\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        \"\"\"Flush data in log buffer to file and reset buffer\"\"\"\n",
    "        self._save_logs_to_file(self._file_name + \"_\" + str(self._current_ep) + self._file_extension)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self._init_logs()\n",
    "\n",
    "    def _save_logs_to_file(self, file_path):\n",
    "        write_mode = 'wb' \n",
    "        with open(file_path, write_mode) as file:\n",
    "            pickle.dump(self.observations, file)\n",
    "            pickle.dump(self.rewards, file)\n",
    "            pickle.dump(self.activations, file)\n",
    "\n",
    "    def _init_logs(self):\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "        self.activations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 6)\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "logs = ProbeDataLogger(policy)\n",
    "with logs.model_hook:\n",
    "    generate_maze_training_data(policy, 3, logs.log, logs.done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load back the data. Including the loading functionality in the logging class might be convenient, but here I will simply recover it manually as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded activations: 5\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_activations(file_path):\n",
    "    \"\"\"\n",
    "    Loads the activations from a specified pickle file.\n",
    "    \n",
    "    Args:\n",
    "    - file_path (str): The path to the pickle file.\n",
    "    \n",
    "    Returns:\n",
    "    - activations (list): The list of activations unpickled from the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Unpickle in the order they were saved: observations, rewards, activations\n",
    "        observations = pickle.load(file)\n",
    "        rewards = pickle.load(file)\n",
    "        activations = pickle.load(file)\n",
    "    \n",
    "    return activations\n",
    "\n",
    "# Example usage\n",
    "file_path = 'probe_logs_1.pkl'\n",
    "activations = load_activations(file_path)\n",
    "print(\"Loaded activations:\", len(activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A Linear Probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
