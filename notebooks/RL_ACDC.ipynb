{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a056713-58b7-4984-b955-540b4b6f8a1a",
   "metadata": {},
   "source": [
    "1. __init__ (Initialization)\n",
    "Original Usage: Initializes experiment with configuration for metrics, model, dataset, and hooks.\n",
    "RL Application: Initialize with the RL model, environment, and possibly a policy or value network specifically targeted for modification.\n",
    "\n",
    "2. verify_model_setup (Model Configuration Verification)\n",
    "Original Usage: Checks that the model configuration is correct for the intended experiment setup.\n",
    "RL Application: Verify that the RL model or specific network components (like CNN or MLP layers within the policy network) are accessible and correctly configured for hooking and manipulation.\n",
    "\n",
    "3. update_cur_metric (Metric Update)\n",
    "Original Usage: Update and log the current metric based on the dataset.\n",
    "RL Application: Update based on RL-specific metrics such as average rewards, entropy of the policy, or other performance indicators after an episode or batch of interactions.\n",
    "\n",
    "4. reverse_topologically_sort_corr (Reverse Topological Sorting)\n",
    "Original Usage: Organizes the model's computational graph in a reverse topological order.\n",
    "RL Application: Potentially useful for analyzing the dependency of outputs on previous layers in deep networks, helping to decide which layers to target for interventions.\n",
    "\n",
    "5. sender_hook and receiver_hook (Manage Data Flow)\n",
    "Original Usage: Attach hooks to the model to intercept and modify data as it flows through the model.\n",
    "RL Application: Attach hooks to the policy or value networks to modify activations or weights dynamically during training, affecting the agent’s decision-making process.\n",
    "\n",
    "6. add_all_sender_hooks and add_sender_hook (Add Hooks to Nodes)\n",
    "Original Usage: Add hooks dynamically to specific nodes or layers in the model based on certain conditions.\n",
    "RL Application: Use in similar fashion to intercept and possibly modify the computation in neural networks, e.g., zeroing out activations of certain neurons to study their impact on agent behavior.\n",
    "\n",
    "7. setup_corrupted_cache (Cache Setup for Modifications)\n",
    "Original Usage: Set up a cache system to store corrupted (modified) activations.\n",
    "RL Application: Useful for experiments where part of the agent’s observations or internal state representations are systematically altered to assess robustness or identify critical information pathways.\n",
    "\n",
    "8. setup_model_hooks (Configure Hooks)\n",
    "Original Usage: Configures the hooks based on the experiment’s needs.\n",
    "RL Application: Could be adapted to dynamically modify how data is processed within the RL model, for instance, to implement dropout or noise injection for robustness testing.\n",
    "\n",
    "9. step (Process One Node)\n",
    "Original Usage: Processes one computational node, evaluates its impact, and decides whether to keep the current configuration.\n",
    "RL Application: Translate to processing one step or episode in RL, evaluating the agent's performance, and deciding whether to keep the modifications.\n",
    "\n",
    "10. remove_redundant_node (Cleanup)\n",
    "Original Usage: Remove nodes that are found to be redundant based on the experiment's criteria.\n",
    "RL Application: This might correspond to pruning certain neurons or layers that are found to be non-contributory towards the agent’s performance.\n",
    "\n",
    "11. increment_current_node (Node Navigation)\n",
    "Original Usage: Move to the next node in the computational graph.\n",
    "RL Application: Adapted to move through different components or layers of the RL model systematically during the experiment.\n",
    "\n",
    "12. save_subgraph and load_subgraph (State Saving and Loading)\n",
    "Original Usage: Save and load configurations of the computational graph.\n",
    "RL Application: Save and load agent configurations or network states at various points to revert to known good settings or explore the effects of changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26b9fe87-a916-46c4-9fb1-4079fb713025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#import statements\n",
    "import matplotlib.patches as patches\n",
    "import networkx as nx\n",
    "import heist\n",
    "import helpers\n",
    "import torch.distributions\n",
    "import torch\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from helpers import generate_action, load_model\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import typing\n",
    "import math\n",
    "\n",
    "from procgen import ProcgenGym3Env\n",
    "import struct\n",
    "import typing\n",
    "from typing import Tuple, Dict, Callable, List, Optional\n",
    "from dataclasses import dataclass\n",
    "from src.policies_modified import ImpalaCNN\n",
    "from procgen_tools.procgen_wrappers import VecExtractDictObs, TransposeFrame, ScaledFloatFrame\n",
    "\n",
    "from gym3 import ToBaselinesVecEnv\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1bc4ff-92d8-4d66-b8ae-81b208965d12",
   "metadata": {},
   "source": [
    "Rough graph\n",
    "\n",
    "input -> conv_seqs.0\n",
    "\n",
    "\n",
    "Residual block 0 and 1\n",
    "'conv_seqs' -> 'conv_seqs.0.conv' -> 'conv_seqs.0.max_pool2d' -> 'conv_seqs.0.res_block0'\n",
    "'conv_seqs.0.res_block0' -> 'conv_seqs.0.res_block0.conv0' -> 'conv_seqs.0.res_block0.conv1' -> 'conv_seqs.0.res_block1'\n",
    "'conv_seqs.0.res_block0' -> 'conv_seqs.0.res_block1' (skip connection)\n",
    "\n",
    "'conv_seqs.0.res_block1' -> conv_seqs.0.res_block1.conv0' -> conv_seqs.0.res_block1.conv1' ->  'conv_seqs.1'\n",
    "'conv_seqs.0.res_block1' -> 'conv_seqs.1'\n",
    "\n",
    "Conv Seq 1\n",
    "\n",
    "Residual block 0 and 1\n",
    "'conv_seqs.1' -> 'conv_seqs.1.conv' -> 'conv_seqs.1.max_pool2d' -> 'conv_seqs.1.res_block0'\n",
    "'conv_seqs.1.res_block0' -> 'conv_seqs.1.res_block0.conv0' -> 'conv_seqs.1.res_block0.conv1' -> 'conv_seqs.1.res_block1'\n",
    "'conv_seqs.1.res_block0' -> 'conv_seqs.1.res_block1' (skip connection)\n",
    "\n",
    "'conv_seqs.1.res_block1' -> conv_seqs.1.res_block1.conv0' -> conv_seqs.1.res_block1.conv1' ->  'conv_seqs.1'\n",
    "'conv_seqs.1.res_block1' -> 'conv_seqs.2'\n",
    "\n",
    "Conv Seq 2\n",
    "\n",
    "Residual block 0 and 1\n",
    "'conv_seqs.2' -> 'conv_seqs.2.conv' -> 'conv_seqs.2.max_pool2d' -> 'conv_seqs.2.res_block0'\n",
    "'conv_seqs.2.res_block0' -> 'conv_seqs.2.res_block0.conv0' -> 'conv_seqs.2.res_block0.conv1' -> 'conv_seqs.2.res_block1'\n",
    "'conv_seqs.2.res_block0' -> 'conv_seqs.2.res_block1' (skip connection)\n",
    "\n",
    "'conv_seqs.2.res_block1' -> conv_seqs.2.res_block1.conv0' -> conv_seqs.2.res_block1.conv1' ->  'hidden_fc'\n",
    "'conv_seqs.2.res_block1' -> 'hidden_fc'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e91f198d-ab90-4852-96e0-259190311cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, child_node):\n",
    "        self.children.append(child_node)\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}\n",
    "\n",
    "    def add_node(self, name: str):\n",
    "        if name not in self.nodes:\n",
    "            self.nodes[name] = Node(name)\n",
    "        return self.nodes[name]\n",
    "\n",
    "    def add_edge(self, from_node: str, to_node: str):\n",
    "        node1 = self.add_node(from_node)\n",
    "        node2 = self.add_node(to_node)\n",
    "        node1.add_child(node2)\n",
    "\n",
    "    def display(self):\n",
    "        for name, node in self.nodes.items():\n",
    "            print(f'Node {name} points to: {[child.name for child in node.children]}')\n",
    "\n",
    "def build_graph():\n",
    "    graph = Graph()\n",
    "\n",
    "    # Define the sequence names and their respective channels\n",
    "    sequences = ['conv_seqs.0', 'conv_seqs.1', 'conv_seqs.2']\n",
    "    last_seq_output = 'input'\n",
    "\n",
    "    # Connect each sequence and define internal module connections\n",
    "    for i, seq in enumerate(sequences):\n",
    "        graph.add_edge(last_seq_output, seq)\n",
    "        graph.add_edge(f'{seq}.res_block0', f'{seq}.res_block1')  # Skip connection\n",
    "\n",
    "        # Connect within ConvSequence\n",
    "        graph.add_edge(seq, f'{seq}.conv')\n",
    "        graph.add_edge(f'{seq}.conv', f'{seq}.max_pool2d')\n",
    "        graph.add_edge(f'{seq}.max_pool2d', f'{seq}.res_block0')\n",
    "        graph.add_edge(f'{seq}.res_block0', f'{seq}.res_block0.conv0')\n",
    "        graph.add_edge(f'{seq}.res_block0.conv0', f'{seq}.res_block0.conv1')\n",
    "        graph.add_edge(f'{seq}.res_block0.conv1', f'{seq}.res_block1')\n",
    "\n",
    "        graph.add_edge(f'{seq}.res_block1', f'{seq}.res_block1.conv0')\n",
    "        graph.add_edge(f'{seq}.res_block1.conv0', f'{seq}.res_block1.conv1')\n",
    "\n",
    "        # Connect to next sequence or to hidden_fc\n",
    "        next_node = sequences[i+1] if i < len(sequences) - 1 else 'hidden_fc'\n",
    "        graph.add_edge(f'{seq}.res_block1.conv1', next_node)\n",
    "        graph.add_edge(f'{seq}.res_block1', next_node)  # Skip connection\n",
    "\n",
    "    # Connect hidden_fc to logits_fc and value_fc\n",
    "    graph.add_node('hidden_fc')\n",
    "    graph.add_node('logits_fc')\n",
    "    graph.add_node('value_fc')\n",
    "    graph.add_edge('hidden_fc', 'logits_fc')\n",
    "    graph.add_edge('hidden_fc', 'value_fc')\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c119fc-01ed-46d0-a43d-4d4212abe4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node input points to: ['conv_seqs.0', 'conv_seqs.1', 'conv_seqs.2']\n",
      "Node conv_seqs.0 points to: ['conv_seqs.0.conv']\n",
      "Node conv_seqs.0.res_block0 points to: ['conv_seqs.0.res_block1', 'conv_seqs.0.res_block0.conv0']\n",
      "Node conv_seqs.0.res_block1 points to: ['conv_seqs.0.res_block1.conv0', 'conv_seqs.1']\n",
      "Node conv_seqs.0.conv points to: ['conv_seqs.0.max_pool2d']\n",
      "Node conv_seqs.0.max_pool2d points to: ['conv_seqs.0.res_block0']\n",
      "Node conv_seqs.0.res_block0.conv0 points to: ['conv_seqs.0.res_block0.conv1']\n",
      "Node conv_seqs.0.res_block0.conv1 points to: ['conv_seqs.0.res_block1']\n",
      "Node conv_seqs.0.res_block1.conv0 points to: ['conv_seqs.0.res_block1.conv1']\n",
      "Node conv_seqs.0.res_block1.conv1 points to: ['conv_seqs.1']\n",
      "Node conv_seqs.1 points to: ['conv_seqs.1.conv']\n",
      "Node conv_seqs.1.res_block0 points to: ['conv_seqs.1.res_block1', 'conv_seqs.1.res_block0.conv0']\n",
      "Node conv_seqs.1.res_block1 points to: ['conv_seqs.1.res_block1.conv0', 'conv_seqs.2']\n",
      "Node conv_seqs.1.conv points to: ['conv_seqs.1.max_pool2d']\n",
      "Node conv_seqs.1.max_pool2d points to: ['conv_seqs.1.res_block0']\n",
      "Node conv_seqs.1.res_block0.conv0 points to: ['conv_seqs.1.res_block0.conv1']\n",
      "Node conv_seqs.1.res_block0.conv1 points to: ['conv_seqs.1.res_block1']\n",
      "Node conv_seqs.1.res_block1.conv0 points to: ['conv_seqs.1.res_block1.conv1']\n",
      "Node conv_seqs.1.res_block1.conv1 points to: ['conv_seqs.2']\n",
      "Node conv_seqs.2 points to: ['conv_seqs.2.conv']\n",
      "Node conv_seqs.2.res_block0 points to: ['conv_seqs.2.res_block1', 'conv_seqs.2.res_block0.conv0']\n",
      "Node conv_seqs.2.res_block1 points to: ['conv_seqs.2.res_block1.conv0', 'hidden_fc']\n",
      "Node conv_seqs.2.conv points to: ['conv_seqs.2.max_pool2d']\n",
      "Node conv_seqs.2.max_pool2d points to: ['conv_seqs.2.res_block0']\n",
      "Node conv_seqs.2.res_block0.conv0 points to: ['conv_seqs.2.res_block0.conv1']\n",
      "Node conv_seqs.2.res_block0.conv1 points to: ['conv_seqs.2.res_block1']\n",
      "Node conv_seqs.2.res_block1.conv0 points to: ['conv_seqs.2.res_block1.conv1']\n",
      "Node conv_seqs.2.res_block1.conv1 points to: ['hidden_fc']\n",
      "Node hidden_fc points to: ['logits_fc', 'value_fc']\n",
      "Node logits_fc points to: []\n",
      "Node value_fc points to: []\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../model_final.pt\"\n",
    "model = helpers.load_model(model_path=model_path)\n",
    "graph = build_graph()\n",
    "graph.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "070e190f-8421-478b-89c5-e57087dd95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Define necessary classes\n",
    "\n",
    "class TorchIndex:\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.indices)\n",
    "\n",
    "class EdgeType:\n",
    "    ADDITION = \"ADDITION\"\n",
    "    PLACEHOLDER = \"PLACEHOLDER\"\n",
    "    DIRECT_COMPUTATION = \"DIRECT_COMPUTATION\"\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, edge_type):\n",
    "        self.edge_type = edge_type\n",
    "        self.present = True\n",
    "\n",
    "class TLACDCInterpNode:\n",
    "    def __init__(self, name, index, incoming_edge_type):\n",
    "        self.name = name\n",
    "        self.index = index\n",
    "        self.incoming_edge_type = incoming_edge_type\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.name}{self.index}\"\n",
    "\n",
    "    def _add_child(self, child_node):\n",
    "        self.children.append(child_node)\n",
    "\n",
    "    def _add_parent(self, parent_node):\n",
    "        self.parents.append(parent_node)\n",
    "\n",
    "class TLACDCCorrespondence:\n",
    "    def __init__(self):\n",
    "        self.graph = defaultdict(dict)\n",
    "        self.edges = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "    def nodes(self) -> List[TLACDCInterpNode]:\n",
    "        return [node for by_index_list in self.graph.values() for node in by_index_list.values()]\n",
    "\n",
    "    def all_edges(self) -> Dict[Tuple[str, TorchIndex, str, TorchIndex], Edge]:\n",
    "        big_dict = {}\n",
    "        for child_name, rest1 in self.edges.items():\n",
    "            for child_index, rest2 in rest1.items():\n",
    "                for parent_name, rest3 in rest2.items():\n",
    "                    for parent_index, edge in rest3.items():\n",
    "                        assert edge is not None, (child_name, child_index, parent_name, parent_index, \"Edges have been setup WRONG somehow...\")\n",
    "                        big_dict[(child_name, child_index, parent_name, parent_index)] = edge\n",
    "        return big_dict\n",
    "\n",
    "    def add_node(self, node: TLACDCInterpNode, safe=True):\n",
    "        if safe:\n",
    "            assert node not in self.nodes(), f\"Node {node} already in graph\"\n",
    "        self.graph[node.name][node.index] = node\n",
    "\n",
    "    def add_edge(self, parent_node: TLACDCInterpNode, child_node: TLACDCInterpNode, edge: Edge, safe=True):\n",
    "        if safe:\n",
    "            if parent_node not in self.nodes():\n",
    "                self.add_node(parent_node)\n",
    "            if child_node not in self.nodes():\n",
    "                self.add_node(child_node)\n",
    "        assert child_node.incoming_edge_type == edge.edge_type, (child_node.incoming_edge_type, edge.edge_type)\n",
    "        parent_node._add_child(child_node)\n",
    "        child_node._add_parent(parent_node)\n",
    "        self.edges[child_node.name][child_node.index][parent_node.name][parent_node.index] = edge\n",
    "\n",
    "    def remove_edge(self, child_name: str, child_index: TorchIndex, parent_name: str, parent_index: TorchIndex):\n",
    "        edge = self.edges[child_name][child_index][parent_name][parent_index]\n",
    "        edge.present = False\n",
    "        del self.edges[child_name][child_index][parent_name][parent_index]\n",
    "        if not self.edges[child_name][child_index][parent_name]:\n",
    "            del self.edges[child_name][child_index][parent_name]\n",
    "        if not self.edges[child_name][child_index]:\n",
    "            del self.edges[child_name][child_index]\n",
    "        if not self.edges[child_name]:\n",
    "            del self.edges[child_name]\n",
    "        parent = self.graph[parent_name][parent_index]\n",
    "        child = self.graph[child_name][child_index]\n",
    "        parent.children.remove(child)\n",
    "        child.parents.remove(parent)\n",
    "\n",
    "def setup_graph_from_model_using_channels(model):\n",
    "    #TODO: slices on the channels are not really working\n",
    "    correspondence = TLACDCCorrespondence()\n",
    "    prev_node_channels = None\n",
    "\n",
    "    for i, conv_seq in enumerate(model.conv_seqs):\n",
    "        out_channels = conv_seq._out_channels\n",
    "        conv_node_base = f\"ConvSeq_{i}\"\n",
    "\n",
    "        # Add conv nodes and their connections\n",
    "        current_conv_nodes = []\n",
    "        for j in range(out_channels):\n",
    "            conv_node = TLACDCInterpNode(name=f\"{conv_node_base}_OutChan_{j}\", index=TorchIndex([j]), incoming_edge_type=EdgeType.ADDITION)\n",
    "            correspondence.add_node(conv_node)\n",
    "            current_conv_nodes.append(conv_node)\n",
    "\n",
    "            if prev_node_channels:\n",
    "                for prev_node in prev_node_channels:\n",
    "                    correspondence.add_edge(parent_node=prev_node, child_node=conv_node, edge=Edge(EdgeType.ADDITION))\n",
    "\n",
    "        prev_node_channels = current_conv_nodes\n",
    "\n",
    "        # Add residual blocks and their connections\n",
    "        for res_block_idx in range(2):\n",
    "            res_block_node_base = f\"ResBlock_{i}_{res_block_idx}\"\n",
    "            current_res_block_nodes = []\n",
    "            for j in range(out_channels):\n",
    "                res_block_node = TLACDCInterpNode(name=f\"{res_block_node_base}_Chan_{j}\", index=TorchIndex([j]), incoming_edge_type=EdgeType.ADDITION)\n",
    "                correspondence.add_node(res_block_node)\n",
    "                current_res_block_nodes.append(res_block_node)\n",
    "\n",
    "                if j < len(prev_node_channels):\n",
    "                    correspondence.add_edge(parent_node=prev_node_channels[j], child_node=res_block_node, edge=Edge(EdgeType.ADDITION))\n",
    "\n",
    "            prev_node_channels = current_res_block_nodes\n",
    "\n",
    "    # Add fully connected (hidden) layer and connect to the last residual block's output\n",
    "    fc_node = TLACDCInterpNode(name=\"Hidden_FC\", index=TorchIndex([0]), incoming_edge_type=EdgeType.ADDITION)\n",
    "    correspondence.add_node(fc_node)\n",
    "\n",
    "    for prev_node in prev_node_channels:\n",
    "        correspondence.add_edge(parent_node=prev_node, child_node=fc_node, edge=Edge(EdgeType.ADDITION))\n",
    "\n",
    "    # Add logits and value fully connected layers and connect them to the hidden FC layer\n",
    "    logits_node = TLACDCInterpNode(name=\"Logits_FC\", index=TorchIndex([0]), incoming_edge_type=EdgeType.ADDITION)\n",
    "    correspondence.add_node(logits_node)\n",
    "    correspondence.add_edge(parent_node=fc_node, child_node=logits_node, edge=Edge(EdgeType.ADDITION))\n",
    "\n",
    "    value_node = TLACDCInterpNode(name=\"Value_FC\", index=TorchIndex([0]), incoming_edge_type=EdgeType.ADDITION)\n",
    "    correspondence.add_node(value_node)\n",
    "    correspondence.add_edge(parent_node=fc_node, child_node=value_node, edge=Edge(EdgeType.ADDITION))\n",
    "\n",
    "    return correspondence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f993264-dfae-4358-895f-e375a1de1c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as np\n",
    "\n",
    "# Dictionary of ordered layer names\n",
    "ordered_layer_names = {\n",
    " 0: 'conv_seqs',\n",
    " 1: 'conv_seqs.0',\n",
    " 2: 'conv_seqs.0.conv',\n",
    " 3: 'conv_seqs.0.max_pool2d',\n",
    " 4: 'conv_seqs.0.res_block0',\n",
    " 5: 'conv_seqs.0.res_block0.conv0',\n",
    " 6: 'conv_seqs.0.res_block0.conv1',\n",
    " 7: 'conv_seqs.0.res_block1',\n",
    " 8: 'conv_seqs.0.res_block1.conv0',\n",
    " 9: 'conv_seqs.0.res_block1.conv1',\n",
    " 10: 'conv_seqs.1',\n",
    " 11: 'conv_seqs.1.conv',\n",
    " 12: 'conv_seqs.1.max_pool2d',\n",
    " 13: 'conv_seqs.1.res_block0',\n",
    " 14: 'conv_seqs.1.res_block0.conv0',\n",
    " 15: 'conv_seqs.1.res_block0.conv1',\n",
    " 16: 'conv_seqs.1.res_block1',\n",
    " 17: 'conv_seqs.1.res_block1.conv0',\n",
    " 18: 'conv_seqs.1.res_block1.conv1',\n",
    " 19: 'conv_seqs.2',\n",
    " 20: 'conv_seqs.2.conv',\n",
    " 21: 'conv_seqs.2.max_pool2d',\n",
    " 22: 'conv_seqs.2.res_block0',\n",
    " 23: 'conv_seqs.2.res_block0.conv0',\n",
    " 24: 'conv_seqs.2.res_block0.conv1',\n",
    " 25: 'conv_seqs.2.res_block1',\n",
    " 26: 'conv_seqs.2.res_block1.conv0',\n",
    " 27: 'conv_seqs.2.res_block1.conv1',\n",
    " 28: 'hidden_fc',\n",
    " 29: 'logits_fc',\n",
    " 30: 'value_fc'\n",
    "}\n",
    "\n",
    "# Extracting all the names into a list\n",
    "layer_names = list(ordered_layer_names.values())\n",
    "\n",
    "\n",
    "class ModelActivations:\n",
    "    def __init__(self, model):\n",
    "        self.activations = {}\n",
    "        self.model = model\n",
    "        self.hooks = []  # To keep track of hooks\n",
    "        self.layer_paths = layer_names\n",
    "\n",
    "    def clear_hooks(self):\n",
    "        # Remove all previously registered hooks\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        self.activations = {}\n",
    "\n",
    "    def get_activation(self, name):\n",
    "        def hook(model, input, output):\n",
    "            processed_output = []\n",
    "            for item in output:\n",
    "                if isinstance(item, torch.Tensor):\n",
    "                    processed_output.append(item.detach())\n",
    "                elif isinstance(item, torch.distributions.Categorical):\n",
    "                    processed_output.append(item.logits.detach())\n",
    "                else:\n",
    "                    processed_output.append(item)\n",
    "            self.activations[name] = tuple(processed_output)\n",
    "        return hook\n",
    "\n",
    "    def register_hook_by_path(self, path, name):\n",
    "        elements = path.split('.')\n",
    "        model = self.model\n",
    "        for i, element in enumerate(elements):\n",
    "            if '[' in element:\n",
    "                base, index = element.replace(']', '').split('[')\n",
    "                index = int(index)\n",
    "                model = getattr(model, base)[index]\n",
    "            else:\n",
    "                model = getattr(model, element)\n",
    "            if i == len(elements) - 1:\n",
    "                hook = model.register_forward_hook(self.get_activation(name))\n",
    "                self.hooks.append(hook)  # Keep track of the hook\n",
    "\n",
    "    def run_with_cache(self, input):\n",
    "        self.clear_hooks()  # Clear any existing hooks\n",
    "        self.activations = {}  # Reset activations\n",
    "        for path in self.layer_paths:\n",
    "            self.register_hook_by_path(path, path.replace('.', '_'))\n",
    "        output = self.model(input)\n",
    "        return output, self.activations\n",
    "    \n",
    "    def patch_activations(self, input, to_patch_activation_tensor = 0,to_change_layer_name='conv_seqs.0.conv',ablate =\"True\"):\n",
    "        cached_activations = {}\n",
    "        \n",
    "        # Define a factory function to create hook functions with a stable layer name\n",
    "        def make_hook(layer_name):\n",
    "            def saves_cache(module, input, output):\n",
    "                cached_activations[layer_name] = output.detach()\n",
    "                if ablate:\n",
    "                        return output* 0\n",
    "                elif layer_name == to_change_layer_name:\n",
    "                    print(\"Here are the shapes\",to_patch_activation_tensor.shape, output.shape)\n",
    "                    if to_patch_activation_tensor.shape == output.shape:                \n",
    "                        return  to_patch_activation_tensor\n",
    "                    \n",
    "            return saves_cache\n",
    "        \n",
    "        # Function to recursively get a sub-module from its name\n",
    "        def get_submodule(module, submodule_name):\n",
    "            names = submodule_name.split('.')\n",
    "            for name in names:\n",
    "                module = getattr(module, name)\n",
    "            return module\n",
    "        \n",
    "        self.clear_hooks()\n",
    "        for name in layer_names:\n",
    "            layer = get_submodule(model, name)\n",
    "            hook = make_hook(name)  # Create a hook with a stable layer name\n",
    "            self.hooks.append(layer.register_forward_hook(hook))\n",
    "\n",
    "        output = model(input)  # Assuming tensor_gem is defined elsewhere\n",
    "        self.clear_hooks()\n",
    "\n",
    "        return output,cached_activations\n",
    "\n",
    "class RLPolicyExperiment(BaseCallback):\n",
    "    \"\"\"\n",
    "    A class to manage an experiment that adjusts and analyzes the components of an RL policy\n",
    "    network dynamically during training, akin to the TLACDCExperiment for neural networks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, env, verbose=1, log_dir=None, abs_value_threshold=False,\n",
    "             using_wandb=False, wandb_entity_name=None, wandb_group_name=None,\n",
    "             wandb_project_name=None, wandb_run_name=None, wandb_notes=None, \n",
    "             wandb_dir=None, wandb_mode='online', wandb_config=None,threshold=200, \n",
    "             parallel_hypotheses=1, metric=lambda x: x.mean().item()):\n",
    "        super().__init__(verbose)\n",
    "        self.model = model\n",
    "        self.model_activations = ModelActivations(model)\n",
    "        self.env = env\n",
    "        self.abs_value_threshold = abs_value_threshold\n",
    "        self.verbose = verbose\n",
    "        self.step_idx = 0\n",
    "        self.using_wandb = using_wandb\n",
    "\n",
    "        # Node definitions are based on the provided structure\n",
    "        self.graph = graph\n",
    "\n",
    "        # Metrics and hypothesis setup\n",
    "        self.metric = metric\n",
    "        \n",
    "\n",
    "    def reverse_topologically_sort_corr(graph):\n",
    "        #sorts the graph in reverse topological order\n",
    "        order = []\n",
    "        # Set all nodes as unvisited\n",
    "        for node in graph.nodes.values():\n",
    "            node.visited = False\n",
    "\n",
    "        # Helper function to perform DFS\n",
    "        def dfs(node):\n",
    "            node.visited = True\n",
    "            for child in node.children:\n",
    "                if not child.visited:\n",
    "                    dfs(child)\n",
    "            # Prepend node to maintain reverse order\n",
    "            order.insert(0, node.name)\n",
    "\n",
    "        # Perform DFS from each unvisited node\n",
    "        for node in graph.nodes.values():\n",
    "            if not node.visited:\n",
    "                dfs(node)\n",
    "        return order\n",
    "\n",
    "\n",
    "    \n",
    "            \n",
    "    def _on_step(self):\n",
    "        order = self.topological_sort()\n",
    "        first_node = self.nodes[order[0]]\n",
    "\n",
    "        # Simulate zero activation for the first node and get the logits change\n",
    "        original_logits = self.model.forward()  # Get original logits with normal activations\n",
    "        self.model.set_zero_activation(first_node.name)  # Method to zero out activations\n",
    "        modified_logits = self.model.forward()  # Get logits with zeroed activations\n",
    "\n",
    "        # Check if change in logits exceeds the threshold\n",
    "        change = any(abs(modified_logits[i] - original_logits[i]) > threshold for i in range(len(modified_logits)))\n",
    "\n",
    "        if not change:\n",
    "            # If the node is not important, remove all edges to and from this node\n",
    "            for parent in first_node.parents:\n",
    "                parent.children.remove(first_node)\n",
    "                self.removed_edges.add((parent.name, first_node.name))\n",
    "            for child in first_node.children:\n",
    "                child.parents.remove(first_node)\n",
    "                self.removed_edges.add((first_node.name, child.name))\n",
    "            self.nodes.pop(first_node.name)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def verify_model_setup(self):\n",
    "        \"\"\"\n",
    "        Verifies if we are able to assess each channel of the model\n",
    "        \"\"\"\n",
    "        assert isinstance(model.conv_seqs, torch.nn.ModuleList), \"conv_seqs should be an instance of torch.nn.ModuleList\"\n",
    "    \n",
    "        # Loop through each ConvSequence in conv_seqs\n",
    "        for i, conv_seq in enumerate(model.conv_seqs):\n",
    "            # Check convolution layer\n",
    "            assert isinstance(conv_seq.conv, torch.nn.Conv2d), f\"conv in ConvSequence {i} should be an instance of torch.nn.Conv2d\"\n",
    "            # Check max pooling layer\n",
    "            assert isinstance(conv_seq.max_pool2d, torch.nn.MaxPool2d), f\"max_pool2d in ConvSequence {i} should be an instance of torch.nn.MaxPool2d\"\n",
    "            # Check each residual block\n",
    "            for j, res_block in enumerate([conv_seq.res_block0, conv_seq.res_block1]):\n",
    "                # Check both convolutional layers within the residual block\n",
    "                assert isinstance(res_block.conv0, torch.nn.Conv2d), f\"conv0 in ResidualBlock {j} of ConvSequence {i} should be an instance of torch.nn.Conv2d\"\n",
    "                assert isinstance(res_block.conv1, torch.nn.Conv2d), f\"conv1 in ResidualBlock {j} of ConvSequence {i} should be an instance of torch.nn.Conv2d\"\n",
    "    \n",
    "        # Check other components of ImpalaCNN\n",
    "        assert isinstance(model.hidden_fc, torch.nn.Linear), \"hidden_fc should be an instance of torch.nn.Linear\"\n",
    "        assert isinstance(model.logits_fc, torch.nn.Linear), \"logits_fc should be an instance of torch.nn.Linear\"\n",
    "        assert isinstance(model.value_fc, torch.nn.Linear), \"value_fc should be an instance of torch.nn.Linear\"\n",
    "\n",
    " \n",
    "\n",
    "   \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba0f8abe-a7d5-408e-93af-015c6b4c91e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'permute'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m venv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#output, cache = model_activations.patch_activations(helpers.observation_to_rgb(input), to_patch_activation_tensor = 0,to_change_layer_name='conv_seqs.0.conv',ablate =\"True\")\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel_activations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhelpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_to_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 91\u001b[0m, in \u001b[0;36mModelActivations.run_with_cache\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_paths:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_hook_by_path(path, path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 91\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ai-safety-camp-2024-model-agents/notebooks/../src/policies_impala.py:83\u001b[0m, in \u001b[0;36mImpalaCNN.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     82\u001b[0m x \u001b[38;5;241m=\u001b[39m obs \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# scale to 0-1\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NHWC => NCHW\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv_seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_seqs:\n\u001b[1;32m     85\u001b[0m     x \u001b[38;5;241m=\u001b[39m conv_seq(x)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'permute'"
     ]
    }
   ],
   "source": [
    "model_activations = ModelActivations(model)\n",
    "\n",
    "start_level = random.randint(1, 10000)\n",
    "venv = heist.create_venv(num=1, num_levels=1, start_level=start_level)\n",
    "input = venv.reset()\n",
    "#output, cache = model_activations.patch_activations(helpers.observation_to_rgb(input), to_patch_activation_tensor = 0,to_change_layer_name='conv_seqs.0.conv',ablate =\"True\")\n",
    "model_activations.run_with_cache(helpers.observation_to_rgb(input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d68ea4-db34-4bca-9854-4eeddbb144ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
